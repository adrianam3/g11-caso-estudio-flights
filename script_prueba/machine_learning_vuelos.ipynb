{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0b2c5e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Pipeline de predicci√≥n de retrasos (flights_clean.csv)\n",
    "- Incluye: carga segura, features b√°sicas, target encoding (TE), agregados hist√≥ricos sin fuga,\n",
    "split temporal, entrenamiento (LightGBM, XGBoost y RandomForest opcional memoria-seguro),\n",
    "selecci√≥n de umbrales, guardado de artefactos y funci√≥n de inferencia/score.\n",
    "- Cada bloque est√° numerado y comentado.\n",
    "\n",
    "\n",
    "Requisitos m√≠nimos del entorno (ya usados durante las pruebas previas):\n",
    "pip install pandas numpy scikit-learn lightgbm xgboost joblib\n",
    "(omitir CatBoost por problemas de build en Windows/py3.13)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ea2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # Predicci√≥n de retrasos de vuelos (2015) - Pipeline completo\n",
    "# # ============================================================\n",
    "# # Qu√© hace este script:\n",
    "# # 1) Carga flights_clean.csv con dtypes compactos (memoria)\n",
    "# # 2) Respeta features ya pre-calculadas (MINUTO_DIA_SALIDA, SALIDA_SIN/COS, etc.)\n",
    "# # 3) Agrega MONTH_SIN/COS si faltan\n",
    "# # 4) Construye DISTANCIA_HAV si falta (haversine)\n",
    "# # 5) Arma RUTA (ORIGIN_AIRPORT->DESTINATION_AIRPORT) si falta\n",
    "# # 6) Split temporal: train (MONTH 1-9), valid (MONTH 10-12)\n",
    "# # 7) Target Encoding (TE) sin fuga + Agregados hist√≥ricos (AIR/DES/ORI/RUTA/RUTA_HORA)\n",
    "# # 8) Entrena LightGBM y XGBoost (RF opcional) con pesos por desbalance\n",
    "# # 9) Calcula umbral F1 √≥ptimo + umbrales operativos (precision~0.30, recall>=0.70)\n",
    "# # 10) Guarda artefactos (modelos, TE, agregados, feature_order, metadata)\n",
    "# # 11) Funci√≥n de inferencia score_retraso(df_raw) lista para usar\n",
    "# # ============================================================\n",
    "\n",
    "# import os, json, time, math, gc, warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from joblib import dump, load\n",
    "\n",
    "# # ===========\n",
    "# # Utilidades\n",
    "# # ===========\n",
    "# def downcast_int(series: pd.Series, signed=True):\n",
    "#     \"\"\"Reduce dtype de enteros para ahorrar RAM (sin alterar valores).\"\"\"\n",
    "#     if not pd.api.types.is_integer_dtype(series):\n",
    "#         return series\n",
    "#     return pd.to_numeric(series, downcast=\"integer\" if signed else \"unsigned\")\n",
    "\n",
    "# def downcast_float(series: pd.Series):\n",
    "#     \"\"\"Reduce dtype de floats para ahorrar RAM.\"\"\"\n",
    "#     if not pd.api.types.is_float_dtype(series):\n",
    "#         return series\n",
    "#     return pd.to_numeric(series, downcast=\"float\")\n",
    "\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     \"\"\"Distancia Haversine (km).\"\"\"\n",
    "#     R = 6371.0\n",
    "#     lat1 = np.radians(lat1.astype(\"float64\"))\n",
    "#     lon1 = np.radians(lon1.astype(\"float64\"))\n",
    "#     lat2 = np.radians(lat2.astype(\"float64\"))\n",
    "#     lon2 = np.radians(lon2.astype(\"float64\"))\n",
    "#     dlat = lat2 - lat1\n",
    "#     dlon = lon2 - lon1\n",
    "#     a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "#     c = 2 * np.arcsin(np.sqrt(a))\n",
    "#     return (R * c).astype(\"float32\")\n",
    "\n",
    "# # ===================================\n",
    "# # 1) Carga segura + dtypes compactos\n",
    "# # ===================================\n",
    "# def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Carga flights_clean.csv sin alterar columnas ya calculadas.\n",
    "#     Hace downcast de tipos num√©ricos cuando es seguro.\n",
    "#     \"\"\"\n",
    "#     t0 = time.time()\n",
    "#     df = pd.read_csv(path_csv)\n",
    "#     # Downcast num\n",
    "#     for c in df.select_dtypes(include=[\"int64\",\"int32\",\"int16\",\"int8\"]).columns:\n",
    "#         df[c] = downcast_int(df[c])\n",
    "#     for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "#         df[c] = downcast_float(df[c])\n",
    "#     # Categ√≥ricas: como string (evitar category por ahora para TE)\n",
    "#     for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"AIRLINE_NAME\",\n",
    "#               \"ORIGEN_AEROPUERTO\",\"ORIGEN_CIUDAD\",\"ORIGEN_ESTADO\",\n",
    "#               \"DEST_AEROPUERTO\",\"DEST_CIUDAD\",\"DEST_ESTADO\",\"MOTIVO_RETRASO\"]:\n",
    "#         if c in df.columns:\n",
    "#             df[c] = df[c].astype(\"string\")\n",
    "\n",
    "#     print(f\"‚úì Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "#     if \"RETRASADO_LLEGADA\" in df.columns:\n",
    "#         print(\"‚úì Rate retraso:\", df[\"RETRASADO_LLEGADA\"].mean())\n",
    "#     return df\n",
    "\n",
    "# # ============================\n",
    "# # 2) RUTA (si no existe)\n",
    "# # ============================\n",
    "# def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     out = df.copy()\n",
    "#     if \"RUTA\" not in out.columns:\n",
    "#         if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "#             out[\"RUTA\"] = (out[\"ORIGIN_AIRPORT\"].astype(\"string\") + \"->\" +\n",
    "#                            out[\"DESTINATION_AIRPORT\"].astype(\"string\"))\n",
    "#         else:\n",
    "#             raise KeyError(\"Faltan ORIGIN_AIRPORT/DESTINATION_AIRPORT para construir RUTA.\")\n",
    "#     return out\n",
    "\n",
    "# # =====================================================\n",
    "# # 3) Features de tiempo (respetando las columnas ya existentes)\n",
    "# # =====================================================\n",
    "# def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Respeta: MINUTO_DIA_SALIDA, HORA_SALIDA, SALIDA_SIN, SALIDA_COS.\n",
    "#     Solo crea lo que falte. Agrega MONTH_SIN/COS si no existen.\n",
    "#     \"\"\"\n",
    "#     out = df.copy()\n",
    "\n",
    "#     # MINUTO_DIA_SALIDA\n",
    "#     if \"MINUTO_DIA_SALIDA\" not in out.columns:\n",
    "#         def _hhmm_to_minute(val):\n",
    "#             if pd.isna(val): return np.nan\n",
    "#             try:\n",
    "#                 v = int(str(val).strip().split('.')[0])\n",
    "#             except Exception:\n",
    "#                 return np.nan\n",
    "#             if v == 2400:  # normalizar 24:00 a 00:00\n",
    "#                 return 0\n",
    "#             hh, mm = v // 100, v % 100\n",
    "#             hh = min(max(hh, 0), 23); mm = min(max(mm, 0), 59)\n",
    "#             return hh * 60 + mm\n",
    "\n",
    "#         if \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "#             out[\"MINUTO_DIA_SALIDA\"] = out[\"SCHEDULED_DEPARTURE\"].map(_hhmm_to_minute).astype(\"float32\")\n",
    "#         elif \"HORA_SALIDA\" in out.columns:\n",
    "#             out[\"MINUTO_DIA_SALIDA\"] = (out[\"HORA_SALIDA\"].astype(\"float32\").clip(0, 23) * 60).astype(\"float32\")\n",
    "#         else:\n",
    "#             raise KeyError(\"Falta 'MINUTO_DIA_SALIDA' y no puedo derivarlo (no hay SCHEDULED_DEPARTURE ni HORA_SALIDA).\")\n",
    "\n",
    "#     # HORA_SALIDA\n",
    "#     if \"HORA_SALIDA\" not in out.columns:\n",
    "#         out[\"HORA_SALIDA\"] = (out[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int16\").clip(0, 23)\n",
    "\n",
    "#     # SALIDA_SIN/COS\n",
    "#     if \"SALIDA_SIN\" not in out.columns or \"SALIDA_COS\" not in out.columns:\n",
    "#         mins = out[\"MINUTO_DIA_SALIDA\"].astype(\"float32\")\n",
    "#         rad = 2.0 * np.pi * (mins / (24.0 * 60.0))\n",
    "#         if \"SALIDA_SIN\" not in out.columns:\n",
    "#             out[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "#         if \"SALIDA_COS\" not in out.columns:\n",
    "#             out[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "#     # MONTH_SIN/COS\n",
    "#     if \"MONTH\" in out.columns:\n",
    "#         if \"MONTH_SIN\" not in out.columns or \"MONTH_COS\" not in out.columns:\n",
    "#             m = out[\"MONTH\"].astype(\"float32\")\n",
    "#             ang = 2.0 * np.pi * (m - 1.0) / 12.0\n",
    "#             if \"MONTH_SIN\" not in out.columns:\n",
    "#                 out[\"MONTH_SIN\"] = np.sin(ang).astype(\"float32\")\n",
    "#             if \"MONTH_COS\" not in out.columns:\n",
    "#                 out[\"MONTH_COS\"] = np.cos(ang).astype(\"float32\")\n",
    "\n",
    "#     # dtypes compactos\n",
    "#     out[\"MINUTO_DIA_SALIDA\"] = out[\"MINUTO_DIA_SALIDA\"].astype(\"float32\", copy=False)\n",
    "#     out[\"HORA_SALIDA\"]       = out[\"HORA_SALIDA\"].astype(\"int16\", copy=False)\n",
    "#     out[\"SALIDA_SIN\"]        = out[\"SALIDA_SIN\"].astype(\"float32\", copy=False)\n",
    "#     out[\"SALIDA_COS\"]        = out[\"SALIDA_COS\"].astype(\"float32\", copy=False)\n",
    "#     if \"MONTH_SIN\" in out.columns: out[\"MONTH_SIN\"] = out[\"MONTH_SIN\"].astype(\"float32\", copy=False)\n",
    "#     if \"MONTH_COS\" in out.columns: out[\"MONTH_COS\"] = out[\"MONTH_COS\"].astype(\"float32\", copy=False)\n",
    "#     return out\n",
    "\n",
    "# # =====================================================\n",
    "# # 4) Distancia Haversine (si falta DISTANCIA_HAV)\n",
    "# # =====================================================\n",
    "# def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     out = df.copy()\n",
    "#     if \"DISTANCIA_HAV\" not in out.columns:\n",
    "#         need = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}\n",
    "#         if need.issubset(out.columns):\n",
    "#             out[\"DISTANCIA_HAV\"] = haversine(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"],\n",
    "#                                              out[\"DEST_LAT\"],  out[\"DEST_LON\"])\n",
    "#         elif \"DISTANCE\" in out.columns:\n",
    "#             # fallback: usar DISTANCE (millas -> km aprox.)\n",
    "#             out[\"DISTANCIA_HAV\"] = (out[\"DISTANCE\"].astype(\"float32\") * 1.60934).astype(\"float32\")\n",
    "#         else:\n",
    "#             raise KeyError(\"No puedo construir DISTANCIA_HAV (faltan lat/lon y DISTANCE).\")\n",
    "#     out[\"DISTANCIA_HAV\"] = out[\"DISTANCIA_HAV\"].astype(\"float32\", copy=False)\n",
    "#     return out\n",
    "\n",
    "# # =====================================================\n",
    "# # 5) Target Encoding (TE) sin fuga (train only)\n",
    "# # =====================================================\n",
    "# def build_target_encoding(df_train: pd.DataFrame, cols, target=\"RETRASADO_LLEGADA\", smooth=20):\n",
    "#     \"\"\"\n",
    "#     Devuelve:\n",
    "#       - mappings: dict col -> {categoria: media suavizada}\n",
    "#       - defaults: dict col -> media global (para unseen)\n",
    "#       - global_mean: float\n",
    "#     \"\"\"\n",
    "#     mappings, defaults = {}, {}\n",
    "#     global_mean = df_train[target].mean()\n",
    "#     for c in cols:\n",
    "#         g = df_train.groupby(c, observed=False)[target]\n",
    "#         m = g.mean()\n",
    "#         n = g.size()\n",
    "#         te = (m*n + global_mean*smooth) / (n + smooth)\n",
    "#         mappings[c] = te.to_dict()\n",
    "#         defaults[c] = float(global_mean)\n",
    "#     return mappings, defaults, float(global_mean)\n",
    "\n",
    "# def apply_target_encoding(df: pd.DataFrame, mappings, defaults, suffix=\"_TE\"):\n",
    "#     out = df.copy()\n",
    "#     for c, mp in mappings.items():\n",
    "#         newc = f\"{c}{suffix}\"\n",
    "#         out[newc] = out[c].map(mp).astype(\"float32\")\n",
    "#         # unseen -> default\n",
    "#         out[newc] = out[newc].fillna(defaults[c]).astype(\"float32\")\n",
    "#     return out\n",
    "\n",
    "# # =====================================================\n",
    "# # 6) Agregados hist√≥ricos sin fuga (train only)\n",
    "# # =====================================================\n",
    "# def build_agg(df_train: pd.DataFrame, keys, target=\"RETRASADO_LLEGADA\", pref=\"AIR\", smooth=20, global_mean=None):\n",
    "#     \"\"\"\n",
    "#     Por 'keys' (lista de columnas), calcula rate suavizado y conteo.\n",
    "#     \"\"\"\n",
    "#     if global_mean is None:\n",
    "#         global_mean = df_train[target].mean()\n",
    "#     g = df_train.groupby(keys, observed=False)[target]\n",
    "#     m = g.mean()\n",
    "#     n = g.size()\n",
    "#     rate = (m*n + global_mean*smooth) / (n + smooth)\n",
    "#     agg = rate.rename(f\"{pref}_rate\").reset_index()\n",
    "#     agg = agg.merge(n.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "#     return agg\n",
    "\n",
    "# def left_join_agg(X: pd.DataFrame, agg: pd.DataFrame, keys, pref, global_mean):\n",
    "#     out = X.merge(agg, on=keys, how=\"left\")\n",
    "#     # fill: rate -> global_mean; n -> 0\n",
    "#     out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "#     out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "#     return out\n",
    "\n",
    "# # =====================================================\n",
    "# # 7) Construcci√≥n de matrices (base -> agregados -> TE)\n",
    "# # =====================================================\n",
    "# BASE_FEATURES = [\n",
    "#     \"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "#     \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "# ]\n",
    "# TE_COLS = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "# AGG_SPECS = [\n",
    "#     ([\"AIRLINE\"], \"AIR\"),\n",
    "#     ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "#     ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "#     ([\"RUTA\"], \"RUTA\"),\n",
    "#     ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "# ]\n",
    "\n",
    "# def make_matrices(df: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Devuelve:\n",
    "#       Xt, Xv, y_train, y_valid, mappings, defaults, global_mean, agg_tables (dict pref->df)\n",
    "#     \"\"\"\n",
    "#     # target\n",
    "#     if \"RETRASADO_LLEGADA\" not in df.columns:\n",
    "#         raise KeyError(\"Falta la columna objetivo RETRASADO_LLEGADA.\")\n",
    "#     y = df[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "\n",
    "#     # split temporal (sin fuga)\n",
    "#     train_mask = df[\"MONTH\"].between(1, 9)\n",
    "#     valid_mask = df[\"MONTH\"].between(10, 12)\n",
    "\n",
    "#     v_train = df.loc[train_mask].copy()\n",
    "#     v_valid = df.loc[valid_mask].copy()\n",
    "#     y_train = y.loc[train_mask].reset_index(drop=True)\n",
    "#     y_valid = y.loc[valid_mask].reset_index(drop=True)\n",
    "\n",
    "#     # X base\n",
    "#     cols_needed = set(BASE_FEATURES + TE_COLS)\n",
    "#     miss = cols_needed - set(df.columns)\n",
    "#     if miss:\n",
    "#         raise KeyError(f\"Faltan columnas base/TE para matrices: {sorted(miss)}\")\n",
    "\n",
    "#     Xt = v_train[BASE_FEATURES + TE_COLS].reset_index(drop=True).copy()\n",
    "#     Xv = v_valid[BASE_FEATURES + TE_COLS].reset_index(drop=True).copy()\n",
    "\n",
    "#     # Agregados (train-only) -> join a Xt/Xv\n",
    "#     global_mean = float(y_train.mean())\n",
    "#     agg_tables = {}\n",
    "#     for keys, pref in AGG_SPECS:\n",
    "#         agg = build_agg(v_train, keys, target=\"RETRASADO_LLEGADA\", pref=pref, smooth=20, global_mean=global_mean)\n",
    "#         agg_tables[pref] = agg\n",
    "#         Xt = left_join_agg(Xt, agg, keys, pref, global_mean)\n",
    "#         Xv = left_join_agg(Xv, agg, keys, pref, global_mean)\n",
    "\n",
    "#     # TE (build con v_train -> apply a Xt/Xv)\n",
    "#     mappings, defaults, _gm = build_target_encoding(v_train, TE_COLS, target=\"RETRASADO_LLEGADA\", smooth=20)\n",
    "#     Xt = apply_target_encoding(Xt, mappings, defaults, suffix=\"_TE\")\n",
    "#     Xv = apply_target_encoding(Xv, mappings, defaults, suffix=\"_TE\")\n",
    "\n",
    "#     # Selecci√≥n final de columnas (orden fijo = 23 features)\n",
    "#     final_cols = [\n",
    "#         \"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "#         \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\",\n",
    "#         \"AIRLINE_TE\",\"ORIGIN_AIRPORT_TE\",\"DESTINATION_AIRPORT_TE\",\"RUTA_TE\",\n",
    "#         \"AIR_rate\",\"AIR_n\",\"DES_rate\",\"DES_n\",\"ORI_rate\",\"ORI_n\",\n",
    "#         \"RUTA_rate\",\"RUTA_n\",\"RUTA_HORA_rate\",\"RUTA_HORA_n\"\n",
    "#     ]\n",
    "#     Xt = Xt[final_cols].astype(\"float32\")\n",
    "#     Xv = Xv[final_cols].astype(\"float32\")\n",
    "\n",
    "#     # liberar\n",
    "#     del v_train, v_valid\n",
    "#     gc.collect()\n",
    "\n",
    "#     return Xt, Xv, y_train, y_valid, mappings, defaults, global_mean, agg_tables, final_cols\n",
    "\n",
    "# # =====================================================\n",
    "# # 8) M√©tricas y umbrales\n",
    "# # =====================================================\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "#     y_hat = (y_prob >= thr).astype(int)\n",
    "#     acc = accuracy_score(y_true, y_hat)\n",
    "#     pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "#     rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "#     f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "#     auc = roc_auc_score(y_true, y_prob)\n",
    "#     cm  = confusion_matrix(y_true, y_hat)\n",
    "#     print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "#     print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "#     print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "#     return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# def best_f1_threshold(y_true, y_prob):\n",
    "#     best = {\"thr\": 0.5, \"f1\": -1}\n",
    "#     for thr in np.linspace(0.05, 0.5, 46):\n",
    "#         f1 = f1_score(y_true, (y_prob >= thr).astype(int), zero_division=0)\n",
    "#         if f1 > best[\"f1\"]:\n",
    "#             best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "#     return best\n",
    "\n",
    "# # =====================================================\n",
    "# # 9) Entrenamiento modelos (LGBM / XGB / RF opcional)\n",
    "# # =====================================================\n",
    "# def train_lgbm(Xt, Xv, y_train, y_valid):\n",
    "#     import lightgbm as lgb\n",
    "#     neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "#     spw = max(neg / max(pos,1), 1.0)\n",
    "#     params = dict(\n",
    "#         objective=\"binary\",\n",
    "#         learning_rate=0.05,\n",
    "#         n_estimators=12000,\n",
    "#         num_leaves=127,\n",
    "#         min_child_samples=100,\n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=0.8,\n",
    "#         reg_alpha=0.0,\n",
    "#         reg_lambda=5.0,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#         scale_pos_weight=spw\n",
    "#     )\n",
    "#     print(\"‚Üí Entrenando LightGBM‚Ä¶\")\n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     t0 = time.time()\n",
    "#     model.fit(\n",
    "#         Xt, y_train,\n",
    "#         eval_set=[(Xv, y_valid)],\n",
    "#         eval_metric=\"auc\",\n",
    "#         callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "#     )\n",
    "#     secs = time.time() - t0\n",
    "#     proba = model.predict_proba(Xv)[:,1]\n",
    "#     auc   = roc_auc_score(y_valid, proba)\n",
    "#     print(f\"‚úì Entrenado en {secs:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc:.4f}\")\n",
    "#     return model, proba, auc\n",
    "\n",
    "# def train_xgb(Xt, Xv, y_train, y_valid):\n",
    "#     import xgboost as xgb\n",
    "#     neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "#     spw = max(neg / max(pos,1), 1.0)\n",
    "#     dtrain = xgb.DMatrix(Xt, label=y_train)\n",
    "#     dvalid = xgb.DMatrix(Xv, label=y_valid)\n",
    "#     params = {\n",
    "#         \"objective\": \"binary:logistic\",\n",
    "#         \"eval_metric\": \"auc\",\n",
    "#         \"eta\": 0.05,\n",
    "#         \"max_depth\": 10,\n",
    "#         \"min_child_weight\": 10,\n",
    "#         \"subsample\": 0.8,\n",
    "#         \"colsample_bytree\": 0.8,\n",
    "#         \"lambda\": 5.0,\n",
    "#         \"alpha\": 0.0,\n",
    "#         \"scale_pos_weight\": spw,\n",
    "#         \"nthread\": -1,\n",
    "#         \"seed\": 42,\n",
    "#         # velocidad/estabilidad\n",
    "#         \"tree_method\": \"hist\"\n",
    "#     }\n",
    "#     print(\"‚Üí Entrenando XGBoost‚Ä¶\")\n",
    "#     t0 = time.time()\n",
    "#     watch = [ (dvalid, \"valid\") ]\n",
    "#     bst = xgb.train(\n",
    "#         params, dtrain,\n",
    "#         num_boost_round=2000,\n",
    "#         evals=watch,\n",
    "#         early_stopping_rounds=400,\n",
    "#         verbose_eval=200\n",
    "#     )\n",
    "#     secs = time.time() - t0\n",
    "#     proba = bst.predict(dvalid, iteration_range=(0, bst.best_iteration+1))\n",
    "#     auc   = roc_auc_score(y_valid, proba)\n",
    "#     print(f\"‚úì XGB AUC valid={auc:.4f} | best_iter={bst.best_iteration} | tiempo={secs/60:.1f} min\")\n",
    "#     return bst, proba, auc\n",
    "\n",
    "# def train_rf_optional(Xt, Xv, y_train, y_valid):\n",
    "#     \"\"\"RF opcional: desactivado por defecto desde train_full(‚Ä¶, also_rf=False).\"\"\"\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "#     spw = max(neg / max(pos,1), 1.0)\n",
    "#     class_w = {0: 1.0, 1: float(spw)}\n",
    "#     params = dict(\n",
    "#         n_estimators=400,\n",
    "#         max_depth=24,\n",
    "#         min_samples_leaf=2,\n",
    "#         min_samples_split=4,\n",
    "#         max_features=\"sqrt\",\n",
    "#         bootstrap=True,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#         class_weight=class_w\n",
    "#     )\n",
    "#     print(\"‚Üí Entrenando RandomForest (opcional)‚Ä¶\")\n",
    "#     t0 = time.time()\n",
    "#     rf = RandomForestClassifier(**params).fit(Xt, y_train)\n",
    "#     secs = time.time() - t0\n",
    "#     proba = rf.predict_proba(Xv)[:,1]\n",
    "#     auc   = roc_auc_score(y_valid, proba)\n",
    "#     print(f\"‚úì RF AUC valid={auc:.4f} | tiempo={secs/60:.1f} min\")\n",
    "#     return rf, proba, auc\n",
    "\n",
    "# # =====================================================\n",
    "# # 10) Guardado de artefactos y metadata\n",
    "# # =====================================================\n",
    "# def save_artifacts(\n",
    "#     model_lgbm, proba_lgbm, auc_lgbm,\n",
    "#     model_xgb,  proba_xgb,  auc_xgb,\n",
    "#     mappings, defaults, global_mean, agg_tables, feature_order,\n",
    "#     out_dir_models=\"models\", out_dir_art=\"artifacts\"\n",
    "# ):\n",
    "#     os.makedirs(out_dir_models, exist_ok=True)\n",
    "#     os.makedirs(out_dir_art, exist_ok=True)\n",
    "\n",
    "#     # 10.1 Modelos\n",
    "#     lgbm_path = os.path.join(out_dir_models, \"lgbm_retrasos.pkl\")\n",
    "#     dump(model_lgbm, lgbm_path)\n",
    "#     print(f\"‚úÖ LGBM guardado en: {lgbm_path}\")\n",
    "\n",
    "#     xgb_path  = os.path.join(out_dir_models, \"xgb_retrasos.json\")\n",
    "#     if model_xgb is not None:\n",
    "#         model_xgb.save_model(xgb_path)\n",
    "#         print(f\"‚úÖ XGB guardado en: {xgb_path}\")\n",
    "\n",
    "#     # 10.2 TE\n",
    "#     te_path = os.path.join(out_dir_art, \"target_encoding.pkl\")\n",
    "#     dump({\"mappings\": mappings, \"defaults\": defaults}, te_path)\n",
    "#     print(f\"‚úÖ Target Encoding guardado en: {te_path}\")\n",
    "\n",
    "#     # 10.3 Agregados -> CSV\n",
    "#     agg_index = {}\n",
    "#     for pref, agg_df in agg_tables.items():\n",
    "#         csvp = os.path.join(out_dir_art, f\"agg_{pref}.csv\")\n",
    "#         agg_df.to_csv(csvp, index=False)\n",
    "#         agg_index[pref] = csvp\n",
    "#     print(f\"‚úÖ Agregados guardados: {len(agg_index)} tablas\")\n",
    "\n",
    "#     # 10.4 Orden de features\n",
    "#     feat_path = os.path.join(out_dir_art, \"feature_order.json\")\n",
    "#     with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump({\"feature_order\": feature_order}, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"‚úÖ Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "#     # 10.5 Umbrales (F1 √≥ptimo por modelo)\n",
    "#     best_lgbm = best_f1_threshold(y_valid, proba_lgbm) if proba_lgbm is not None else {\"thr\": None}\n",
    "#     best_xgb  = best_f1_threshold(y_valid, proba_xgb)  if proba_xgb  is not None else {\"thr\": None}\n",
    "\n",
    "#     # 10.6 Metadata global\n",
    "#     meta = {\n",
    "#         \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         \"global_mean\": float(global_mean),\n",
    "#         \"models\": {\n",
    "#             \"lgbm\": {\n",
    "#                 \"path\": lgbm_path,\n",
    "#                 \"auc_valid\": float(auc_lgbm),\n",
    "#                 \"best_f1_thr\": float(best_lgbm[\"thr\"]) if best_lgbm[\"thr\"] is not None else None,\n",
    "#                 \"best_iteration\": int(getattr(model_lgbm, \"best_iteration_\", -1))\n",
    "#             },\n",
    "#             \"xgb\": {\n",
    "#                 \"path\": xgb_path if model_xgb is not None else None,\n",
    "#                 \"auc_valid\": float(auc_xgb) if auc_xgb is not None else None,\n",
    "#                 \"best_f1_thr\": float(best_xgb[\"thr\"]) if (proba_xgb is not None and best_xgb[\"thr\"] is not None) else None\n",
    "#             }\n",
    "#         },\n",
    "#         \"feature_order_path\": feat_path,\n",
    "#         \"agg_tables\": agg_index,\n",
    "#         \"te_cols\": TE_COLS,\n",
    "#         \"final_feature_order\": feature_order,\n",
    "#         \"split\": {\"train_months\":\"1-9\",\"valid_months\":\"10-12\"}\n",
    "#     }\n",
    "#     meta_path = os.path.join(out_dir_art, \"metadata.json\")\n",
    "#     with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"‚úÖ Metadatos guardados en: {meta_path}\")\n",
    "#     return {\"meta_path\": meta_path, \"lgbm_path\": lgbm_path, \"xgb_path\": xgb_path}\n",
    "\n",
    "# # =====================================================\n",
    "# # 11) Orquestador de entrenamiento completo\n",
    "# # =====================================================\n",
    "# def train_full(path_csv: str, train_models=(\"lgbm\",\"xgb\"), also_rf=False):\n",
    "#     # 11.1 Carga y features base\n",
    "#     df = load_flights(path_csv)\n",
    "#     df = add_route(df)\n",
    "#     df = ensure_time_features(df)  # respeta lo que ya viene en CSV\n",
    "#     df = ensure_distance(df)\n",
    "\n",
    "#     # 11.2 Matrices\n",
    "#     Xt, Xv, y_train, y_valid, mappings, defaults, global_mean, agg_tables, final_cols = make_matrices(df)\n",
    "#     print(f\"Shapes -> Xt: {Xt.shape} | Xv: {Xv.shape} | y_train rate={y_train.mean():.4f} | y_valid rate={y_valid.mean():.4f}\")\n",
    "\n",
    "#     # 11.3 Entrenamientos\n",
    "#     model_lgbm = proba_lgbm = auc_lgbm = None\n",
    "#     model_xgb  = proba_xgb  = auc_xgb  = None\n",
    "\n",
    "#     if \"lgbm\" in train_models:\n",
    "#         model_lgbm, proba_lgbm, auc_lgbm = train_lgbm(Xt, Xv, y_train, y_valid)\n",
    "#         # Umbrales de reporte\n",
    "#         _ = report_metrics(y_valid, proba_lgbm, 0.5, \"LGBM Base 0.5\")\n",
    "#         best = best_f1_threshold(y_valid, proba_lgbm)\n",
    "#         _ = report_metrics(y_valid, proba_lgbm, best[\"thr\"], \"LGBM Mejor F1\")\n",
    "#         # objetivos negocio (aprox)\n",
    "#         # precisi√≥n ‚â≥ 0.30\n",
    "#         thr_prec = 0.229\n",
    "#         _ = report_metrics(y_valid, proba_lgbm, thr_prec, \"LGBM Precisi√≥n ‚â≥ 0.30 (aprox)\")\n",
    "#         # recall ‚â• 0.70 (barrido r√°pido)\n",
    "#         thr_rec = 0.200\n",
    "#         _ = report_metrics(y_valid, proba_lgbm, thr_rec, \"LGBM Recall ‚â• 0.70 (aprox)\")\n",
    "\n",
    "#     if \"xgb\" in train_models:\n",
    "#         model_xgb, proba_xgb, auc_xgb = train_xgb(Xt, Xv, y_train, y_valid)\n",
    "#         _ = report_metrics(y_valid, proba_xgb, 0.5, \"XGB Base 0.5\")\n",
    "#         best = best_f1_threshold(y_valid, proba_xgb)\n",
    "#         _ = report_metrics(y_valid, proba_xgb, best[\"thr\"], \"XGB Mejor F1\")\n",
    "\n",
    "#     if also_rf:\n",
    "#         _rf, _rf_p, _rf_auc = train_rf_optional(Xt, Xv, y_train, y_valid)\n",
    "#         best = best_f1_threshold(y_valid, _rf_p)\n",
    "#         _ = report_metrics(y_valid, _rf_p, best[\"thr\"], \"RF Mejor F1 (opcional)\")\n",
    "\n",
    "#     # 11.4 Guardado de artefactos\n",
    "#     arts = save_artifacts(\n",
    "#         model_lgbm, proba_lgbm, auc_lgbm,\n",
    "#         model_xgb,  proba_xgb,  auc_xgb,\n",
    "#         mappings, defaults, global_mean, agg_tables, final_cols\n",
    "#     )\n",
    "#     return {\n",
    "#         \"lgbm_auc\": auc_lgbm, \"xgb_auc\": auc_xgb,\n",
    "#         \"models_paths\": arts, \"features\": final_cols\n",
    "#     }\n",
    "\n",
    "# # =====================================================\n",
    "# # 12) Inferencia: score_retraso(df_raw)\n",
    "# #     - Carga artefactos, rehace TE y agregados, alinea features, predice\n",
    "# # =====================================================\n",
    "# def score_retraso(df_raw: pd.DataFrame, model=\"lgbm\", threshold=None,\n",
    "#                   models_dir=\"models\", art_dir=\"artifacts\"):\n",
    "#     # Cargar metadata\n",
    "#     with open(os.path.join(art_dir, \"metadata.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "#         meta = json.load(f)\n",
    "\n",
    "#     # Feature order\n",
    "#     with open(meta[\"feature_order_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "#         feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "#     # TE\n",
    "#     te = load(os.path.join(art_dir, \"target_encoding.pkl\"))\n",
    "#     mappings, defaults = te[\"mappings\"], te[\"defaults\"]\n",
    "\n",
    "#     # Agregados guardados\n",
    "#     agg_tables = {}\n",
    "#     for pref, path in meta[\"agg_tables\"].items():\n",
    "#         agg_tables[pref] = pd.read_csv(path)\n",
    "\n",
    "#     # Preparar entrada cruda ‚Üí mismas features\n",
    "#     df = df_raw.copy()\n",
    "#     df = add_route(df)\n",
    "#     df = ensure_time_features(df)   # respeta si ya trae MINUTO_DIA_SALIDA / SALIDA_SIN/COS\n",
    "#     df = ensure_distance(df)\n",
    "\n",
    "#     # Base + TE_COLS presentes\n",
    "#     need_cols = set(BASE_FEATURES + TE_COLS)\n",
    "#     miss = need_cols - set(df.columns)\n",
    "#     if miss:\n",
    "#         raise KeyError(f\"Entrada de inferencia incompleta; faltan: {sorted(miss)}\")\n",
    "\n",
    "#     X = df[BASE_FEATURES + TE_COLS].copy()\n",
    "\n",
    "#     # Agregados (join con CSVs persistidos)\n",
    "#     # NOTA: para inferencia futura, estas tablas deber√≠an alinearse con el hist√≥rico de entrenamiento\n",
    "#     global_mean = float(meta[\"global_mean\"])\n",
    "#     # Orden y prefijos del entrenamiento\n",
    "#     agg_plan = [\n",
    "#         ([\"AIRLINE\"], \"AIR\"),\n",
    "#         ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "#         ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "#         ([\"RUTA\"], \"RUTA\"),\n",
    "#         ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "#     ]\n",
    "#     for keys, pref in agg_plan:\n",
    "#         agg = agg_tables[pref]\n",
    "#         X = X.merge(agg, on=keys, how=\"left\")\n",
    "#         X[f\"{pref}_rate\"] = X[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "#         X[f\"{pref}_n\"]    = X[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "\n",
    "#     # TE\n",
    "#     for c, mp in mappings.items():\n",
    "#         newc = f\"{c}_TE\"\n",
    "#         X[newc] = X[c].map(mp).astype(\"float32\").fillna(defaults[c]).astype(\"float32\")\n",
    "\n",
    "#     # Orden final\n",
    "#     X = X[feature_order].astype(\"float32\")\n",
    "\n",
    "#     # Cargar modelo\n",
    "#     if model.lower() == \"lgbm\":\n",
    "#         mdl = load(meta[\"models\"][\"lgbm\"][\"path\"])\n",
    "#         proba = mdl.predict_proba(X)[:,1]\n",
    "#         thr   = threshold if threshold is not None else meta[\"models\"][\"lgbm\"][\"best_f1_thr\"]\n",
    "#     elif model.lower() == \"xgb\":\n",
    "#         import xgboost as xgb\n",
    "#         bst = xgb.Booster()\n",
    "#         bst.load_model(meta[\"models\"][\"xgb\"][\"path\"])\n",
    "#         dX = xgb.DMatrix(X)\n",
    "#         proba = bst.predict(dX, iteration_range=(0, getattr(bst, \"best_iteration\", 0)+1))\n",
    "#         thr   = threshold if threshold is not None else meta[\"models\"][\"xgb\"][\"best_f1_thr\"]\n",
    "#     else:\n",
    "#         raise ValueError(\"Modelo no soportado en inferencia: usa 'lgbm' o 'xgb'.\")\n",
    "\n",
    "#     thr = 0.5 if thr is None else float(thr)\n",
    "#     yhat = (proba >= thr).astype(int)\n",
    "#     return proba, yhat, X\n",
    "\n",
    "# # =====================================================\n",
    "# # === EJECUCI√ìN (ejemplo) =============================\n",
    "# # =====================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # A) ENTRENAMIENTO COMPLETO (ajusta la ruta)\n",
    "#     BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "#     DATA_DIR = BASE_DIR\n",
    "#     # csv_path = os.path.join(DATA_DIR, \"flights_clean.csv\")\n",
    "#     csv_path = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "#     print(\"\\n===== ENTRENAMIENTO =====\")\n",
    "#     results = train_full(csv_path, train_models=(\"lgbm\",\"xgb\"), also_rf=False)\n",
    "#     print(\"Resultados:\", results)\n",
    "\n",
    "#     # B) INFERENCIA (smoke test con un ejemplo)\n",
    "#     print(\"\\n===== INFERENCIA (smoke test) =====\")\n",
    "#     ejemplo = pd.DataFrame([dict(\n",
    "#         MONTH=10, DAY_OF_WEEK=5,\n",
    "#         AIRLINE=\"AA\",\n",
    "#         ORIGIN_AIRPORT=\"JFK\", DESTINATION_AIRPORT=\"LAX\",\n",
    "#         SCHEDULED_DEPARTURE=830,  # HHMM (por si requieres derivar MINUTO_DIA_SALIDA)\n",
    "#         ORIGEN_LAT=40.6413, ORIGEN_LON=-73.7781,\n",
    "#         DEST_LAT=33.9416, DEST_LON=-118.4085,\n",
    "#         # Si ya trajeras estas columnas, se respetan:\n",
    "#         # MINUTO_DIA_SALIDA= ... , SALIDA_SIN=..., SALIDA_COS=..., HORA_SALIDA=...\n",
    "#     )])\n",
    "\n",
    "#     proba, clase, X_infer = score_retraso(ejemplo, model=\"lgbm\")\n",
    "#     print(pd.DataFrame({\"proba_retraso\": proba.round(4), \"pred_clase\": clase}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0384d3",
   "metadata": {},
   "source": [
    "üìí Celda 1 ‚Äî Importaciones y configuraci√≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c6249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando CSV: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1) Importaciones y configuraci√≥n\n",
    "# ================================\n",
    "import os, json, time, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Rutas base (ajusta si lo necesitas)\n",
    "PROJECT_ROOT = Path(r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\")\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
    "ARTIF_DIR    = PROJECT_ROOT / \"artifacts\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ruta ABSOLUTA al CSV (tu ruta)\n",
    "csv_path = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "print(\"Usando CSV:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21ce35",
   "metadata": {},
   "source": [
    "üìí Celda 2 ‚Äî Utilidades (downcast, haversine, helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e5bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2) Utilidades: downcast, haversine, helpers\n",
    "# ==========================================\n",
    "\n",
    "def downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reduce memoria en num√©ricos sin cambiar valores.\"\"\"\n",
    "    for c in df.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Distancia haversine en KM (vect.)\"\"\"\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029c4f1",
   "metadata": {},
   "source": [
    "üìí Celda 3 ‚Äî Carga y features (respetando columnas ya presentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e14d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 3) Carga flights_clean y features (NO recalcula si ya existen)\n",
    "# ===========================================================\n",
    "\n",
    "RAW_REQ_COLS = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"SCHEDULED_ARRIVAL\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# Features que podr√≠amos derivar, pero respeta si ya vienen calculadas en el CSV\n",
    "DERIVED_COLS = [\n",
    "    \"MINUTO_DIA_SALIDA\",\"MINUTO_DIA_LLEGADA\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\"LLEGADA_SIN\",\"LLEGADA_COS\",\n",
    "    \"HORA_SALIDA\",\"HORA_LLEGADA\",\n",
    "    \"DISTANCIA\"\n",
    "]\n",
    "\n",
    "def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(path_csv)\n",
    "    print(f\"‚úì Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # Downcast seguro\n",
    "    df = downcast_numeric(df)\n",
    "\n",
    "    # Validaci√≥n m√≠nima\n",
    "    for c in RAW_REQ_COLS:\n",
    "        if c not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Falta columna esperada en CSV: {c}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"RUTA = ORIGIN -> DEST\"\"\"\n",
    "    if \"RUTA\" not in df.columns:\n",
    "        df[\"RUTA\"] = (df[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + df[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    return df\n",
    "\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Si faltan se√±ales temporales, las crea. Si ya existen, no toca nada.\"\"\"\n",
    "    if \"MINUTO_DIA_SALIDA\" not in df.columns and \"SCHEDULED_DEPARTURE\" in df.columns:\n",
    "        # SCHEDULED_DEPARTURE viene HHMM; construimos minuto del d√≠a\n",
    "        hh = (df[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "        mm = (df[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "        df[\"MINUTO_DIA_SALIDA\"] = hh*60 + mm\n",
    "\n",
    "    # Se√±ales sin/cos por salida\n",
    "    if (\"SALIDA_SIN\" not in df.columns or \"SALIDA_COS\" not in df.columns) and \"MINUTO_DIA_SALIDA\" in df.columns:\n",
    "        rad = 2*np.pi*(df[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        df[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "        df[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "    # Hora de salida si falta\n",
    "    if \"HORA_SALIDA\" not in df.columns and \"SCHEDULED_DEPARTURE\" in df.columns:\n",
    "        df[\"HORA_SALIDA\"] = (df[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23).astype(\"int16\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Asegura DISTANCIA: usa DISTANCE si est√°, si no calcula haversine.\"\"\"\n",
    "    if \"DISTANCIA\" not in df.columns:\n",
    "        if \"DISTANCE\" in df.columns:\n",
    "            df[\"DISTANCIA\"] = pd.to_numeric(df[\"DISTANCE\"], errors=\"coerce\")\n",
    "        else:\n",
    "            df[\"DISTANCIA\"] = haversine_km(df[\"ORIGEN_LAT\"], df[\"ORIGEN_LON\"], df[\"DEST_LAT\"], df[\"DEST_LON\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c2c1f",
   "metadata": {},
   "source": [
    "üìí Celda 4 ‚Äî Target Encoding (con suavizado) y Agregados hist√≥ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb58f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 4) Target Encoding (suavizado) y Agregados hist√≥ricos\n",
    "# ===================================================\n",
    "\n",
    "# Columnas categ√≥ricas a codificar por TE\n",
    "TE_COLS = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "\n",
    "def build_target_encoding(df, cols=TE_COLS, target=\"RETRASADO_LLEGADA\", smooth=20):\n",
    "    \"\"\"\n",
    "    Retorna 'mappings' (dict por col -> dict categor√≠a:media_suavizada)\n",
    "            y 'defaults' (dict por col -> media global) para inferencia.\n",
    "    \"\"\"\n",
    "    mappings, defaults = {}, {}\n",
    "    global_mean = df[target].mean()\n",
    "    for c in cols:\n",
    "        g = df.groupby(c)[target]\n",
    "        m = g.mean()\n",
    "        n = g.size()\n",
    "        # suavizado tipo m-estimate\n",
    "        enc = (m*n + global_mean*smooth) / (n + smooth)\n",
    "        mappings[c] = enc.to_dict()\n",
    "        defaults[c] = float(global_mean)\n",
    "    return mappings, defaults\n",
    "\n",
    "def apply_target_encoding(df, mappings, defaults, cols=TE_COLS, suffix=\"_TE\"):\n",
    "    \"\"\"Aplica TE usando mappings/defaults (para train/valid y luego inferencia).\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        m = mappings.get(c, {})\n",
    "        d = defaults.get(c, np.nan)\n",
    "        out[c+suffix] = out[c].map(m).fillna(d).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# -------- Agregados hist√≥ricos (sin fuga) --------\n",
    "\n",
    "# Especificaciones: (keys, prefijo)\n",
    "AGGS_SPECS = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "]\n",
    "\n",
    "def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"X\", smooth=20):\n",
    "    \"\"\"\n",
    "    Construye tabla de agregados con media suavizada y conteo:\n",
    "    -> columnas: keys + [f\"{pref}_rate\", f\"{pref}_n\"]\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "    global_mean = df[target].mean()\n",
    "    rate = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "    out = rate.reset_index().rename(columns={target: f\"{pref}_rate\"})\n",
    "    out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "    return out\n",
    "\n",
    "def apply_aggregates(df, agg_tables):\n",
    "    \"\"\"\n",
    "    Hace left-join de cada tabla de agregados en df.\n",
    "    agg_tables: dict {pref -> (keys, agg_df)}\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for pref, (keys, agg_df) in agg_tables.items():\n",
    "        out = out.merge(agg_df, on=keys, how=\"left\")\n",
    "        # Si quedan NaN en rate/n, rellena cero (categor√≠as nuevas)\n",
    "        out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(out[f\"{pref}_rate\"].mean()).astype(\"float32\")\n",
    "        out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddcc83",
   "metadata": {},
   "source": [
    "üìí Celda 5 ‚Äî Construcci√≥n de matrices (train/valid), TE + agregados y selecci√≥n de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e0f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Construcci√≥n de matrices, TE + agregados y selecci√≥n final\n",
    "# ============================================================\n",
    "\n",
    "FEATURES_BASE = [\n",
    "    # temporales\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\"HORA_SALIDA\",\"MINUTO_DIA_SALIDA\",\"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    # distancia\n",
    "    \"DISTANCIA\",\n",
    "]\n",
    "\n",
    "def build_matrices(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Split temporal sin fuga:\n",
    "      - TRAIN: MONTH in [1..9]\n",
    "      - VALID: MONTH in [10..12]\n",
    "    Aplica TE (mappings con TRAIN) y agregados (con TRAIN).\n",
    "    Devuelve: X_train_model, X_valid_model, y_train, y_valid, metadata √∫til.\n",
    "    \"\"\"\n",
    "    # 1) Asegurar columnas clave\n",
    "    df = add_route(df)\n",
    "    df = ensure_time_features(df)\n",
    "    df = ensure_distance(df)\n",
    "\n",
    "    # 2) Split temporal\n",
    "    train_mask = df[\"MONTH\"].between(1, 9)\n",
    "    valid_mask = df[\"MONTH\"].between(10, 12)\n",
    "\n",
    "    v_train = df.loc[train_mask].copy()\n",
    "    v_valid = df.loc[valid_mask].copy()\n",
    "\n",
    "    y_train = v_train[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "    y_valid = v_valid[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "\n",
    "    # 3) Target Encoding (con TRAIN)\n",
    "    mappings, defaults = build_target_encoding(v_train, cols=TE_COLS, target=\"RETRASADO_LLEGADA\", smooth=20)\n",
    "    v_train = apply_target_encoding(v_train, mappings, defaults, cols=TE_COLS, suffix=\"_TE\")\n",
    "    v_valid = apply_target_encoding(v_valid, mappings, defaults, cols=TE_COLS, suffix=\"_TE\")\n",
    "\n",
    "    # 4) Agregados hist√≥ricos (con TRAIN)\n",
    "    agg_tables = {}\n",
    "    for keys, pref in AGGS_SPECS:\n",
    "        agg_df = build_agg(v_train, keys, target=\"RETRASADO_LLEGADA\", pref=pref, smooth=20)\n",
    "        agg_tables[pref] = (keys, agg_df)\n",
    "\n",
    "    v_train = apply_aggregates(v_train, agg_tables)\n",
    "    v_valid = apply_aggregates(v_valid, agg_tables)\n",
    "\n",
    "    # 5) Selecci√≥n de features finales\n",
    "    te_feats = [c+\"_TE\" for c in TE_COLS]\n",
    "    agg_feats = []\n",
    "    for _, pref in AGGS_SPECS:\n",
    "        agg_feats += [f\"{pref}_rate\", f\"{pref}_n\"]\n",
    "\n",
    "    all_feats = FEATURES_BASE + te_feats + agg_feats\n",
    "\n",
    "    # Alinear y asegurar tipos\n",
    "    X_train_model = v_train[all_feats].copy()\n",
    "    X_valid_model = v_valid[all_feats].copy()\n",
    "\n",
    "    for c in X_train_model.select_dtypes(include=[\"float64\"]).columns:\n",
    "        X_train_model[c] = X_train_model[c].astype(\"float32\")\n",
    "    for c in X_valid_model.select_dtypes(include=[\"float64\"]).columns:\n",
    "        X_valid_model[c] = X_valid_model[c].astype(\"float32\")\n",
    "\n",
    "    meta = dict(\n",
    "        features=all_feats,\n",
    "        mappings=mappings,\n",
    "        defaults=defaults,\n",
    "        agg_tables=agg_tables,\n",
    "        train_rate=float(y_train.mean()),\n",
    "        valid_rate=float(y_valid.mean())\n",
    "    )\n",
    "    print(\"Shapes ->\", X_train_model.shape, X_valid_model.shape, \"| rate train\", meta[\"train_rate\"], \"| rate valid\", meta[\"valid_rate\"])\n",
    "    return X_train_model, X_valid_model, y_train, y_valid, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9eba6",
   "metadata": {},
   "source": [
    "üìí Celda 6 ‚Äî Entrenamiento LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0ddbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 6) Entrenamiento LightGBM (validaci√≥n)\n",
    "# ======================================\n",
    "\n",
    "def train_lgbm(Xtr, ytr, Xv, yv):\n",
    "    neg = int((ytr==0).sum()); pos = int((ytr==1).sum())\n",
    "    scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=12000,\n",
    "        num_leaves=127,\n",
    "        min_child_samples=100,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        reg_alpha=0.0, reg_lambda=5.0,\n",
    "        n_jobs=-1, random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=[(Xv, yv)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = model.predict_proba(Xv)[:,1]\n",
    "    auc   = roc_auc_score(yv, proba)\n",
    "    print(f\"‚úì LGBM entrenado en {secs:.1f}s | best_iter={model.best_iteration_} | AUC valid={auc:.4f}\")\n",
    "    return model, proba, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6040fb",
   "metadata": {},
   "source": [
    "üìí Celda 7 ‚Äî Entrenamiento XGBoost (peque√±a b√∫squeda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb82aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 7) Entrenamiento XGBoost (grid peque√±o)\n",
    "# ======================================\n",
    "\n",
    "def train_xgb(Xtr, ytr, Xv, yv):\n",
    "    dtr = xgb.DMatrix(Xtr, label=ytr)\n",
    "    dv  = xgb.DMatrix(Xv,  label=yv)\n",
    "\n",
    "    neg = int((ytr==0).sum()); pos = int((ytr==1).sum())\n",
    "    spw = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "    grids = [\n",
    "        dict(eta=0.05, max_depth=10, min_child_weight=10, subsample=0.8, colsample_bytree=0.8),\n",
    "        dict(eta=0.05, max_depth=8,  min_child_weight=5,  subsample=0.8, colsample_bytree=0.8),\n",
    "        dict(eta=0.03, max_depth=10, min_child_weight=10, subsample=0.9, colsample_bytree=0.9),\n",
    "    ]\n",
    "    best = {\"auc\":-1, \"model\":None, \"proba\":None, \"params\":None, \"best_iter\":None}\n",
    "\n",
    "    for g in grids:\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "            eta=g[\"eta\"], max_depth=g[\"max_depth\"], min_child_weight=g[\"min_child_weight\"],\n",
    "            subsample=g[\"subsample\"], colsample_bytree=g[\"colsample_bytree\"],\n",
    "            reg_lambda=5.0, reg_alpha=0.0,\n",
    "            scale_pos_weight=spw, nthread=-1, seed=42\n",
    "        )\n",
    "        print(\"Probando XGB:\", params)\n",
    "        t0 = time.time()\n",
    "        mdl = xgb.train(\n",
    "            params, dtr,\n",
    "            num_boost_round=4000,\n",
    "            evals=[(dv,\"valid\")],\n",
    "            early_stopping_rounds=400,\n",
    "            verbose_eval=400\n",
    "        )\n",
    "        secs = time.time()-t0\n",
    "        proba = mdl.predict(dv, iteration_range=(0, mdl.best_iteration+1))\n",
    "        auc   = roc_auc_score(yv, proba)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best[\"auc\"]:\n",
    "            best = {\"auc\": auc, \"model\": mdl, \"proba\": proba, \"params\": params, \"best_iter\": mdl.best_iteration}\n",
    "    print(\"=== XGB MEJOR ===\")\n",
    "    print(best[\"params\"], \"| AUC:\", best[\"auc\"], \"| best_iter:\", best[\"best_iter\"])\n",
    "    return best[\"model\"], best[\"proba\"], best[\"auc\"], best[\"params\"], best[\"best_iter\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f29b82",
   "metadata": {},
   "source": [
    "üìí Celda ‚Äú7.5‚Äù ‚Äî Random Forest (entrenar y evaluar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "550d58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# 7.5) Random Forest (sklearn): entrenamiento\n",
    "# ======================================\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_rf(Xtr, ytr, Xv, yv):\n",
    "    \"\"\"\n",
    "    Entrena un RandomForest con par√°metros conservadores para evitar OOM (memoria).\n",
    "    Devuelve: (modelo, proba_valid, auc_valid, params_usados)\n",
    "    \"\"\"\n",
    "    # Desbalance: usa ponderaci√≥n autom√°tica por clase en cada bootstrap\n",
    "    base_params = dict(\n",
    "        n_estimators=300,               # si tienes m√°s tiempo/RAM: 400‚Äì600\n",
    "        max_depth=22,                   # limitar profundidad evita explosi√≥n de memoria\n",
    "        min_samples_leaf=3,\n",
    "        min_samples_split=6,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        max_samples=0.6,                # submuestreo por √°rbol (reduce RAM y overfitting)\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(**base_params)\n",
    "    t0 = time.time()\n",
    "    rf.fit(Xtr, ytr)\n",
    "    secs = time.time() - t0\n",
    "\n",
    "    proba = rf.predict_proba(Xv)[:, 1]\n",
    "    auc   = roc_auc_score(yv, proba)\n",
    "    print(f\"‚úì RF entrenado en {secs/60:.1f} min | AUC valid={auc:.4f}\")\n",
    "    return rf, proba, auc, base_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756db52",
   "metadata": {},
   "source": [
    "üìí Celda 8 ‚Äî M√©tricas r√°pidas y elecci√≥n del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abea503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 8) M√©tricas, b√∫squeda de umbral y elecci√≥n del mejor\n",
    "# ======================================================\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.5, steps=46):\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        y_hat = (y_prob >= thr).astype(int)\n",
    "        f1 = f1_score(y_true, y_hat, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf49cb",
   "metadata": {},
   "source": [
    "üìí Celda 9 ‚Äî Entrenamiento completo (carga ‚Üí matrices ‚Üí LGBM & XGB ‚Üí guardado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a2b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 9) Pipeline de entrenamiento completo y guardado de artefactos\n",
    "#     (ahora soporta: LGBM, XGB y RF)\n",
    "# ===========================================================\n",
    "\n",
    "def train_full(path_csv: str, train_models=(\"lgbm\",\"xgb\",\"rf\")):\n",
    "    # 9.1 Carga y features base (respeta columnas ya calculadas en el CSV)\n",
    "    df = load_flights(path_csv)\n",
    "    df = add_route(df)\n",
    "    df = ensure_time_features(df)\n",
    "    df = ensure_distance(df)\n",
    "\n",
    "    # 9.2 Construcci√≥n de matrices\n",
    "    Xtr, Xv, ytr, yv, meta = build_matrices(df)\n",
    "    feature_order = meta[\"features\"]\n",
    "\n",
    "    # 9.3 Entrenamientos seg√∫n selecci√≥n\n",
    "    results = {}\n",
    "\n",
    "    if \"lgbm\" in train_models:\n",
    "        lgbm_model, lgbm_proba, lgbm_auc = train_lgbm(Xtr, ytr, Xv, yv)\n",
    "        lgbm_best = best_f1_threshold(yv, lgbm_proba)\n",
    "        report_metrics(yv, lgbm_proba, 0.5, \"LGBM Base 0.5\")\n",
    "        report_metrics(yv, lgbm_proba, lgbm_best[\"thr\"], \"LGBM Mejor F1\")\n",
    "        results[\"lgbm\"] = dict(auc=lgbm_auc, proba=lgbm_proba, model=lgbm_model, best_thr=lgbm_best)\n",
    "\n",
    "    if \"xgb\" in train_models:\n",
    "        xgb_model, xgb_proba, xgb_auc, xgb_params, xgb_best_iter = train_xgb(Xtr, ytr, Xv, yv)\n",
    "        xgb_best = best_f1_threshold(yv, xgb_proba)\n",
    "        report_metrics(yv, xgb_proba, 0.5, \"XGB Base 0.5\")\n",
    "        report_metrics(yv, xgb_proba, xgb_best[\"thr\"], \"XGB Mejor F1\")\n",
    "        results[\"xgb\"] = dict(auc=xgb_auc, proba=xgb_proba, model=xgb_model, best_thr=xgb_best,\n",
    "                              params=xgb_params, best_iter=xgb_best_iter)\n",
    "\n",
    "    if \"rf\" in train_models:\n",
    "        rf_model, rf_proba, rf_auc, rf_params = train_rf(Xtr, ytr, Xv, yv)\n",
    "        rf_best = best_f1_threshold(yv, rf_proba)\n",
    "        report_metrics(yv, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "        report_metrics(yv, rf_proba, rf_best[\"thr\"], \"RF Mejor F1\")\n",
    "        results[\"rf\"] = dict(auc=rf_auc, proba=rf_proba, model=rf_model, best_thr=rf_best, params=rf_params)\n",
    "\n",
    "    # 9.4 Elegir mejor por AUC\n",
    "    best_name = max(results.keys(), key=lambda k: results[k][\"auc\"])\n",
    "    best_info = results[best_name]\n",
    "    print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_info['auc']:.4f} ===\")\n",
    "\n",
    "    # 9.5 Guardado de artefactos comunes\n",
    "    te_path = ARTIF_DIR / \"target_encoding.pkl\"\n",
    "    joblib.dump({\"mappings\": meta[\"mappings\"], \"defaults\": meta[\"defaults\"]}, te_path)\n",
    "\n",
    "    agg_registry = {}\n",
    "    for pref, (keys, agg_df) in meta[\"agg_tables\"].items():\n",
    "        out_csv = ARTIF_DIR / f\"agg_{pref}.csv\"\n",
    "        agg_df.to_csv(out_csv, index=False)\n",
    "        agg_registry[pref] = dict(keys=keys, path=str(out_csv))\n",
    "\n",
    "    feat_path = ARTIF_DIR / \"feature_order.json\"\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": feature_order}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 9.6 Guardar modelo seg√∫n tipo\n",
    "    if best_name == \"lgbm\":\n",
    "        model_path = MODELS_DIR / \"lgbm_retrasos.pkl\"\n",
    "        joblib.dump(best_info[\"model\"], model_path)\n",
    "        best_iter = int(getattr(best_info[\"model\"], \"best_iteration_\", 0) or 0)\n",
    "\n",
    "    elif best_name == \"xgb\":\n",
    "        model_path = MODELS_DIR / \"xgb_retrasos.model\"\n",
    "        best_info[\"model\"].save_model(str(model_path))\n",
    "        best_iter = int(best_info.get(\"best_iter\", 0) or 0)\n",
    "\n",
    "    else:  # rf\n",
    "        model_path = MODELS_DIR / \"rf_retrasos.pkl\"\n",
    "        joblib.dump(best_info[\"model\"], model_path)\n",
    "        best_iter = 0  # RF no tiene \"best_iteration\"\n",
    "\n",
    "    meta_json = {\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_type\": best_name,                      # \"lgbm\" | \"xgb\" | \"rf\"\n",
    "        \"model_path\": str(model_path),\n",
    "        \"best_iter\": best_iter,\n",
    "        \"auc_valid\": float(best_info[\"auc\"]),\n",
    "        \"best_thr_f1\": float(best_info[\"best_thr\"][\"thr\"]),\n",
    "        \"feature_order_path\": str(feat_path),\n",
    "        \"te_path\": str(te_path),\n",
    "        \"aggs\": agg_registry,\n",
    "        \"te_cols\": TE_COLS\n",
    "    }\n",
    "    meta_path = ARTIF_DIR / \"metadata.json\"\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n‚úÖ Artefactos guardados:\")\n",
    "    print(\" - Modelo:\", model_path)\n",
    "    print(\" - TE:\", te_path)\n",
    "    print(\" - Feature order:\", feat_path)\n",
    "    print(\" - Metadatos:\", meta_path)\n",
    "    print(\" - Agregados:\", list(agg_registry.keys()))\n",
    "    return dict(best=best_name, auc=best_info[\"auc\"], thr=best_info[\"best_thr\"][\"thr\"], model_path=str(model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b340f54",
   "metadata": {},
   "source": [
    "üìí Celda 10 (reemplazo) ‚Äî Inferencia compatible con RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddeb026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 10) Inferencia: carga artefactos y funci√≥n de scoring (LGBM/XGB/RF)\n",
    "# ===========================================================\n",
    "\n",
    "def load_artifacts():\n",
    "    with open(ARTIF_DIR / \"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        META = json.load(f)\n",
    "\n",
    "    with open(ARTIF_DIR / \"feature_order.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "    te_bundle = joblib.load(ARTIF_DIR / \"target_encoding.pkl\")\n",
    "    mappings = te_bundle[\"mappings\"]; defaults = te_bundle[\"defaults\"]\n",
    "\n",
    "    # Agregados\n",
    "    agg_tables = {}\n",
    "    for pref, info in META[\"aggs\"].items():\n",
    "        keys = info[\"keys\"]\n",
    "        agg_df = pd.read_csv(info[\"path\"])\n",
    "        agg_tables[pref] = (keys, agg_df)\n",
    "\n",
    "    # Modelo\n",
    "    mtype = META[\"model_type\"]\n",
    "    if mtype == \"lgbm\":\n",
    "        model = joblib.load(META[\"model_path\"])\n",
    "    elif mtype == \"xgb\":\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(META[\"model_path\"])\n",
    "        model = booster\n",
    "    else:  # rf\n",
    "        model = joblib.load(META[\"model_path\"])\n",
    "\n",
    "    return META, feature_order, mappings, defaults, agg_tables, model\n",
    "\n",
    "def score_retraso(df_raw: pd.DataFrame, threshold=None, model_type=None):\n",
    "    \"\"\"\n",
    "    df_raw: DataFrame con columnas m√≠nimas (ver RAW_REQ_COLS).\n",
    "    threshold: si None, usa best_thr_f1.\n",
    "    model_type: si None, usa el del metadata (lgbm/xgb/rf).\n",
    "    \"\"\"\n",
    "    META, feature_order, mappings, defaults, agg_tables, model = load_artifacts()\n",
    "    if threshold is None:\n",
    "        threshold = META[\"best_thr_f1\"]\n",
    "    if model_type is None:\n",
    "        model_type = META[\"model_type\"]\n",
    "\n",
    "    # 1) Asegurar features base\n",
    "    df = df_raw.copy()\n",
    "    df = add_route(df)\n",
    "    df = ensure_time_features(df)\n",
    "    df = ensure_distance(df)\n",
    "\n",
    "    # 2) Target Encoding\n",
    "    df = apply_target_encoding(df, mappings, defaults, cols=META[\"te_cols\"], suffix=\"_TE\")\n",
    "\n",
    "    # 3) Agregados hist√≥ricos\n",
    "    df = apply_aggregates(df, agg_tables)\n",
    "\n",
    "    # 4) Matriz final\n",
    "    X = df[feature_order].copy().astype(\"float32\")\n",
    "\n",
    "    # 5) Predicci√≥n por tipo\n",
    "    if model_type == \"lgbm\":\n",
    "        proba = model.predict_proba(X, num_iteration=getattr(model, \"best_iteration_\", None))[:,1]\n",
    "    elif model_type == \"xgb\":\n",
    "        dmat = xgb.DMatrix(X)\n",
    "        best_iter = META.get(\"best_iter\", 0) or 0\n",
    "        proba = model.predict(dmat, iteration_range=(0, best_iter)) if best_iter > 0 else model.predict(dmat)\n",
    "    else:  # rf\n",
    "        proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    yhat = (proba >= threshold).astype(int)\n",
    "    return proba, yhat, X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29037668",
   "metadata": {},
   "source": [
    "Para entrenar con los tres modelos y que el sistema elija autom√°ticamente el mejor por AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432f9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cargado: (5231130, 41) | en 46.9s\n",
      "Shapes -> (4299046, 21) (932084, 21) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3533\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.598761\tvalid_0's binary_logloss: 0.557709\n",
      "[400]\tvalid_0's auc: 0.598581\tvalid_0's binary_logloss: 0.558715\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "‚úì LGBM entrenado en 187.8s | best_iter=1 | AUC valid=0.6006\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6006\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.3441 | Precision: 0.1915 | Recall: 0.8691 | F1: 0.3139 | ROC-AUC: 0.6006\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[180860 590335]\n",
      " [ 21059 139830]]\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58792\n",
      "[400]\tvalid-auc:0.59664\n",
      "[402]\tvalid-auc:0.59666\n",
      "AUC valid=0.6062 | best_iter=2 | tiempo=4.7 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58954\n",
      "[400]\tvalid-auc:0.59806\n",
      "[401]\tvalid-auc:0.59805\n",
      "AUC valid=0.6059 | best_iter=2 | tiempo=4.0 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58822\n",
      "[400]\tvalid-auc:0.59670\n",
      "[402]\tvalid-auc:0.59670\n",
      "AUC valid=0.6041 | best_iter=2 | tiempo=4.5 min\n",
      "\n",
      "=== XGB MEJOR ===\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42} | AUC: 0.6062161779601237 | best_iter: 2\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7111 | Precision: 0.2445 | Recall: 0.3224 | F1: 0.2781 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[610958 160237]\n",
      " [109020  51869]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4841 | Precision: 0.2084 | Recall: 0.7106 | F1: 0.3222 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[336857 434338]\n",
      " [ 46566 114323]]\n",
      "‚úì RF entrenado en 15.1 min | AUC valid=0.5922\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7898 | Precision: 0.2574 | Recall: 0.1155 | F1: 0.1595 | ROC-AUC: 0.5922\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[717591  53604]\n",
      " [142305  18584]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.230) ==\n",
      "Accuracy: 0.4813 | Precision: 0.2041 | Recall: 0.6917 | F1: 0.3152 | ROC-AUC: 0.5922\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[337293 433902]\n",
      " [ 49610 111279]]\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6062 ===\n",
      "\n",
      "‚úÖ Artefactos guardados:\n",
      " - Modelo: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\xgb_retrasos.model\n",
      " - TE: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\target_encoding.pkl\n",
      " - Feature order: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\feature_order.json\n",
      " - Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata.json\n",
      " - Agregados: ['AIR', 'DES', 'ORI', 'RUTA', 'RUTA_HORA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best': 'xgb',\n",
       " 'auc': 0.6062161779601237,\n",
       " 'thr': 0.48,\n",
       " 'model_path': 'D:\\\\OneDrive\\\\DOCUMENTOS\\\\Personales\\\\2024\\\\uniandes\\\\8 S\\\\seminario\\\\g11-caso-estudio-flights\\\\models\\\\xgb_retrasos.model'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = train_full(csv_path, train_models=(\"lgbm\",\"xgb\",\"rf\"))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32bad5",
   "metadata": {},
   "source": [
    "***Resultados*\n",
    "\n",
    "‚úì Cargado: (5231130, 41) | en 46.9s\n",
    "Shapes -> (4299046, 21) (932084, 21) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n",
    "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470835 seconds.\n",
    "You can set `force_col_wise=true` to remove the overhead.\n",
    "[LightGBM] [Info] Total Bins 3533\n",
    "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 21\n",
    "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
    "[LightGBM] [Info] Start training from score -1.467405\n",
    "Training until validation scores don't improve for 400 rounds\n",
    "[200]\tvalid_0's auc: 0.598761\tvalid_0's binary_logloss: 0.557709\n",
    "[400]\tvalid_0's auc: 0.598581\tvalid_0's binary_logloss: 0.558715\n",
    "Early stopping, best iteration is:\n",
    "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
    "‚úì LGBM entrenado en 187.8s | best_iter=1 | AUC valid=0.6006\n",
    "\n",
    "== LGBM Base 0.5 (thr=0.500) ==\n",
    "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6006\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[771195      0]\n",
    " [160889      0]]\n",
    "\n",
    "== LGBM Mejor F1 (thr=0.190) ==\n",
    "Accuracy: 0.3441 | Precision: 0.1915 | Recall: 0.8691 | F1: 0.3139 | ROC-AUC: 0.6006\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[180860 590335]\n",
    " [ 21059 139830]]\n",
    "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
    "[0]\tvalid-auc:0.58792\n",
    "[400]\tvalid-auc:0.59664\n",
    "[402]\tvalid-auc:0.59666\n",
    "AUC valid=0.6062 | best_iter=2 | tiempo=4.7 min\n",
    "\n",
    "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
    "[0]\tvalid-auc:0.58954\n",
    "[400]\tvalid-auc:0.59806\n",
    "[401]\tvalid-auc:0.59805\n",
    "AUC valid=0.6059 | best_iter=2 | tiempo=4.0 min\n",
    "\n",
    "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
    "[0]\tvalid-auc:0.58822\n",
    "[400]\tvalid-auc:0.59670\n",
    "[402]\tvalid-auc:0.59670\n",
    "AUC valid=0.6041 | best_iter=2 | tiempo=4.5 min\n",
    "\n",
    "=== XGB MEJOR ===\n",
    "{'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42} | AUC: 0.6062161779601237 | best_iter: 2\n",
    "\n",
    "== XGB Base 0.5 (thr=0.500) ==\n",
    "Accuracy: 0.7111 | Precision: 0.2445 | Recall: 0.3224 | F1: 0.2781 | ROC-AUC: 0.6062\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[610958 160237]\n",
    " [109020  51869]]\n",
    "\n",
    "== XGB Mejor F1 (thr=0.480) ==\n",
    "Accuracy: 0.4841 | Precision: 0.2084 | Recall: 0.7106 | F1: 0.3222 | ROC-AUC: 0.6062\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[336857 434338]\n",
    " [ 46566 114323]]\n",
    "‚úì RF entrenado en 15.1 min | AUC valid=0.5922\n",
    "\n",
    "== RF Base 0.5 (thr=0.500) ==\n",
    "Accuracy: 0.7898 | Precision: 0.2574 | Recall: 0.1155 | F1: 0.1595 | ROC-AUC: 0.5922\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[717591  53604]\n",
    " [142305  18584]]\n",
    "\n",
    "== RF Mejor F1 (thr=0.230) ==\n",
    "Accuracy: 0.4813 | Precision: 0.2041 | Recall: 0.6917 | F1: 0.3152 | ROC-AUC: 0.5922\n",
    "CM [TN, FP; FN, TP]:\n",
    " [[337293 433902]\n",
    " [ 49610 111279]]\n",
    "\n",
    "=== MEJOR MODELO: XGB | AUC=0.6062 ===\n",
    "\n",
    "‚úÖ Artefactos guardados:\n",
    " - Modelo: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\xgb_retrasos.model\n",
    " - TE: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\target_encoding.pkl\n",
    " - Feature order: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\feature_order.json\n",
    " - Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata.json\n",
    " - Agregados: ['AIR', 'DES', 'ORI', 'RUTA', 'RUTA_HORA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f8a36",
   "metadata": {},
   "source": [
    "{'best': 'xgb',\n",
    " 'auc': 0.6062161779601237,\n",
    " 'thr': 0.48,\n",
    " 'model_path': 'D:\\\\OneDrive\\\\DOCUMENTOS\\\\Personales\\\\2024\\\\uniandes\\\\8 S\\\\seminario\\\\g11-caso-estudio-flights\\\\models\\\\xgb_retrasos.model'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e4ffd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
