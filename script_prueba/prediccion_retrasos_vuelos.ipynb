{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0a1e6",
   "metadata": {},
   "source": [
    "\n",
    "# PredicciÃ³n de Retrasos de Vuelos en la Industria AÃ©rea âœˆï¸\n",
    "\n",
    "**Objetivo:** Construir un modelo que **prediga si un vuelo sufrirÃ¡ un retraso de llegada mayor a 15 minutos** (`RETRASADO_LLEGADA` = 1).  \n",
    "Este notebook sigue la estructura de `machine_learning.ipynb`, con secciones de carga de datos, EDA, preparaciÃ³n, entrenamiento con **LightGBM**, evaluaciÃ³n y conclusiones.\n",
    "\n",
    "**Dataset de entrada:** `data/processed/flights_clean.csv` (resultado del pipeline de limpieza e ingenierÃ­a de caracterÃ­sticas).  \n",
    "**TamaÃ±o esperado:** ~5.2M filas (podrÃ­a requerir >8GB RAM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481317d",
   "metadata": {},
   "source": [
    "## 1. Importaciones y configuraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc87b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"LightGBM no estÃ¡ instalado. Instala con: pip install lightgbm\") from e\n",
    "\n",
    "# ConfiguraciÃ³n visual\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb56eb",
   "metadata": {},
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133005",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eafda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.read_csv(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59569525",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feab64",
   "metadata": {},
   "source": [
    "## 3. InspecciÃ³n rÃ¡pida de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v.info(memory_usage='deep', show_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b78f3",
   "metadata": {},
   "source": [
    "## 4. DistribuciÃ³n de retrasos (llegada > 15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert \"RETRASADO_LLEGADA\" in v.columns, \"No existe la columna RETRASADO_LLEGADA en el dataset limpio.\"\n",
    "\n",
    "conteo = v[\"RETRASADO_LLEGADA\"].value_counts().sort_index()\n",
    "porc = (conteo / conteo.sum() * 100).round(2)\n",
    "\n",
    "print(\"ğŸ“Š DistribuciÃ³n de vuelos segÃºn retraso en llegada (>15 min):\\n\")\n",
    "print(f\"A tiempo (0): {conteo.get(0,0):,} vuelos ({porc.get(0,0):.2f}%)\")\n",
    "print(f\"Retrasados (1): {conteo.get(1,0):,} vuelos ({porc.get(1,0):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509f74a",
   "metadata": {},
   "source": [
    "## 5. SelecciÃ³n de variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf346a",
   "metadata": {},
   "source": [
    "\n",
    "Usaremos variables **categÃ³ricas y de tiempo** ya generadas en el pipeline:\n",
    "\n",
    "- `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT` (categÃ³ricas)\n",
    "- `MONTH`, `DAY_OF_WEEK` (tiempo)\n",
    "- CodificaciÃ³n cÃ­clica: `SALIDA_SIN`, `SALIDA_COS` (derivadas de la hora programada de salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\"\n",
    "]\n",
    "\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "# Liberar memoria del dataframe original si es necesario\n",
    "del v\n",
    "gc.collect()\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c69ea",
   "metadata": {},
   "source": [
    "## 6. CodificaciÃ³n de variables categÃ³ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430587be",
   "metadata": {},
   "source": [
    "\n",
    "Para eficiencia con >5M de filas, usamos **Label Encoding** para `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT`.  \n",
    "LightGBM maneja bien etiquetas enteras y permite splits por categorÃ­a, especialmente cuando las variables no son ordinales reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef32330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Guardar encoders en memoria (opcional: persistir a disco si se desea)\n",
    "print(\"âœ… CategÃ³ricas codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8235487",
   "metadata": {},
   "source": [
    "## 7. DivisiÃ³n Train/Test (estratificada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2506a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0e536",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento: LightGBM (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = dict(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",  # âœ… Compensa desbalance (18/82 aprox.)\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"âœ… Modelo entrenado en {t1 - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9c69f",
   "metadata": {},
   "source": [
    "## 9. EvaluaciÃ³n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {prec:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1-score:   {f1:.4f}\")\n",
    "print(f\"ROC-AUC:    {roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8211b",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31077444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC = {roc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC - Retrasos de Llegada\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575726a",
   "metadata": {},
   "source": [
    "### Matriz de confusiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8431adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233e1f",
   "metadata": {},
   "source": [
    "## 10. Importancia de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = lgb.plot_importance(model, max_num_features=20, importance_type=\"gain\")\n",
    "plt.title(\"Importancia de variables (LightGBM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75011e51",
   "metadata": {},
   "source": [
    "## 11. FunciÃ³n de predicciÃ³n (para integraciÃ³n futura con API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preparar_entrada(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute):\n",
    "    \"\"\"\n",
    "    Prepara un diccionario con las features necesarias para predicciÃ³n.\n",
    "    - scheduled_hour: 0-23\n",
    "    - scheduled_minute: 0-59\n",
    "    \"\"\"\n",
    "    minuto_dia = scheduled_hour * 60 + scheduled_minute\n",
    "    salida_sin = math.sin(2 * math.pi * minuto_dia / (24*60))\n",
    "    salida_cos = math.cos(2 * math.pi * minuto_dia / (24*60))\n",
    "    row = {\n",
    "        \"AIRLINE\": encoders[\"AIRLINE\"].transform([str(airline)])[0],\n",
    "        \"ORIGIN_AIRPORT\": encoders[\"ORIGIN_AIRPORT\"].transform([str(origin)])[0],\n",
    "        \"DESTINATION_AIRPORT\": encoders[\"DESTINATION_AIRPORT\"].transform([str(destination)])[0],\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def predecir_probabilidad_delay(sample_dict):\n",
    "    \"\"\"Recibe un diccionario de features y devuelve probabilidad de retraso en llegada (>15 min).\"\"\"\n",
    "    df = pd.DataFrame([sample_dict])[list(model.feature_name_)]\n",
    "    proba = model.predict_proba(df)[:, 1][0]\n",
    "    return float(proba)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo = preparar_entrada(\"AA\", \"JFK\", \"LAX\", 5, 4, 14, 30)\n",
    "print(\"Probabilidad de retraso (ejemplo):\", round(predecir_probabilidad_delay(ejemplo), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53002b4",
   "metadata": {},
   "source": [
    "## 12. Conclusiones y siguientes pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91e733",
   "metadata": {},
   "source": [
    "\n",
    "- El modelo **LightGBM** con `class_weight=\"balanced\"` maneja correctamente el desbalance (~18% retrasos).\n",
    "- Las variables de ubicaciÃ³n (aeropuerto y aerolÃ­nea) y la codificaciÃ³n cÃ­clica de la hora suelen aportar poder predictivo.\n",
    "- Para producciÃ³n:\n",
    "  - serializar `model` y `encoders` con `joblib`,\n",
    "  - crear un endpoint `/flights/predict-delay` con FastAPI,\n",
    "  - validar en datos recientes y monitorear mÃ©tricas.\n",
    "\n",
    "**Mejoras posibles:**\n",
    "- Agregar variable de **distancia Haversine**.\n",
    "- Agregar **mes/dÃ­a** como seno/coseno (estacionalidad).\n",
    "- RegularizaciÃ³n y **bÃºsqueda de hiperparÃ¡metros** (Optuna).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cf1c",
   "metadata": {},
   "source": [
    "## 13. (Opcional) Guardado de modelo y encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from joblib import dump\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "# dump(model, \"models/lgbm_delay_model.joblib\")\n",
    "# dump(encoders, \"models/label_encoders.joblib\")\n",
    "# print(\"âœ… Modelo y encoders guardados en carpeta models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3b12a",
   "metadata": {},
   "source": [
    "### 14. Resumen: \n",
    "Resumo lo que muestran tus capturas y quÃ© harÃ­a para mejorarlo rÃ¡pido:\n",
    "\n",
    "Lectura rÃ¡pida de resultados\n",
    "\n",
    "Accuracy ~0.783: por debajo del baseline â€œtodo a tiempoâ€ (â‰ˆ0.815). Normal cuando forzamos a detectar mÃ¡s retrasos.\n",
    "\n",
    "Precision â‰ˆ 0.63 | Recall â‰ˆ 0.64 | F1 â‰ˆ 0.63 | ROC-AUC â‰ˆ 0.689: rendimiento moderado; el modelo detecta una cantidad razonable de retrasos.\n",
    "\n",
    "Importancia de variables: domina SALIDA_SIN (hora del dÃ­a), luego AIRLINE, MONTH, aeropuertos, DAY_OF_WEEK, SALIDA_COS. Tiene sentido: hora, aerolÃ­nea, estacionalidad y aeropuertos pesan mucho.\n",
    "\n",
    "Nota: si la matriz de confusiÃ³n no cuadra con tus mÃ©tricas, recuerda que confusion_matrix(y_test, y_pred) retorna [[TN, FP],[FN, TP]]. Verifica que estÃ©s leyendo TP = [1,1] y FP = [0,1], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a80dcd",
   "metadata": {},
   "source": [
    "###  RevisiÃ³n 2  - Revsiones dicionales post resultados.. \n",
    "\n",
    "a continuaciÃ³n (en orden de impacto vs. esfuerzo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87803653",
   "metadata": {},
   "source": [
    "1) Nuevas features: Haversine + estacionalidad\n",
    "\n",
    "Pega esta celda despuÃ©s de cargar v o (si ya tenÃ­as X, y) vuelve a crear X con estas columnas nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ca2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "# Si aÃºn tienes el DF completo como 'v', Ãºsalo. Si ya lo liberaste, vuelve a cargar flights_clean y rehaz lo mÃ­nimo.\n",
    "try:\n",
    "    v\n",
    "except NameError:\n",
    "    import os, time\n",
    "    DATA_PATH = \"data/processed/flights_clean.csv\"\n",
    "    t0 = time.time()\n",
    "    v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    print(\"Re-cargado v:\", v.shape, f\"en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Chequeo de columnas necesarias\n",
    "req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "       \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\"]\n",
    "falta = [c for c in req if c not in v.columns]\n",
    "if falta:\n",
    "    raise ValueError(\"Faltan columnas en v: \" + \", \".join(falta))\n",
    "\n",
    "# 1) Distancia Haversine\n",
    "v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# 2) Estacionalidad anual del mes (seno/coseno)\n",
    "v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# 3) (Opcional) Minuto crudo del dÃ­a si lo tienes; si no, lo calculo rÃ¡pido\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Requiere HORA_SALIDA y MIN_SALIDA; si no estÃ¡n, se puede derivar de SCHEDULED_DEPARTURE HHMM\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(np.int16)\n",
    "    else:\n",
    "        if \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "            hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# === SelecciÃ³n de variables actualizada ===\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\n",
    "    \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "]\n",
    "\n",
    "# (Opcional) aÃ±adir \"MINUTO_DIA_SALIDA\" si existe:\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "X.dtypes, X.shape, y.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655b525",
   "metadata": {},
   "source": [
    "2) CodificaciÃ³n categÃ³rica igual que antes (LabelEncoder)\n",
    "\n",
    "Si ya lo tenÃ­as, puedes saltarla; si no, ejÃ©cÃºtala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"Codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ead0fb",
   "metadata": {},
   "source": [
    "3) Split y entrenamiento con early stopping\n",
    "\n",
    "Entrenamos con validaciÃ³n en el set de test y paramos cuando deje de mejorar AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f580ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usar scale_pos_weight, ver mÃ¡s abajo\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iteration_={model.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77873373",
   "metadata": {},
   "source": [
    "ponderaciÃ³n explÃ­cita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903de63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = y_train.sum(); neg = len(y_train)-pos\n",
    "spw = neg / pos\n",
    "# usar scale_pos_weight=spw y quitar class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65c187",
   "metadata": {},
   "source": [
    "4) MÃ©tricas a umbral 0.5 y bÃºsqueda del mejor umbral\n",
    "\n",
    "Calcula mÃ©tricas en 0.5, luego busca mejor F1 y mejor recall con precisiÃ³n mÃ­nima (p. ej. â‰¥0.60). Imprime confusiones y compara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# MÃ©tricas al umbral 0.5\n",
    "y_pred05 = (y_proba>=0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"Umbral 0.5\")\n",
    "\n",
    "# BÃºsqueda de mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "\n",
    "# BÃºsqueda max recall con precisiÃ³n mÃ­nima (ajusta min_prec segun negocio)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"prec\":0}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_test, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_test, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":thr, \"rec\":rec, \"prec\":pre}\n",
    "\n",
    "y_pred_rec = (y_proba>=best_rec[\"thr\"]).astype(int)\n",
    "rec_res = metricas(y_test, y_pred_rec, y_proba, f\"Max Recall con Prec â‰¥ {min_prec:.2f} (thr={best_rec['thr']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2e2a7",
   "metadata": {},
   "source": [
    "5) (Opcional) Curva Precision-Recall para elegir umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65519f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b425c",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 3 -  nueva revisiÃ³n "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932e89d",
   "metadata": {},
   "source": [
    "Despues de los resultados anteriores, se sugiere los siguiente\n",
    "aplicar Target Encoding con K-Fold (sin fuga) a RUTA, AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT. Incluyen: creaciÃ³n de RUTA, codificaciÃ³n KFold con smoothing, split estratificado, entrenamiento LightGBM con early stopping, comparaciÃ³n de mÃ©tricas y guardado de mapeos para usar en producciÃ³n/API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4713d",
   "metadata": {},
   "source": [
    "Notas finales\n",
    "\n",
    "El Target Encoding K-Fold suele subir AUC/F1 sobre tus resultados actuales, especialmente con RUTA_TE.\n",
    "\n",
    "El smoothing controla el â€œoverfitâ€ en categorÃ­as raras. Para rutas con pocos vuelos, empuja la media hacia la global. Puedes probar 20, 50, 100.\n",
    "\n",
    "Para producciÃ³n, ideal: precalcular DISTANCIA_HAV a partir de lat/lon del origen/destino (que ya tienes en catÃ¡logos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f25528",
   "metadata": {},
   "source": [
    "0) (Opcional) Asegurar features base\n",
    "\n",
    "Si ya calculaste DISTANCIA_HAV, MONTH_SIN/COS, etc., puedes saltar esta celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# RUTA (origen_destino)\n",
    "v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# Distancia (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# (opcional) minuto del dÃ­a\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "v.shape, v.columns[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876fc30",
   "metadata": {},
   "source": [
    "1) Funciones de Target Encoding K-Fold (sin fuga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76725cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_target_encode(train_df, col, target, n_splits=5, smoothing=50, random_state=42):\n",
    "    \"\"\"\n",
    "    KFold target encoding sin fuga: devuelve\n",
    "      enc_train: serie con el encoding para el train (via KFold)\n",
    "      mapping: dict {categoria: media_suavizada} usando TODO el train (para aplicar en test/producciÃ³n)\n",
    "      default: media global del target (para categorÃ­as nuevas)\n",
    "    smoothing: mayor valor => mÃ¡s peso de la media global si hay pocos datos por categorÃ­a\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = float(train_df[target].mean())\n",
    "\n",
    "    enc_train = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, train_df[target]):\n",
    "        tr, val = train_df.iloc[tr_idx], train_df.iloc[val_idx]\n",
    "        stats = tr.groupby(col)[target].mean()\n",
    "        cnts  = tr[col].value_counts()\n",
    "        smooth = (cnts*stats + smoothing*global_mean) / (cnts + smoothing)\n",
    "        enc_train.iloc[val_idx] = val[col].map(smooth).fillna(global_mean).astype(np.float32)\n",
    "\n",
    "    # mapping final con TODO el train (para test/producciÃ³n)\n",
    "    full_stats = train_df.groupby(col)[target].mean()\n",
    "    full_cnts  = train_df[col].value_counts()\n",
    "    mapping = ((full_cnts*full_stats + smoothing*global_mean)/(full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc_train, mapping, global_mean\n",
    "\n",
    "def aplicar_target_encoding_test(test_df, col, mapping, default):\n",
    "    return test_df[col].map(mapping).fillna(default).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321751c",
   "metadata": {},
   "source": [
    "2) PreparaciÃ³n de train/test y generaciÃ³n de *_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cols_te = [\"RUTA\", \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# Variables numÃ©ricas base (sÃºmalas a gusto)\n",
    "num_features = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\"\n",
    "]\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    num_features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "# Split estratificado ANTES del encoding para evitar fuga\n",
    "train_df, test_df = train_test_split(v, test_size=0.2, stratify=v[target], random_state=42)\n",
    "\n",
    "# Generar encodings en train y mappings para test\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    tr_enc, mapping, default = kfold_target_encode(train_df, c, target, n_splits=5, smoothing=50, random_state=42)\n",
    "    train_df[f\"{c}_TE\"] = tr_enc\n",
    "    test_df[f\"{c}_TE\"]  = aplicar_target_encoding_test(test_df, c, mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Conjunto final de features\n",
    "features = num_features + [f\"{c}_TE\" for c in cols_te]\n",
    "\n",
    "X_train = train_df[features].copy()\n",
    "y_train = train_df[target].astype(int).copy()\n",
    "X_test  = test_df[features].copy()\n",
    "y_test  = test_df[target].astype(int).copy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a74ce",
   "metadata": {},
   "source": [
    "3) Entrenamiento LightGBM con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usa scale_pos_weight = (neg/pos)\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_te = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model_te.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Entrenado TE en {t1-t0:.1f}s | best_iteration_={model_te.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcbcfc",
   "metadata": {},
   "source": [
    "4) MÃ©tricas a 0.5 + bÃºsqueda de mejor umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model_te.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# Umbral 0.5\n",
    "y_pred05 = (y_proba >= 0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"TE - Umbral 0.5\")\n",
    "\n",
    "# Mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"TE - Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3be42",
   "metadata": {},
   "source": [
    "5) (Opcional) Guardar modelo + mapeos + umbral\n",
    "\n",
    "Ãštil para tu endpoint /flights/predict-delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "dump(model_te, \"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"mappings\": {k: {str(kk): float(vv) for kk, vv in mp.items()} for k, mp in mappings.items()},\n",
    "               \"defaults\": defaults}, f)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1[\"thr\"])  # o fija el que defina negocio\n",
    "with open(\"models/umbral.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "print(\"âœ… Guardados: modelo, mappings y umbral.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4d5dc",
   "metadata": {},
   "source": [
    "6) (ProducciÃ³n) Aplicar TE a un nuevo registro\n",
    "\n",
    "Ejemplo de funciÃ³n para preparar features a partir de un vuelo nuevo (usa mapeos TE; categorÃ­as no vistas â†’ media global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db72026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json\n",
    "import numpy as np\n",
    "\n",
    "# Cargar mappings en producciÃ³n:\n",
    "# from joblib import load; model_te = load(\"models/lgbm_delay_te.joblib\")\n",
    "# mappings = json.load(open(\"models/te_mappings.json\"))[\"mappings\"]\n",
    "# defaults = json.load(open(\"models/te_mappings.json\"))[\"defaults\"]\n",
    "# thr = json.load(open(\"models/umbral.json\"))[\"threshold\"]\n",
    "\n",
    "def preparar_features_api(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute,\n",
    "                          mappings=mappings, defaults=defaults):\n",
    "    # cÃ­clicos de hora\n",
    "    minuto_dia = scheduled_hour*60 + scheduled_minute\n",
    "    salida_sin = math.sin(2*math.pi*minuto_dia/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*minuto_dia/(24*60))\n",
    "    # cÃ­clicos de mes\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    fila = {\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos,\n",
    "        \"MONTH_SIN\": month_sin,\n",
    "        \"MONTH_COS\": month_cos,\n",
    "        # DISTANCIA_HAV deberÃ­a venir precalculada en back si tienes lat/lon; si no, dejar 0 o estimar\n",
    "        \"DISTANCIA_HAV\": 0.0,  \n",
    "        \"RUTA_TE\": float(mappings[\"RUTA\"].get(ruta, defaults[\"RUTA\"])),\n",
    "        \"AIRLINE_TE\": float(mappings[\"AIRLINE\"].get(airline, defaults[\"AIRLINE\"])),\n",
    "        \"ORIGIN_AIRPORT_TE\": float(mappings[\"ORIGIN_AIRPORT\"].get(origin, defaults[\"ORIGIN_AIRPORT\"])),\n",
    "        \"DESTINATION_AIRPORT_TE\": float(mappings[\"DESTINATION_AIRPORT\"].get(destination, defaults[\"DESTINATION_AIRPORT\"]))\n",
    "    }\n",
    "    return pd.DataFrame([fila])[list(model_te.feature_name_)]\n",
    "\n",
    "def predecir_prob_retraso_api(df_features, modelo=model_te, thr=UMBRAL_OPERATIVO):\n",
    "    proba = float(modelo.predict_proba(df_features)[:,1][0])\n",
    "    return {\"prob_delay\": proba, \"delayed\": int(proba >= thr)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62839ad",
   "metadata": {},
   "source": [
    "## revisiÃ³n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fce5d",
   "metadata": {},
   "source": [
    "modelo champion (LightGBM + LabelEncoder) con validaciÃ³n temporal, tuning ligero, elecciÃ³n de umbral y guardado de artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de3547",
   "metadata": {},
   "source": [
    "1) Carga optimizada desde CSV (solo columnas + dtypes chicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da44dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- FunciÃ³n Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# --- Chequeos mÃ­nimos (ya existen en tu CSV segÃºn lo que enviaste) ---\n",
    "cols_req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\n",
    "            \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\",\n",
    "            \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\"]\n",
    "faltan = [c for c in cols_req if c not in v.columns]\n",
    "if faltan:\n",
    "    raise ValueError(\"Faltan columnas base en v: \" + \", \".join(faltan))\n",
    "\n",
    "# 1) Distancia Haversine (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"],\n",
    "                                      v[\"DEST_LAT\"],   v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# 2) Estacionalidad del mes (seno/coseno) (si no existen)\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# 3) Minuto del dÃ­a de salida (si no existe)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Ya tienes HORA_SALIDA y MIN_SALIDA en tu CSV\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "\n",
    "# (Opcional) ruta texto (Ãºtil para anÃ¡lisis; para modelar, mejor encoding posterior)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Dtypes compactos para ahorro de RAM\n",
    "v[\"RETRASADO_LLEGADA\"] = v[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "v[\"MONTH\"] = v[\"MONTH\"].astype(\"int8\")\n",
    "v[\"DAY_OF_WEEK\"] = v[\"DAY_OF_WEEK\"].astype(\"int8\")\n",
    "v[\"DISTANCIA_HAV\"] = v[\"DISTANCIA_HAV\"].astype(\"float32\")\n",
    "v[\"SALIDA_SIN\"] = v[\"SALIDA_SIN\"].astype(\"float32\")\n",
    "v[\"SALIDA_COS\"] = v[\"SALIDA_COS\"].astype(\"float32\")\n",
    "v[\"MONTH_SIN\"] = v[\"MONTH_SIN\"].astype(\"float32\")\n",
    "v[\"MONTH_COS\"] = v[\"MONTH_COS\"].astype(\"float32\")\n",
    "\n",
    "print(\"âœ… Columnas derivadas listas. v.shape:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d3c7",
   "metadata": {},
   "source": [
    "SelecciÃ³n de variables (features) y salida X, y\n",
    "\n",
    "Incluye las nuevas columnas; deja listas para el siguiente paso (split temporal + encoding + fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de entrada para el modelo\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# CategÃ³ricas crudas (luego se codifican con LabelEncoder o category.codes)\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# NumÃ©ricas/derivadas\n",
    "num_cols = [\n",
    "    \"MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\", \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\", \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\"\n",
    "]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "\n",
    "# Validar presencia\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(\"Faltan columnas para features/target: \" + \", \".join(missing))\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(int)\n",
    "\n",
    "print(\"âœ… Features y target listos.\")\n",
    "print(\"X.shape:\", X.shape, \"| y.mean (rate retraso):\", y.mean().round(4))\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811818a7",
   "metadata": {},
   "source": [
    "3) Split temporal (train: meses 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ba8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos v[\"MONTH\"] para crear los masks y luego cortamos X,y sin hacer copias grandes\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask]\n",
    "y_train = y.loc[train_mask].astype(\"int8\")\n",
    "\n",
    "X_valid = X.loc[valid_mask]\n",
    "y_valid = y.loc[valid_mask].astype(\"int8\")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Valid:\", X_valid.shape,\n",
    "      \"| Rate train:\", float(y_train.mean()),\n",
    "      \"| Rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b634f",
   "metadata": {},
   "source": [
    "4) CodificaciÃ³n de categÃ³ricas (LabelEncoder) solo con train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc885a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "# Ajuste en train y transformaciÃ³n consistente en valid\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[c] = le.fit_transform(X_train[c].astype(str))\n",
    "    X_valid[c] = le.transform(X_valid[c].astype(str))\n",
    "    encoders[c] = le\n",
    "\n",
    "# Tipos compactos\n",
    "X_train = X_train.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "X_valid = X_valid.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "\n",
    "X_train.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595cf824",
   "metadata": {},
   "source": [
    "5) Entrenamiento LightGBM con early stopping (validaciÃ³n temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb, time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    # class_weight=\"balanced\",   # o comenta y usa scale_pos_weight=scale_pos_weight\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28654e3",
   "metadata": {},
   "source": [
    "6) MÃ©tricas a 0.5 y bÃºsqueda de umbral (mejor F1 y â€œprecisiÃ³n mÃ­nimaâ€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# 6.1 Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# 6.2 Mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e93e34",
   "metadata": {},
   "source": [
    "(Opcional si Operaciones pide precisiÃ³n mÃ­nima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 MÃ¡ximo Recall con PrecisiÃ³n mÃ­nima (ajusta min_prec)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"pre\":0}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_valid, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_valid, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":float(thr), \"rec\":float(rec), \"pre\":float(pre)}\n",
    "if best_rec[\"rec\"] > 0:\n",
    "    best_rec_res = report_metrics(y_valid, valid_proba, best_rec[\"thr\"], f\"Max Recall con Prec â‰¥ {min_prec:.2f}\")\n",
    "    best_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391b2dc",
   "metadata": {},
   "source": [
    "7) Guardar artefactos (modelo + encoders + umbral + metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])  # o fija el que defina negocio\n",
    "\n",
    "dump(model, \"models/lgbm_delay_champion.joblib\")\n",
    "dump(encoders, \"models/label_encoders.joblib\")\n",
    "with open(\"models/threshold.json\",\"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train.columns),\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"note\": \"LightGBM + LabelEncoder + temporal split (features derivadas activadas)\"\n",
    "}\n",
    "with open(\"models/metadata.json\",\"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ… Guardados: modelo, encoders, threshold y metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac669c",
   "metadata": {},
   "source": [
    "8) (Opcional) Curvas ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as sk_auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_valid, valid_proba)\n",
    "plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_valid, valid_proba):.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',c='grey'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC - ValidaciÃ³n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_valid, valid_proba)\n",
    "prauc = sk_auc(rec, prec)\n",
    "plt.figure(); plt.plot(rec, prec, label=f\"PR AUC={prauc:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall - ValidaciÃ³n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7517c08",
   "metadata": {},
   "source": [
    "### **RevisiÃ³n 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2daa7",
   "metadata": {},
   "source": [
    "guion completo para pasar de flights_clean.csv a un modelo LightGBM con Target Encoding KFold (TE) en AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT y RUTA, con split temporal (1â€“9 vs 10â€“12), bÃºsqueda de umbral operativo, y guardado de artefactos (modelo, mappings TE, threshold, metadata) + una funciÃ³n de predicciÃ³n para producciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9a80e",
   "metadata": {},
   "source": [
    "Paso 0 Â· Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")\n",
    "# print(\"Leyendo:\", os.path.abspath(DATA_PATH))\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "print(\"ğŸ“„ Usando archivo:\", DATA_PATH)\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"âœ… Cargado {v.shape} en {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff90a",
   "metadata": {},
   "source": [
    "Paso 1 Â· Carga del CSV (solo columnas Ãºtiles + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que vamos a usar en el pipeline\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# Ver quÃ© hay realmente en el archivo y cargar sÃ³lo lo que exista\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "if missing: print(\"âš ï¸ Faltan en CSV (no pasa nada, se calcularÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"HORA_SALIDA\":\"int8\",\"MIN_SALIDA\":\"int8\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ff53d",
   "metadata": {},
   "source": [
    "Paso 2 Â· Derivar columnas que faltan (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# Distancia Haversine\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH\" in v.columns and \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto del dÃ­a de salida\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "        ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "\n",
    "# Ruta (texto)\n",
    "if \"RUTA\" not in v.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# NormalizaciÃ³n de tipos\n",
    "to_float32 = [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]\n",
    "for c in to_float32:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "for c in [\"MONTH\",\"DAY_OF_WEEK\",\"RETRASADO_LLEGADA\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"int8\")\n",
    "\n",
    "print(\"âœ… Derivadas listas | columnas:\", len(v.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09913e8",
   "metadata": {},
   "source": [
    "Paso 3 Â· Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "missing_feats = [c for c in cat_cols + num_cols if c not in v.columns]\n",
    "if missing_feats: print(\"âš ï¸ Faltaron features (se omiten):\", missing_feats)\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(\"int8\")\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303c6b9",
   "metadata": {},
   "source": [
    "Paso 4 Â· Split temporal (train 1â€“9, valid 10â€“12) + copias seguras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598da970",
   "metadata": {},
   "source": [
    "Paso 5 Â· Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 5 (FIX) Â· Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA) ===\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kfold_target_encode(x_col: pd.Series, y: pd.Series, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - x_col: Series (columna categÃ³rica) indexada igual que y\n",
    "      - y:     Series binaria (0/1) indexada igual que x_col\n",
    "    Devuelve:\n",
    "      - enc_train: Series con el encoding para cada fila del train (out-of-fold)\n",
    "      - mapping:   dict categorÃ­a -> valor TE (para aplicar en valid/test/producciÃ³n)\n",
    "      - global_mean: float con la media global del target (fallback)\n",
    "    \"\"\"\n",
    "    assert x_col.index.equals(y.index), \"x_col y y deben estar alineados por Ã­ndice\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=y.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(x_col, y):\n",
    "        # construimos df temporal con col + target para este fold\n",
    "        df_tr = pd.DataFrame({\"col\": x_col.iloc[tr_idx].astype(str), \"y\": y.iloc[tr_idx].astype(int)})\n",
    "        df_val = pd.DataFrame({\"col\": x_col.iloc[val_idx].astype(str)})\n",
    "\n",
    "        stats = df_tr.groupby(\"col\")[\"y\"].mean()\n",
    "        cnts  = df_tr.groupby(\"col\")[\"y\"].size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = df_val[\"col\"].map(smoothed).fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar fuera del train)\n",
    "    df_full = pd.DataFrame({\"col\": x_col.astype(str), \"y\": y.astype(int)})\n",
    "    stats_f = df_full.groupby(\"col\")[\"y\"].mean()\n",
    "    cnts_f  = df_full.groupby(\"col\")[\"y\"].size()\n",
    "    mapping = ((stats_f*cnts_f + global_mean*smoothing) / (cnts_f + smoothing)).astype(\"float32\").to_dict()\n",
    "\n",
    "    return enc.astype(\"float32\"), mapping, global_mean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "# --- columnas a codificar (sÃ³lo las que existan en X_train) ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "\n",
    "# Trabajamos sobre copias para evitar warnings\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    enc_train, mapping, default = kfold_target_encode(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_train\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Modelo us\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c27e0",
   "metadata": {},
   "source": [
    "Ese NameError indica que no se creÃ³ X_train_model/X_valid_model (normalmente porque la celda del Paso 5 â€“ Target Encoding no se ejecutÃ³ o fallÃ³ a mitad). Deja una celda puente antes del entrenamiento que:\n",
    "\n",
    "Verifica que corriste los pasos 3â€“5.\n",
    "\n",
    "Si hay TE, arma X_train_model/X_valid_model.\n",
    "\n",
    "Da mensajes claros si falta algo.\n",
    "\n",
    "Pega y ejecuta esto justo antes del Paso 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95170f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Puente de seguridad antes de entrenar (Paso 6) ---\n",
    "# Requiere X_train, X_valid, y_train, y_valid de los pasos 3â€“4\n",
    "assert 'X_train' in globals() and 'X_valid' in globals(), \"Falta correr el Paso 3â€“4 (definir X_train/X_valid).\"\n",
    "assert 'y_train' in globals() and 'y_valid' in globals(), \"Falta correr el Paso 3â€“4 (definir y_train/y_valid).\"\n",
    "\n",
    "# Columnas categÃ³ricas originales que se codifican con TE\n",
    "cols_te_base = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "cols_te = [c for c in cols_te_base if c in X_train.columns or f\"{c}_TE\" in X_train.columns]\n",
    "\n",
    "# Â¿existen columnas TE? (AIRLINE_TE, etc.)\n",
    "te_cols_present = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" in X_train.columns]\n",
    "if len(te_cols_present) != len(cols_te):\n",
    "    faltan = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" not in X_train.columns]\n",
    "    raise ValueError(\n",
    "        \"Faltan columnas de Target Encoding. \"\n",
    "        f\"No se ejecutÃ³ (o fallÃ³) el Paso 5.\\nFaltan: {faltan}\"\n",
    "    )\n",
    "\n",
    "# Construir matrices finales: todas las numÃ©ricas + columnas _TE; quitamos las categÃ³ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te_base if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te_base if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ… Listo para entrenar\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas ejemplo:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983568",
   "metadata": {},
   "source": [
    "Paso 6 Â· Entrenamiento LightGBM con early stopping (balanceado por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=5.0,\n",
    "    scale_pos_weight=scale_pos_weight,  # en vez de class_weight\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59d392",
   "metadata": {},
   "source": [
    "Paso 7 Â· MÃ©tricas (0.5, mejor F1) y umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_res = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (bÃºsqueda gruesa)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9520168",
   "metadata": {},
   "source": [
    "Paso 8 Â· Guardar artefactos (modelo, mappings TE, defaults y threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "# Guardamos mappings y defaults de TE\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train_model.columns),\n",
    "    \"cols_te\": cols_te,\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"scale_pos_weight\": float(scale_pos_weight),\n",
    "    \"note\": \"LightGBM + KFold Target Encoding (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)\"\n",
    "}\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ… Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0cbfe",
   "metadata": {},
   "source": [
    "Paso 9 Â· (ProducciÃ³n) FunciÃ³n de predicciÃ³n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar artefactos (en tu API/servicio)\n",
    "from joblib import load\n",
    "import json, numpy as np, math\n",
    "\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f: te_mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f: te_defaults = json.load(f)\n",
    "with open(\"models/threshold.json\",\"r\",encoding=\"utf-8\") as f: UMBRAL = json.load(f)[\"threshold\"]\n",
    "\n",
    "def prep_features_for_inference(\n",
    "    month:int, day_of_week:int,\n",
    "    airline:str, origin:str, destination:str,\n",
    "    scheduled_hour:int, scheduled_minute:int,\n",
    "    origen_lat:float, origen_lon:float, dest_lat:float, dest_lon:float\n",
    "):\n",
    "    # cÃ­clicos\n",
    "    salida_min = int(scheduled_hour)*60 + int(scheduled_minute)\n",
    "    salida_sin = math.sin(2*math.pi*salida_min/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*salida_min/(24*60))\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    # haversine\n",
    "    def haversine_km(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1 = math.radians(lat1); lon1 = math.radians(lon1)\n",
    "        lat2 = math.radians(lat2); lon2 = math.radians(lon2)\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist_hav = haversine_km(origen_lat, origen_lon, dest_lat, dest_lon)\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    # aplicar TE (map -> valor; si no existe categorÃ­a, usar default global)\n",
    "    def te(col, val):\n",
    "        mp = te_mappings[col]; default = te_defaults[col]\n",
    "        return float(mp.get(str(val), default))\n",
    "\n",
    "    row = {\n",
    "        \"MONTH\": np.int8(month),\n",
    "        \"DAY_OF_WEEK\": np.int8(day_of_week),\n",
    "        \"SALIDA_SIN\": np.float32(salida_sin),\n",
    "        \"SALIDA_COS\": np.float32(salida_cos),\n",
    "        \"MONTH_SIN\": np.float32(month_sin),\n",
    "        \"MONTH_COS\": np.float32(month_cos),\n",
    "        \"DISTANCIA_HAV\": np.float32(dist_hav),\n",
    "        \"MINUTO_DIA_SALIDA\": np.int16(salida_min),\n",
    "        # TE\n",
    "        \"AIRLINE_TE\": np.float32(te(\"AIRLINE\", airline)),\n",
    "        \"ORIGIN_AIRPORT_TE\": np.float32(te(\"ORIGIN_AIRPORT\", origin)),\n",
    "        \"DESTINATION_AIRPORT_TE\": np.float32(te(\"DESTINATION_AIRPORT\", destination)),\n",
    "        \"RUTA_TE\": np.float32(te(\"RUTA\", ruta)),\n",
    "    }\n",
    "\n",
    "    # El orden de columnas debe coincidir con X_train_model.columns\n",
    "    cols = list(c for c in model.feature_name_)\n",
    "    Xrow = np.array([[row[c] for c in cols]], dtype=np.float32)\n",
    "    return Xrow\n",
    "\n",
    "def predict_delay_prob(\n",
    "    month, day_of_week, airline, origin, destination,\n",
    "    scheduled_hour, scheduled_minute,\n",
    "    origen_lat, origen_lon, dest_lat, dest_lon\n",
    "):\n",
    "    Xrow = prep_features_for_inference(\n",
    "        month, day_of_week, airline, origin, destination,\n",
    "        scheduled_hour, scheduled_minute, origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    )\n",
    "    proba = float(model.predict_proba(Xrow)[0,1])\n",
    "    delayed = proba >= UMBRAL\n",
    "    return {\"prob_delay\": round(proba,4), \"delayed\": bool(delayed), \"threshold\": UMBRAL}\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# predict_delay_prob(7, 4, \"DL\", \"ATL\", \"JFK\", 14, 30, 33.64, -84.43, 40.64, -73.78)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b547da0",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bb55d",
   "metadata": {},
   "source": [
    "0. Encabezado y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75909967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0 Â· Imports, ruta y helpers\n",
    "import os, time, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb26ea",
   "metadata": {},
   "source": [
    "1. Carga del CSV (eficiente: usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1 Â· Carga del CSV (solo columnas Ãºtiles + dtypes compactos)\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")  # ajusta si lo tienes en otra ruta\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# leer solo el header para ver quÃ© hay\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"Columnas cargadas:\", present, \"\\nFaltantes (se derivan si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae384338",
   "metadata": {},
   "source": [
    "2. Derivar columnas que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2 Â· DerivaciÃ³n de features faltantes\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# Minuto del dÃ­a (si no vino ya)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# Ruta (texto)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"âœ… Derivadas OK | columnas:\", len(v.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b124f86",
   "metadata": {},
   "source": [
    "3. Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb86f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3 Â· SelecciÃ³n de variables\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3b6a",
   "metadata": {},
   "source": [
    "4. Split temporal (train: meses 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4 Â· Split temporal (evita fuga)\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dd98a",
   "metadata": {},
   "source": [
    "5. Target Encoding KFold (sin fuga) para categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Target Encoding KFold ROBUSTO (sin fuga) + armado de matrices\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) PRERREQUISITOS / ALINEACIONES ---\n",
    "# Deben existir de pasos previos:\n",
    "#   v, X, y, X_train, X_valid, y_train, y_valid\n",
    "for obj_name in [\"X_train\", \"X_valid\", \"y_train\", \"y_valid\"]:\n",
    "    assert obj_name in globals(), f\"Falta {obj_name}. Ejecuta los pasos previos.\"\n",
    "\n",
    "# Asegurar que y_train/y_valid estÃ¡n alineados con X_train/X_valid\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_valid = y_valid.loc[X_valid.index]\n",
    "\n",
    "# Crear RUTA si no existe\n",
    "if \"RUTA\" not in X_train.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(X_train.columns):\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    X_train[\"RUTA\"] = (X_train[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_train[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    X_valid[\"RUTA\"] = (X_valid[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_valid[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Columnas a codificar (solo si existen)\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "print(\"TE sobre:\", cols_te)\n",
    "\n",
    "# --- 1) Funciones TE robustas (operan con Series) ---\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42) -> tuple[pd.Series, dict, float]:\n",
    "    \"\"\"\n",
    "    s: Serie categÃ³rica (mismo Ã­ndice que y)\n",
    "    y: Serie binaria 0/1 (mismo Ã­ndice que s)\n",
    "    Devuelve:\n",
    "      enc     -> Serie con el encoding KFold para s (alineada a s.index)\n",
    "      mapping -> dict valor_categoria -> encoding_final (con TODO el train)\n",
    "      gmean   -> media global (fallback)\n",
    "    \"\"\"\n",
    "    # AlineaciÃ³n defensiva por Ã­ndice\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx]\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    # NormalizaciÃ³n de tipos\n",
    "    s = s.astype(\"string\")  # evita NaNs tipo objeto raros\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    # Mapping final con TODO el train (para producciÃ³n/valid)\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float) -> pd.Series:\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "# --- 2) Ejecutar TE ---\n",
    "mappings, defaults = {}, {}\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid[f\"{c}_TE\"] = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "print(\"âœ… TE aplicado sin fuga.\")\n",
    "print(\"Ejemplo TE:\", {k: list(v)[:2] if hasattr(v, \"__iter__\") else v for k,v in list(mappings.items())[:1]})\n",
    "\n",
    "# --- 3) Construir matrices finales: quitamos las categorÃ­as crudas ---\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"Listo para entrenar:\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas (primeras 12):\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3435a",
   "metadata": {},
   "source": [
    "6. Entrenamiento LightGBM (early stopping, balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Chequeos rÃ¡pidos\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "print(\"Shapes:\", X_train_model.shape, X_valid_model.shape)\n",
    "\n",
    "# Balanceo por proporciÃ³n de clases (pos/neg)\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    # usa uno u otro balanceo (recomiendo este):\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5051f6",
   "metadata": {},
   "source": [
    "7) MÃ©tricas base (0.5) + bÃºsqueda de mejor umbral (por F1) y matriz de confusiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (bÃºsqueda simple)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627403",
   "metadata": {},
   "source": [
    "### revisiÃ³n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf6512",
   "metadata": {},
   "source": [
    "agregados histÃ³ricos + reentrenamiento con LGBM y comparaciÃ³n con el modelo anterior\n",
    "\n",
    "agregar seÃ±ales histÃ³ricas sin fuga (congestiÃ³n por ruta y hora), re-entrenar LightGBM y comparar contra tu baseline. \n",
    "\n",
    "EstÃ¡ pensado para ejecutarlo despuÃ©s de tu pipeline anterior (ya se debe tener v, X_train, X_valid, y_train, y_valid, X_train_model, X_valid_model listos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee88653",
   "metadata": {},
   "source": [
    "1) Agregados histÃ³ricos (solo con TRAIN 1â€“9) â†’ sin fuga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 1: Carga eficiente con tus nombres + fallbacks seguros ===\n",
    "import time, numpy as np, pandas as pd\n",
    "\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "\n",
    "# Fallbacks de carga (para poder derivar luego):\n",
    "# 1) Si no viene RETRASADO_LLEGADA, intenta traer ARRIVAL_DELAY para crearlo.\n",
    "if \"RETRASADO_LLEGADA\" not in present and \"ARRIVAL_DELAY\" in header:\n",
    "    present.append(\"ARRIVAL_DELAY\")\n",
    "\n",
    "# 2) Si no vienen SALIDA_SIN/COS, con SCHEDULED_DEPARTURE podemos derivarlas (ya estÃ¡ en present).\n",
    "# 3) Si faltan lat/lon y existen equivalentes en tu archivo, aÃ±ade aquÃ­ sus alias si aplica.\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"ARRIVAL_DELAY\":\"float32\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(\"â†’ Cargado:\", v.shape, \"| en\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# === Derivaciones mÃ­nimas para completar faltantes ===\n",
    "\n",
    "# Target desde ARRIVAL_DELAY si no vino RETRASADO_LLEGADA\n",
    "if \"RETRASADO_LLEGADA\" not in v.columns:\n",
    "    if \"ARRIVAL_DELAY\" not in v.columns:\n",
    "        raise ValueError(\"No hay RETRASADO_LLEGADA ni ARRIVAL_DELAY para derivarlo.\")\n",
    "    v[\"RETRASADO_LLEGADA\"] = (v[\"ARRIVAL_DELAY\"] > 15).astype(\"int8\")\n",
    "\n",
    "# SALIDA_SIN/COS desde SCHEDULED_DEPARTURE si faltan\n",
    "if (\"SALIDA_SIN\" not in v.columns or \"SALIDA_COS\" not in v.columns) and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    minuto = (hs*60 + ms).astype(\"int16\")\n",
    "    v[\"SALIDA_SIN\"] = np.sin(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "    v[\"SALIDA_COS\"] = np.cos(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "\n",
    "# Distancia Haversine (opcional) si mÃ¡s adelante la quieres usar y tienes lat/lon\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(v[\"ORIGEN_LAT\"]); lon1 = np.radians(v[\"ORIGEN_LON\"])\n",
    "    lat2 = np.radians(v[\"DEST_LAT\"]);   lon2 = np.radians(v[\"DEST_LON\"])\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    v[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes (opcional)\n",
    "if \"MONTH\" in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# RUTA (Ãºtil para Target Encoding)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(f\"âœ… Listo: rate={v['RETRASADO_LLEGADA'].mean():.4f} | cols={len(v.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e5160",
   "metadata": {},
   "source": [
    "Paso 2 Â· Split temporal (train=meses 1â€“9, valid=10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el DataFrame v cargado en el Paso 1\n",
    "assert \"MONTH\" in v.columns and \"RETRASADO_LLEGADA\" in v.columns, \"Faltan MONTH o RETRASADO_LLEGADA.\"\n",
    "\n",
    "# Variables base (ajusta si quieres usar mÃ¡s adelante otras)\n",
    "# Si ya creaste RUTA en el Paso 1, se usarÃ¡; si no existe, la generamos aquÃ­.\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in v.columns]\n",
    "num_cols = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "                        \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"] if c in v.columns]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’\",\n",
    "      \"X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b3ff8",
   "metadata": {},
   "source": [
    "Paso 3 Â· Target Encoding KFold (sin fuga) para categÃ³ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1dfa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42):\n",
    "    # AlineaciÃ³n defensiva\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx].astype(\"string\")\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = cat_cols[:]  # todas las categÃ³ricas disponibles\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "# Copias para no tocar los originales\n",
    "X_train_te = X_train.copy()\n",
    "X_valid_te = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train_te[c], y_train,\n",
    "                                                          n_splits=5, smoothing=50, seed=42)\n",
    "    X_train_te[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid_te[f\"{c}_TE\"]  = apply_te(X_valid_te[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Matrices finales de entrenamiento (quitamos las columnas categÃ³ricas crudas)\n",
    "X_train_model = X_train_te.drop(columns=cols_te).copy()\n",
    "X_valid_model = X_valid_te.drop(columns=cols_te).copy()\n",
    "\n",
    "print(\"âœ… TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7828e",
   "metadata": {},
   "source": [
    "(Opcional fuerte) Paso 4 Â· Agregados histÃ³ricos RUTAÃ—HORA (sin fuga)\n",
    "\n",
    "Si quieres mejorar precisiÃ³n/recall, agrega estadÃ­sticos histÃ³ricos usando solo train (meses 1â€“9) y mapea a train/valid. Si no lo necesitas ahora, salta al paso 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "assert \"SCHEDULED_DEPARTURE\" in v.columns, \"Requiere SCHEDULED_DEPARTURE para obtener HORA_SALIDA.\"\n",
    "HORA_all = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "RUTA_all = v[\"RUTA\"].astype(str) if \"RUTA\" in v.columns else (\n",
    "    v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    ")\n",
    "\n",
    "train_mask_all = v[\"MONTH\"].between(1, 9)\n",
    "\n",
    "aux = pd.DataFrame({\n",
    "    \"RUTA\": RUTA_all[train_mask_all].values,\n",
    "    \"HORA_SALIDA\": HORA_all[train_mask_all].values,\n",
    "    \"RETRASADO_LLEGADA\": v.loc[train_mask_all, \"RETRASADO_LLEGADA\"].values\n",
    "})\n",
    "# Si existe ARRIVAL_DELAY, incluimos media de retraso\n",
    "if \"ARRIVAL_DELAY\" in v.columns:\n",
    "    aux[\"ARRIVAL_DELAY\"] = v.loc[train_mask_all, \"ARRIVAL_DELAY\"].values\n",
    "\n",
    "agg_dict = {\"rate_delay\": (\"RETRASADO_LLEGADA\",\"mean\"),\n",
    "            \"n\": (\"RETRASADO_LLEGADA\",\"size\")}\n",
    "if \"ARRIVAL_DELAY\" in aux.columns:\n",
    "    agg_dict[\"mean_arr_delay\"] = (\"ARRIVAL_DELAY\",\"mean\")\n",
    "\n",
    "agg_ruta_hora = (aux\n",
    "    .groupby([\"RUTA\",\"HORA_SALIDA\"], observed=True)\n",
    "    .agg(**agg_dict)\n",
    "    .reset_index())\n",
    "\n",
    "g_rate = float(agg_ruta_hora[\"rate_delay\"].mean())\n",
    "g_n    = 0.0\n",
    "g_mean_arr = float(agg_ruta_hora[\"mean_arr_delay\"].mean()) if \"mean_arr_delay\" in agg_ruta_hora.columns else 0.0\n",
    "\n",
    "def add_route_hour_stats(X_in, idx):\n",
    "    tmp = pd.DataFrame({\n",
    "        \"RUTA\": RUTA_all.loc[idx].astype(str).values,\n",
    "        \"HORA_SALIDA\": HORA_all.loc[idx].astype(\"int16\").values\n",
    "    }, index=idx)\n",
    "    merged = tmp.merge(agg_ruta_hora, on=[\"RUTA\",\"HORA_SALIDA\"], how=\"left\")\n",
    "    X_in[\"RUTA_HORA_rate\"] = merged[\"rate_delay\"].fillna(g_rate).astype(\"float32\").values\n",
    "    X_in[\"RUTA_HORA_n\"]    = merged[\"n\"].fillna(g_n).astype(\"float32\").values\n",
    "    if \"mean_arr_delay\" in merged.columns:\n",
    "        X_in[\"RUTA_HORA_mean_arr\"] = merged[\"mean_arr_delay\"].fillna(g_mean_arr).astype(\"float32\").values\n",
    "    return X_in\n",
    "\n",
    "X_train_model = add_route_hour_stats(X_train_model.copy(), X_train_model.index)\n",
    "X_valid_model = add_route_hour_stats(X_valid_model.copy(), X_valid_model.index)\n",
    "\n",
    "print(\"âœ” Agregados aÃ±adidos:\",\n",
    "      [c for c in X_train_model.columns if c.startswith(\"RUTA_HORA_\")],\n",
    "      \"| tiempo:\", f\"{time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2668c",
   "metadata": {},
   "source": [
    "Paso 5 Â· Entrenamiento con LightGBM (early stopping + balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd192744",
   "metadata": {},
   "source": [
    "Paso 6 Â· MÃ©tricas y selecciÃ³n de umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (puedes restringir por precisiÃ³n mÃ­nima si tu negocio lo requiere)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], f\"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c1e68",
   "metadata": {},
   "source": [
    "Paso 7 Â· Guardado de artefactos (modelo + TE + columnas + umbral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Umbral operativo (elige el que te convenga: best[\"thr\"] o un umbral de negocio)\n",
    "UMBRAL_OPERATIVO = float(best[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:{str(kk):float(vv) for kk,vv in v.items()} for k,v in mappings.items()}, f)\n",
    "with open(\"models/te_defaults.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:float(v) for k,v in defaults.items()}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cat_encoded=cols_te,\n",
    "    split=\"temporal: train 1-9, valid 10-12\",\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    auc_valid=float(auc_val),\n",
    "    threshold=float(UMBRAL_OPERATIVO),\n",
    "    notes=\"LightGBM con TE KFold (sin fuga)\" + (\" + agregados RUTAÃ—HORA\" if any(c.startswith(\"RUTA_HORA_\") for c in X_train_model.columns) else \"\")\n",
    ")\n",
    "with open(\"models/metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Guardado: models/lgbm_delay.joblib, te_mappings.json, te_defaults.json, metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91db1cb",
   "metadata": {},
   "source": [
    "Paso 8 Â· FunciÃ³n de inferencia (para API / dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from joblib import load\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    defaults = json.load(f)\n",
    "with open(\"models/metadata.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "FEATURES = meta[\"features\"]\n",
    "THRESHOLD = float(meta[\"threshold\"])\n",
    "\n",
    "def infer_delay(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"df_input: columnas mÃ­nimas: AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, MONTH, DAY_OF_WEEK, SCHEDULED_DEPARTURE (+ lat/lon si quieres DISTANCIA_HAV).\n",
    "       Devuelve: proba_delay y pred (0/1) usando THRESHOLD guardado.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in df.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(df.columns):\n",
    "        df[\"RUTA\"] = df[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + df[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # Derivar MINUTO_DIA_SALIDA / SALIDA_SIN/COS si hace falta\n",
    "    if \"MINUTO_DIA_SALIDA\" not in df.columns and \"SCHEDULED_DEPARTURE\" in df.columns:\n",
    "        hs = (df[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "        ms = (df[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "        df[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "    if \"SALIDA_SIN\" not in df.columns and \"MINUTO_DIA_SALIDA\" in df.columns:\n",
    "        m = df[\"MINUTO_DIA_SALIDA\"].astype(\"float32\")\n",
    "        df[\"SALIDA_SIN\"] = np.sin(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "        df[\"SALIDA_COS\"] = np.cos(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN/COS\n",
    "    if \"MONTH\" in df.columns:\n",
    "        df[\"MONTH_SIN\"] = np.sin(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "        df[\"MONTH_COS\"] = np.cos(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    # (Opcional) DISTANCIA_HAV si hay lat/lon\n",
    "    if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(df.columns) and \"DISTANCIA_HAV\" not in df.columns:\n",
    "        R = 6371.0\n",
    "        lat1 = np.radians(df[\"ORIGEN_LAT\"]); lon1 = np.radians(df[\"ORIGEN_LON\"])\n",
    "        lat2 = np.radians(df[\"DEST_LAT\"]);   lon2 = np.radians(df[\"DEST_LON\"])\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "        df[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "    # Target Encoding con mappings guardados\n",
    "    for c, mapping in mappings.items():\n",
    "        default = float(defaults.get(c, df_input.get(c, pd.Series([],dtype='object')).mean() if c in df_input.columns else 0.0))\n",
    "        df[f\"{c}_TE\"] = df[c].astype(\"string\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "    # Construir matriz final con las FEATURES del entrenamiento\n",
    "    X_inf = df.reindex(columns=FEATURES, fill_value=0).copy()\n",
    "\n",
    "    proba = model.predict_proba(X_inf)[:,1]\n",
    "    pred  = (proba >= THRESHOLD).astype(int)\n",
    "    out = df_input.copy()\n",
    "    out[\"proba_delay\"] = proba\n",
    "    out[\"pred_delay\"]  = pred\n",
    "    return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# infer_delay(pd.DataFrame([{\n",
    "#   \"AIRLINE\":\"AA\",\"ORIGIN_AIRPORT\":\"JFK\",\"DESTINATION_AIRPORT\":\"LAX\",\"MONTH\":7,\"DAY_OF_WEEK\":5,\"SCHEDULED_DEPARTURE\": 1730\n",
    "# }]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c585",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b79497",
   "metadata": {},
   "source": [
    "plit temporal (1â€“9 vs 10â€“12), Target Encoding KFold sin fuga, agregados histÃ³ricos sin fuga, entrena LightGBM con early stopping y guarda artefactos (modelo, mapeos TE y threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c42b74",
   "metadata": {},
   "source": [
    "Paso 0 Â· Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 0: imports y ruta =====\n",
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# Ajusta a tu ruta:\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1911550",
   "metadata": {},
   "source": [
    "Paso 1 Â· Carga del CSV (usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0776802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Columnas presentes: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA'] \n",
      "â†’ Faltantes (se derivarÃ¡n si aplica): []\n",
      "âœ“ Cargado: (5231130, 14) | en 24.1s\n",
      "âœ“ Rate retraso: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 1: carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"â†’ Columnas presentes:\", present, \"\\nâ†’ Faltantes (se derivarÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(f\"âœ“ Cargado: {v.shape} | en {time.time()-t0:.1f}s\")\n",
    "print(\"âœ“ Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3546d",
   "metadata": {},
   "source": [
    "Paso 2 Â· Derivar features que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO/HORA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa4560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 2: features derivados =====\n",
    "\n",
    "# Haversine (km)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto y hora de salida (si no existen)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23).astype(\"int16\")\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59).astype(\"int16\")\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"]//60).astype(\"int16\")\n",
    "\n",
    "# Ruta\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Cast finales (por si cargaron como object)\n",
    "for c in [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "\n",
    "print(\"âœ“ Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340eed",
   "metadata": {},
   "source": [
    "Paso 3 Â· Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaf7e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 3: selecciÃ³n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "            \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"]\n",
    "\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54d84f",
   "metadata": {},
   "source": [
    "Paso 4 Â· Split temporal (train: 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2ae477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split â†’ X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 4: split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’ X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac9816",
   "metadata": {},
   "source": [
    "Paso 5 Â· Target Encoding KFold (sin fuga) para categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be913863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TE aplicado (fix categorÃ­as) | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 5 (FIX): Target Encoding KFold (sin fuga) robusto para columnas categÃ³ricas =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - Agrupa SIEMPRE usando la columna como object (evita comportamiento de 'category').\n",
    "      - Mapea con dict (no Series) para asegurarnos de obtener floats.\n",
    "    Devuelve:\n",
    "      enc (Series float32, aligned con train_df.index),\n",
    "      mapping (dict {categoria: valor_TE}),\n",
    "      global_mean (float)\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=\"float32\")\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        # claves como object para evitar dtype 'category'\n",
    "        keys_tr = train_df.iloc[tr_idx][col].astype(\"object\")\n",
    "        stats = y.iloc[tr_idx].groupby(keys_tr).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(keys_tr).size()\n",
    "        smoothed = ((stats*cnts + global_mean*smoothing) / (cnts + smoothing)).astype(\"float32\")\n",
    "        smoothed_dict = smoothed.to_dict()\n",
    "\n",
    "        keys_val = train_df.iloc[val_idx][col].astype(\"object\")\n",
    "        mapped = keys_val.map(smoothed_dict).astype(\"float32\")\n",
    "        enc.iloc[val_idx] = mapped.fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar en valid/test/producciÃ³n)\n",
    "    keys_all = train_df[col].astype(\"object\")\n",
    "    full_stats = y.groupby(keys_all).mean()\n",
    "    full_cnts  = y.groupby(keys_all).size()\n",
    "    final = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).astype(\"float32\")\n",
    "    mapping = final.to_dict()\n",
    "\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    # asegurar object antes de mapear, retornando float32\n",
    "    return series.astype(\"object\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "\n",
    "# --- Aplica TE en las columnas categÃ³ricas seleccionadas ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    # aÃ±ade columna _TE\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr.values.astype(\"float32\")\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = float(default)\n",
    "\n",
    "# matrices finales: dejamos numÃ©ricas + _TE (quitamos las categorÃ­as crudas)\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ“ TE aplicado (fix categorÃ­as) | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eca9a7",
   "metadata": {},
   "source": [
    "Paso 6 Â· Agregados histÃ³ricos (sin fuga) y uniÃ³n a matrices del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d93489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agregados aplicados en 9.3s\n",
      "âœ“ Columnas nuevas: ['AIR_n', 'AIR_rate', 'DES_n', 'DES_rate', 'ORI_n', 'ORI_rate', 'RUTA_HORA_n', 'RUTA_HORA_rate', 'RUTA_n', 'RUTA_rate'] \n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 6 Â· Agregados histÃ³ricos (sin fuga) â€” versiÃ³n robusta =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# 0) Asegurar llaves en 'v' (DataFrame completo)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "        v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "    else:\n",
    "        raise KeyError(\"No puedo crear RUTA: faltan ORIGIN_AIRPORT/DESTINATION_AIRPORT en 'v'.\")\n",
    "\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23).astype(\"int8\")\n",
    "    else:\n",
    "        raise KeyError(\"No puedo derivar HORA_SALIDA: falta MINUTO_DIA_SALIDA o SCHEDULED_DEPARTURE.\")\n",
    "\n",
    "# 1) MÃ¡scaras temporales (sin fuga)\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask]\n",
    "global_mean = float(v_train[target].mean())\n",
    "\n",
    "# 2) Inyectar las llaves requeridas en X_train / X_valid por Ã­ndice (asegura disponibilidad)\n",
    "needed_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "for k in needed_keys:\n",
    "    if k not in X_train.columns:\n",
    "        X_train[k] = v.loc[train_mask, k].values\n",
    "    if k not in X_valid.columns:\n",
    "        X_valid[k] = v.loc[valid_mask, k].values\n",
    "\n",
    "# 3) Definir specs de agregados y filtrar dinÃ¡micamente a las claves existentes\n",
    "candidate_aggs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "]\n",
    "available_in_vtrain = set(v_train.columns)\n",
    "aggs_specs = [(keys, pref) for keys, pref in candidate_aggs if set(keys).issubset(available_in_vtrain)]\n",
    "if not aggs_specs:\n",
    "    raise RuntimeError(\"No hay combinaciones de agregados vÃ¡lidas; revisa que existan las columnas clave.\")\n",
    "\n",
    "# 4) Funciones de agregaciÃ³n con suavizado y join\n",
    "def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"AGG\", smooth=20):\n",
    "    g = df.groupby(keys, observed=True)[target].agg([\"mean\",\"size\"]).reset_index()\n",
    "    g.rename(columns={\"mean\":f\"{pref}_rate\",\"size\":f\"{pref}_n\"}, inplace=True)\n",
    "    # suavizado para grupos pequeÃ±os\n",
    "    g[f\"{pref}_rate\"] = ((g[f\"{pref}_rate\"]*g[f\"{pref}_n\"] + global_mean*smooth) / (g[f\"{pref}_n\"] + smooth)).astype(\"float32\")\n",
    "    g[f\"{pref}_n\"] = g[f\"{pref}_n\"].astype(\"int32\")\n",
    "    return g\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "    merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "    merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "    return merged\n",
    "\n",
    "# 5) Construir matrices de trabajo partiendo de las del modelo (si existen) o de X_* crudas\n",
    "Xt = X_train_model.copy() if \"X_train_model\" in globals() else X_train.copy()\n",
    "Xv = X_valid_model.copy() if \"X_valid_model\" in globals() else X_valid.copy()\n",
    "\n",
    "# 6) Asegurar que las llaves estÃ©n presentes en Xt/Xv para poder hacer merge por columnas (y no por Ã­ndice)\n",
    "for keys, _ in aggs_specs:\n",
    "    for k in keys:\n",
    "        if k not in Xt.columns:\n",
    "            Xt[k] = X_train[k].values\n",
    "        if k not in Xv.columns:\n",
    "            Xv[k] = X_valid[k].values\n",
    "\n",
    "# 7) Aplicar agregados\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "print(f\"âœ“ Agregados aplicados en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# 8) (Opcional) eliminar las llaves aÃ±adidas si no estaban antes en la matriz del modelo\n",
    "if \"X_train_model\" in globals():\n",
    "    # deja solo columnas originales del modelo + nuevas *_rate/_n\n",
    "    keep_cols = set(X_train_model.columns) | {c for c in Xt.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")}\n",
    "    drop_cols = [c for c in Xt.columns if c not in keep_cols]\n",
    "    Xt = Xt.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    Xv = Xv.drop(columns=[c for c in Xv.columns if c not in (set(X_valid_model.columns) | keep_cols)], errors=\"ignore\")\n",
    "\n",
    "# 9) Actualizar matrices del modelo\n",
    "X_train_model = Xt.copy()\n",
    "X_valid_model = Xv.copy()\n",
    "\n",
    "new_cols = sorted([c for c in X_train_model.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")])\n",
    "print(\"âœ“ Columnas nuevas:\", new_cols[:10], \"...\" if len(new_cols) > 10 else \"\")\n",
    "print(\"Shapes ->\", X_train_model.shape, X_valid_model.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ea05c",
   "metadata": {},
   "source": [
    "Paso 7 Â· Entrenamiento LightGBM con early stopping (balanceo por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d12841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.304088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's auc: 0.599939\tvalid_0's binary_logloss: 0.569545\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.596086\tvalid_0's binary_logloss: 0.459714\n",
      "âœ“ Entrenado en 563.2s | best_iter=3\n",
      "ROC-AUC valid=0.5961\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 7: entrenamiento LightGBM =====\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,  # balanceo\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "print(f\"âœ“ Entrenado en {time.time()-t0:.1f}s | best_iter={model.best_iteration_}\")\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e322e",
   "metadata": {},
   "source": [
    "Paso 8 Â· MÃ©tricas a 0.5 y bÃºsqueda de mejor F1 (umbral operativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c225b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1=0.0000 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.192) ==\n",
      "Accuracy: 0.3391 | Precision: 0.1908 | Recall: 0.8728 | F1=0.3131 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[175601 595594]\n",
      " [ 20466 140423]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3390509868209303,\n",
       " 'pre': 0.190787712783808,\n",
       " 'rec': 0.8727942867442771,\n",
       " 'f1': 0.31312757412705455,\n",
       " 'thr': 0.19210526315789472}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Paso 8: mÃ©tricas =====\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc_val:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, thr=thr)\n",
    "\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best = {\"thr\":0.0, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 20):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b443d",
   "metadata": {},
   "source": [
    "Paso 9 Â· Guardar artefactos (modelo, mappings TE, defaults, threshold y metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49d2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Guardados: modelo + TE mappings/defaults + threshold + metadata.\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 9: guardar artefactos =====\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cols_te=cols_te,\n",
    "    auc_valid=float(auc_val),\n",
    "    split={\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    note=\"LightGBM + KFold Target Encoding + agregados histÃ³ricos (sin fuga)\"\n",
    ")\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815d8f",
   "metadata": {},
   "source": [
    "(Opcional) Paso 10 Â· FunciÃ³n de predicciÃ³n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a688b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 10: funciÃ³n de predicciÃ³n (producciÃ³n) =====\n",
    "import math\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"r\", encoding=\"utf-8\") as f: te_map = json.load(f)\n",
    "with open(\"models/te_defaults.json\", \"r\", encoding=\"utf-8\") as f: te_def = json.load(f)\n",
    "with open(\"models/threshold.json\", \"r\", encoding=\"utf-8\") as f: UMBRAL_OPERATIVO = json.load(f)[\"threshold\"]\n",
    "with open(\"models/metadata.json\", \"r\", encoding=\"utf-8\") as f: meta = json.load(f)\n",
    "feat_order = meta[\"features\"]\n",
    "\n",
    "def prep_features(sample):\n",
    "    \"\"\"\n",
    "    sample: dict con al menos\n",
    "      month, day_of_week, airline, origin, destination, scheduled_hour, scheduled_minute,\n",
    "      origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    Devuelve: DataFrame(1, features) listo para model.predict_proba\n",
    "    \"\"\"\n",
    "    month = int(sample[\"month\"])\n",
    "    dow   = int(sample[\"day_of_week\"])\n",
    "    airline = str(sample[\"airline\"])\n",
    "    origin  = str(sample[\"origin\"])\n",
    "    dest    = str(sample[\"destination\"])\n",
    "    sh      = int(sample[\"scheduled_hour\"])\n",
    "    sm      = int(sample[\"scheduled_minute\"])\n",
    "    olat, olon = float(sample[\"origen_lat\"]), float(sample[\"origen_lon\"])\n",
    "    dlat, dlon = float(sample[\"dest_lat\"]), float(sample[\"dest_lon\"])\n",
    "\n",
    "    # cÃ­clicos\n",
    "    salida_sin = math.sin(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    salida_cos = math.cos(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    month_sin  = math.sin(2*math.pi * (month/12))\n",
    "    month_cos  = math.cos(2*math.pi * (month/12))\n",
    "\n",
    "    # distancia\n",
    "    def hv(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1,lon1,lat2,lon2])\n",
    "        dlat = lat2-lat1; dlon = lon2-lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist = hv(olat,olon,dlat,dlon)\n",
    "\n",
    "    ruta = f\"{origin}_{dest}\"\n",
    "\n",
    "    row = dict(\n",
    "        MONTH=month, DAY_OF_WEEK=dow,\n",
    "        SALIDA_SIN=salida_sin, SALIDA_COS=salida_cos,\n",
    "        MONTH_SIN=month_sin, MONTH_COS=month_cos,\n",
    "        DISTANCIA_HAV=dist, MINUTO_DIA_SALIDA=sh*60+sm,\n",
    "        HORA_SALIDA=sh\n",
    "    )\n",
    "\n",
    "    # TE para categorÃ­as\n",
    "    for col, val in dict(AIRLINE=airline, ORIGIN_AIRPORT=origin, DESTINATION_AIRPORT=dest, RUTA=ruta).items():\n",
    "        if f\"{col}_TE\" in feat_order:\n",
    "            mapping = te_map.get(col, {})\n",
    "            default = te_def.get(col, float(np.mean(list(mapping.values()) or [0.18])))\n",
    "            row[f\"{col}_TE\"] = float(mapping.get(str(val), default))\n",
    "\n",
    "    X = pd.DataFrame([row], columns=feat_order).fillna(0)\n",
    "    return X\n",
    "\n",
    "def predecir(sample):\n",
    "    X1 = prep_features(sample)\n",
    "    proba = float(model.predict_proba(X1)[:,1])\n",
    "    pred  = int(proba >= UMBRAL_OPERATIVO)\n",
    "    return dict(prob=proba, delayed=pred, thr=UMBRAL_OPERATIVO)\n",
    "\n",
    "# Ejemplo:\n",
    "# predecir(dict(\n",
    "#   month=11, day_of_week=5, airline=\"AA\", origin=\"JFK\", destination=\"MIA\",\n",
    "#   scheduled_hour=14, scheduled_minute=30, origen_lat=40.6413, origen_lon=-73.7781,\n",
    "#   dest_lat=25.7959, dest_lon=-80.2870\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4adc5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral para PrecisiÃ³n â‰¥ 0.3: 0.229 | PrecisiÃ³n=0.308 | Recall=0.017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba)\n",
    "# encuentra el primer umbral con precisiÃ³n >= 0.30\n",
    "target_prec = 0.30\n",
    "idx = np.where(prec >= target_prec)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_op = thr[i-1] if i>0 else 0.99\n",
    "    print(f\"Umbral para PrecisiÃ³n â‰¥ {target_prec}: {thr_op:.3f} | PrecisiÃ³n={prec[i]:.3f} | Recall={rec[i]:.3f}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisiÃ³n en el set actual.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e89ef4",
   "metadata": {},
   "source": [
    "calcular el umbral a una precisiÃ³n objetivo (o recall objetivo),\n",
    "\n",
    "y un pequeÃ±o grid de LightGBM listo para pegar y correr con early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a837143",
   "metadata": {},
   "source": [
    "1) Elegir umbral operativo (por precisiÃ³n objetivo o por recall objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddda93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Umbral por PrecisiÃ³n â‰¥ 0.3 (thr=0.229) ==\n",
      "Accuracy: 0.8238 | Precision: 0.3080 | Recall: 0.0166 | F1: 0.0315 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[765203   5992]\n",
      " [158222   2667]]\n",
      "\n",
      "== Umbral por Recall â‰¥ 0.7 (thr=0.200) ==\n",
      "Accuracy: 0.4825 | Precision: 0.2062 | Recall: 0.7010 | F1: 0.3186 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[336982 434213]\n",
      " [ 48113 112776]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Usa las probabilidades del modelo ya entrenado\n",
    "y_proba_valid = model.predict_proba(X_valid_model)[:, 1]\n",
    "\n",
    "def report_at_threshold(y_true, y_proba, thr, title=\"\"):\n",
    "    y_hat = (y_proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# --- a) Umbral para alcanzar una precisiÃ³n objetivo (ej: 0.30) ---\n",
    "target_precision = 0.30\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba_valid)\n",
    "# thr tiene len = len(prec)-1 => alinear Ã­ndices\n",
    "idx = np.where(prec[:-1] >= target_precision)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_prec = float(thr[i])\n",
    "    res_prec = report_at_threshold(y_valid, y_proba_valid, thr_prec, f\"Umbral por PrecisiÃ³n â‰¥ {target_precision}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisiÃ³n en el set de validaciÃ³n.\")\n",
    "\n",
    "# --- b) Umbral para alcanzar un recall objetivo (ej: 0.70) ---\n",
    "target_recall = 0.70\n",
    "# Buscar umbral mÃ­nimo que consiga ese recall (prioriza mayor umbral para bajar falsos positivos)\n",
    "idx = np.where(rec[:-1] >= target_recall)[0]\n",
    "if len(idx):\n",
    "    i = idx[-1]  # el mÃ¡s alto que aÃºn cumple el recall objetivo\n",
    "    thr_rec = float(thr[i])\n",
    "    res_rec = report_at_threshold(y_valid, y_proba_valid, thr_rec, f\"Umbral por Recall â‰¥ {target_recall}\")\n",
    "else:\n",
    "    print(\"No se alcanza ese recall en el set de validaciÃ³n.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5fc01",
   "metadata": {},
   "source": [
    "2) Mini-grid de LightGBM con early stopping (rÃ¡pido y efectivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c766e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.670011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.535804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.4 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.536024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=3.2 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.9 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.446640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.369460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.7 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.485948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.467874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.1 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.452842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602254\tvalid_0's binary_logloss: 0.581103\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.424657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m     params[k] = v\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProbando:\u001b[39m\u001b[33m\"\u001b[39m, {k:params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys})\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m auc, secs, mdl = \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAUC valid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | tiempo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecs/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min | best_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmdl.best_iteration_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > best[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_eval\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     32\u001b[39m model = lgb.LGBMClassifier(**params)\n\u001b[32m     33\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m secs = time.time()-t0\n\u001b[32m     41\u001b[39m proba = model.predict_proba(X_valid_model)[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time, itertools\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Clase balanceada por ratio neg/pos (ya lo usaste antes)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "base = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,          # se explora mÃ¡s abajo tambiÃ©n 0.03\n",
    "    n_estimators=12000,          # early_stopping lo frenarÃ¡ antes si no mejora\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,               # bagging_fraction\n",
    "    colsample_bytree=0.85,       # feature_fraction\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"learning_rate\": [0.05, 0.03],\n",
    "    \"num_leaves\":    [63, 127],\n",
    "    \"min_child_samples\": [100, 300],\n",
    "    \"colsample_bytree\":  [0.8, 0.9],\n",
    "    \"subsample\":         [0.8, 0.9],\n",
    "}\n",
    "\n",
    "def train_eval(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    model.fit(\n",
    "        X_train_model, y_train,\n",
    "        eval_set=[(X_valid_model, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = model.predict_proba(X_valid_model)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return auc, secs, model\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"secs\": None, \"model\": None}\n",
    "\n",
    "keys = list(grid.keys())\n",
    "for values in itertools.product(*[grid[k] for k in keys]):\n",
    "    params = base.copy()\n",
    "    for k,v in zip(keys, values):\n",
    "        params[k] = v\n",
    "    print(\"\\nProbando:\", {k:params[k] for k in keys})\n",
    "    auc, secs, mdl = train_eval(params)\n",
    "    print(f\"AUC valid={auc:.4f} | tiempo={secs/60:.1f} min | best_iter={mdl.best_iteration_}\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best = {\"auc\": auc, \"params\": params, \"secs\": secs, \"model\": mdl}\n",
    "\n",
    "print(\"\\n=== MEJOR CONFIGURACIÃ“N ===\")\n",
    "print(best[\"params\"])\n",
    "print(f\"AUC valid={best['auc']:.4f} | tiempo={best['secs']/60:.1f} min | best_iter={best['model'].best_iteration_}\")\n",
    "\n",
    "# Opcional: guardar el mejor modelo\n",
    "# from joblib import dump\n",
    "# dump(best[\"model\"], \"models/lgbm_best.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21757cd8",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44b5e8",
   "metadata": {},
   "source": [
    "Carga eficiente + dtypes compactos\n",
    "\n",
    "DerivaciÃ³n de features\n",
    "\n",
    "Split temporal (1â€“9 vs 10â€“12)\n",
    "\n",
    "Target Encoding KFold sin fuga\n",
    "\n",
    "Agregados histÃ³ricos sin fuga\n",
    "\n",
    "Entrenamiento LightGBM + mini-bÃºsqueda\n",
    "\n",
    "SelecciÃ³n de umbral operativo\n",
    "\n",
    "Guardado de artefactos\n",
    "\n",
    "FunciÃ³n de scoring para producciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eac698",
   "metadata": {},
   "source": [
    "1) Carga eficiente del CSV (solo columnas Ãºtiles + dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fb8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Columnas presentes: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA']\n",
      "â†’ Faltantes (se derivarÃ¡n si aplica): []\n",
      "âœ“ Cargado: (5231130, 14)  |  en 36.3s\n",
      "âœ“ Rate retraso: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\",\n",
    "    # opcional de anÃ¡lisis (no para predecir): \"MOTIVO_RETRASO\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"â†’ Columnas presentes:\", present)\n",
    "print(\"â†’ Faltantes (se derivarÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"MOTIVO_RETRASO\":\"category\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ“ Cargado: {v.shape}  |  en {t1-t0:.1f}s\")\n",
    "print(\"âœ“ Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c109b3",
   "metadata": {},
   "source": [
    "2) Derivar features (distancia, estacionalidad, hora/minuto, ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f7b7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Derivar features =====\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    a = np.sin((lat2-lat1)/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin((lon2-lon1)/2)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns):\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"],v[\"ORIGEN_LON\"],v[\"DEST_LAT\"],v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mensual\n",
    "if \"MONTH\" in v:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "\n",
    "# Hora / minuto del dÃ­a programados\n",
    "if \"SCHEDULED_DEPARTURE\" in v:\n",
    "    mins = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    hrs  = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hrs*60 + mins).astype(\"int32\")\n",
    "    v[\"HORA_SALIDA\"]       = hrs.astype(\"int16\")\n",
    "\n",
    "# Ruta texto\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"âœ“ Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0ffbf",
   "metadata": {},
   "source": [
    "3) SelecciÃ³n de variables (features/target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e20c1776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) SelecciÃ³n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# usa solo lo que exista\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444bc1b",
   "metadata": {},
   "source": [
    "4) Split temporal (train=meses 1â€“9, valid=10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c906f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split â†’ X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) Split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’ X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ba89e",
   "metadata": {},
   "source": [
    "5) Target Encoding (KFold sin fuga) sobre categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b95fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TE aplicado | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Target Encoding KFold (sin fuga) =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        stats = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = (\n",
    "            train_df.iloc[val_idx][col].astype(str).map(smoothed)\n",
    "                  .fillna(global_mean).astype(np.float32)\n",
    "        )\n",
    "\n",
    "    full_stats = y.groupby(train_df[col].astype(str)).mean()\n",
    "    full_cnts  = y.groupby(train_df[col].astype(str)).size()\n",
    "    mapping = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# quitar columnas categÃ³ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ“ TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b05f9",
   "metadata": {},
   "source": [
    "6) Agregados histÃ³ricos (sin fuga) y join a las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 6) Agregados histÃ³ricos (1â€“9) =====\n",
    "# global_mean = float(y_train.mean())\n",
    "\n",
    "# v_train = v.loc[train_mask, [\"RETRASADO_LLEGADA\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]].copy()\n",
    "\n",
    "# # def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "# #     g = df.groupby(keys)[target]\n",
    "# #     stats = g.mean()\n",
    "# #     cnts  = g.size()\n",
    "# #     agg = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "# #     out = agg.reset_index().rename(columns={0:f\"{pref}_rate\"})\n",
    "# #     out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "# #     return out\n",
    "\n",
    "# # def left_join_agg(X_left, keys, pref):\n",
    "# #     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "# #     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "# #     merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "# #     merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "# #     return merged\n",
    "\n",
    "# # --- FIX funciones de agregados ---\n",
    "\n",
    "# def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "#     \"\"\"\n",
    "#     Devuelve un DF con columnas: keys + [f\"{pref}_rate\", f\"{pref}_n\"]\n",
    "#     usando suavizado para evitar overfitting en claves con pocos registros.\n",
    "#     \"\"\"\n",
    "#     g = df.groupby(keys)[target]\n",
    "#     stats = g.mean()                         # media por clave\n",
    "#     cnts  = g.size()                         # conteo por clave\n",
    "\n",
    "#     # smoothed rate (float32) y count (int32)\n",
    "#     smoothed = ((stats*cnts + global_mean*smooth) / (cnts + smooth)).astype(\"float32\")\n",
    "#     cnts     = cnts.astype(\"int32\")\n",
    "\n",
    "#     # Â¡OJO! Convertimos explÃ­citamente a DataFrame con nombres correctos\n",
    "#     out_rate = smoothed.to_frame(name=f\"{pref}_rate\").reset_index()\n",
    "#     out_cnt  = cnts.to_frame(name=f\"{pref}_n\").reset_index()\n",
    "\n",
    "#     # Unimos por las llaves\n",
    "#     out = out_rate.merge(out_cnt, on=keys, how=\"left\")\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def left_join_agg(X_left, keys, pref):\n",
    "#     \"\"\"\n",
    "#     Hace left-join contra el DF de agregados y rellena NaN con defaults.\n",
    "#     \"\"\"\n",
    "#     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "\n",
    "#     # merge\n",
    "#     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "\n",
    "#     # asegurar que las columnas existan (si no, crearlas con defaults)\n",
    "#     rate_col = f\"{pref}_rate\"\n",
    "#     n_col    = f\"{pref}_n\"\n",
    "\n",
    "#     if rate_col not in merged.columns:\n",
    "#         merged[rate_col] = np.nan\n",
    "#     if n_col not in merged.columns:\n",
    "#         merged[n_col] = np.nan\n",
    "\n",
    "#     merged[rate_col] = merged[rate_col].fillna(global_mean).astype(\"float32\")\n",
    "#     merged[n_col]    = merged[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "#     return merged\n",
    "\n",
    "\n",
    "# Xt = X_train_model.copy()\n",
    "# Xv = X_valid_model.copy()\n",
    "\n",
    "# # AÃ±adimos llaves temporales para poder unir\n",
    "# Xt[\"AIRLINE\"] = v.loc[train_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xt[\"ORIGIN_AIRPORT\"] = v.loc[train_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xt[\"DESTINATION_AIRPORT\"] = v.loc[train_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xt[\"RUTA\"] = v.loc[train_mask, \"RUTA\"].astype(str).values\n",
    "# Xt[\"HORA_SALIDA\"] = v.loc[train_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# Xv[\"AIRLINE\"] = v.loc[valid_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xv[\"ORIGIN_AIRPORT\"] = v.loc[valid_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xv[\"DESTINATION_AIRPORT\"] = v.loc[valid_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xv[\"RUTA\"] = v.loc[valid_mask, \"RUTA\"].astype(str).values\n",
    "# Xv[\"HORA_SALIDA\"] = v.loc[valid_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# aggs_specs = [\n",
    "#     ([\"AIRLINE\"], \"AIR\"),\n",
    "#     ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "#     ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "#     ([\"RUTA\"], \"RUTA\"),\n",
    "#     ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "# ]\n",
    "\n",
    "# t0 = time.time()\n",
    "# for keys, pref in aggs_specs:\n",
    "#     Xt = left_join_agg(Xt, keys, pref)\n",
    "#     Xv = left_join_agg(Xv, keys, pref)\n",
    "# # limpiar llaves temporales\n",
    "# drop_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "# Xt.drop(columns=[c for c in drop_keys if c in Xt], inplace=True)\n",
    "# Xv.drop(columns=[c for c in drop_keys if c in Xv], inplace=True)\n",
    "# t1 = time.time()\n",
    "\n",
    "# print(f\"âœ“ Agregados aplicados en {t1-t0:.1f}s\")\n",
    "# print(\"Shapes ->\", Xt.shape, Xv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8afbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agregados aplicados en 18.5s\n",
      "âœ“ Columnas nuevas: ['AIR_rate', 'AIR_n', 'DES_rate', 'DES_n', 'ORI_rate', 'ORI_n', 'RUTA_rate', 'RUTA_n', 'RUTA_HORA_rate', 'RUTA_HORA_n']\n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6) Agregados histÃ³ricos (sin fuga) y uniÃ³n a matrices del modelo\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- 6.1 Asegurar variables base ---\n",
    "assert 'RETRASADO_LLEGADA' in v.columns, \"No existe la columna target en 'v'.\"\n",
    "target = 'RETRASADO_LLEGADA'\n",
    "\n",
    "# Si no existen las mÃ¡scaras, las recreamos\n",
    "if 'train_mask' not in globals() or 'valid_mask' not in globals():\n",
    "    train_mask = v['MONTH'].between(1, 9)\n",
    "    valid_mask = v['MONTH'].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask].copy()                # SOLO meses 1â€“9 para evitar fuga\n",
    "global_mean = float(v_train[target].mean())       # media global para smoothing/NaN\n",
    "\n",
    "# Asegurar columnas de llaves en v (por si faltan)\n",
    "if 'RUTA' not in v.columns and {'ORIGIN_AIRPORT','DESTINATION_AIRPORT'}.issubset(v.columns):\n",
    "    v['RUTA'] = (v['ORIGIN_AIRPORT'].astype(str) + '_' +\n",
    "                 v['DESTINATION_AIRPORT'].astype(str))\n",
    "\n",
    "if 'HORA_SALIDA' not in v.columns and 'SCHEDULED_DEPARTURE' in v.columns:\n",
    "    # SCHEDULED_DEPARTURE en HHMM (int). Tomamos la hora.\n",
    "    v['HORA_SALIDA'] = (v['SCHEDULED_DEPARTURE'] // 100).clip(0, 23).astype('int16')\n",
    "\n",
    "# --- 6.2 Funciones robustas de agregados y merge ---\n",
    "def build_agg(df, keys, target='RETRASADO_LLEGADA', pref='', smooth=20):\n",
    "    \"\"\"\n",
    "    Devuelve DataFrame con columnas: keys + [f'{pref}_rate', f'{pref}_n'].\n",
    "    rate con suavizado; n = conteo.\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "\n",
    "    smoothed = ((stats * cnts + global_mean * smooth) / (cnts + smooth)).astype('float32')\n",
    "    cnts     = cnts.astype('int32')\n",
    "\n",
    "    out_rate = smoothed.to_frame(name=f'{pref}_rate').reset_index()\n",
    "    out_cnt  = cnts.to_frame(name=f'{pref}_n').reset_index()\n",
    "    out = out_rate.merge(out_cnt, on=keys, how='left')\n",
    "    return out\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    \"\"\"\n",
    "    Left join contra el DF de agregados (solo entrenado con v_train).\n",
    "    Rellena NaN con defaults: rate=global_mean, n=0.\n",
    "    \"\"\"\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how='left')\n",
    "\n",
    "    rate_col = f'{pref}_rate'\n",
    "    n_col    = f'{pref}_n'\n",
    "    if rate_col not in merged.columns:\n",
    "        merged[rate_col] = np.nan\n",
    "    if n_col not in merged.columns:\n",
    "        merged[n_col] = np.nan\n",
    "\n",
    "    merged[rate_col] = merged[rate_col].fillna(global_mean).astype('float32')\n",
    "    merged[n_col]    = merged[n_col].fillna(0).astype('int32')\n",
    "    return merged\n",
    "\n",
    "# --- 6.3 Preparar matrices Xt/Xv con llaves temporales (sin fuga) ---\n",
    "Xt = X_train_model.copy()\n",
    "Xv = X_valid_model.copy()\n",
    "\n",
    "# Agregamos llaves desde 'v' usando el Ã­ndice original (alineaciÃ³n 1:1)\n",
    "for col in ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']:\n",
    "    if col in v.columns:\n",
    "        if col not in Xt.columns:\n",
    "            Xt[col] = v.loc[X_train_model.index, col].values\n",
    "        if col not in Xv.columns:\n",
    "            Xv[col] = v.loc[X_valid_model.index, col].values\n",
    "\n",
    "# --- 6.4 EspecificaciÃ³n de agregados a calcular ---\n",
    "aggs_specs = []\n",
    "if 'AIRLINE' in Xt.columns:\n",
    "    aggs_specs.append((['AIRLINE'], 'AIR'))\n",
    "if 'DESTINATION_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['DESTINATION_AIRPORT'], 'DES'))\n",
    "if 'ORIGIN_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['ORIGIN_AIRPORT'], 'ORI'))\n",
    "if 'RUTA' in Xt.columns:\n",
    "    aggs_specs.append((['RUTA'], 'RUTA'))\n",
    "if {'RUTA', 'HORA_SALIDA'}.issubset(Xt.columns):\n",
    "    aggs_specs.append((['RUTA', 'HORA_SALIDA'], 'RUTA_HORA'))  # opcional, suele ayudar\n",
    "\n",
    "# --- 6.5 Aplicar agregados y limpiar llaves temporales ---\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "\n",
    "# Eliminamos llaves si no eran parte de X_*_model original\n",
    "temp_keys = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']\n",
    "Xt.drop(columns=[c for c in temp_keys if c in Xt.columns and c not in X_train_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "Xv.drop(columns=[c for c in temp_keys if c in Xv.columns and c not in X_valid_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "new_cols = [c for c in Xt.columns if c.endswith('_rate') or c.endswith('_n')]\n",
    "print(f\"âœ“ Agregados aplicados en {t1-t0:.1f}s\")\n",
    "print(\"âœ“ Columnas nuevas:\", new_cols)\n",
    "print(\"Shapes ->\", Xt.shape, Xv.shape)\n",
    "\n",
    "# Actualizamos las matrices del modelo para usar estas versiones enriquecidas\n",
    "X_train_model = Xt\n",
    "X_valid_model = Xv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5c90c",
   "metadata": {},
   "source": [
    "7) Entrenamiento LightGBM + mini-bÃºsqueda + AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07595942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 7) Entrenamiento LightGBM =====\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "# grid = [\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=300, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.03, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "# ]\n",
    "\n",
    "# best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "# for p in grid:\n",
    "#     params = dict(\n",
    "#         objective=\"binary\",\n",
    "#         learning_rate=p[\"learning_rate\"],\n",
    "#         n_estimators=12000,\n",
    "#         num_leaves=p[\"num_leaves\"],\n",
    "#         min_child_samples=p[\"min_child_samples\"],\n",
    "#         subsample=p[\"subsample\"],\n",
    "#         colsample_bytree=p[\"colsample_bytree\"],\n",
    "#         reg_alpha=0.0,\n",
    "#         reg_lambda=5.0,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#         scale_pos_weight=scale_pos_weight,\n",
    "#     )\n",
    "#     print(\"Probando:\", {k:params[k] for k in [\"learning_rate\",\"num_leaves\",\"min_child_samples\",\"colsample_bytree\",\"subsample\"]})\n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     t0 = time.time()\n",
    "#     model.fit(\n",
    "#         Xt, y_train,\n",
    "#         eval_set=[(Xv, y_valid)],\n",
    "#         eval_metric=\"auc\",\n",
    "#         callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "#     )\n",
    "#     t1 = time.time()\n",
    "#     val_proba = model.predict_proba(Xv)[:,1]\n",
    "#     auc_val = roc_auc_score(y_valid, val_proba)\n",
    "#     print(f\"AUC valid={auc_val:.4f} | tiempo={(t1-t0)/60:.1f} min | best_iter={model.best_iteration_}\\n\")\n",
    "#     if auc_val > best[\"auc\"]:\n",
    "#         best.update({\"auc\": auc_val, \"model\": model, \"params\": params})\n",
    "\n",
    "# print(\"=== MEJOR CONFIGURACIÃ“N ===\")\n",
    "# print(best[\"params\"])\n",
    "# print(f\"AUC valid={best['auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53af73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.602295\tvalid_0's binary_logloss: 0.583084\n",
      "[400]\tvalid_0's auc: 0.602473\tvalid_0's binary_logloss: 0.580826\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602496\tvalid_0's binary_logloss: 0.460413\n",
      "âœ… Entrenado en 261.8s | best_iter=1 | ROC-AUC valid=0.6025\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4275 | Precision: 0.2009 | Recall: 0.7782 | F1: 0.3194 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[273276 497919]\n",
      " [ 35688 125201]]\n",
      "â†’ Umbral F1 Ã³ptimo: {'thr': 0.2, 'f1': 0.3193866396941872}\n",
      "\n",
      "== Umbral por PrecisiÃ³n â‰³ 0.30 (aprox) (thr=0.209) ==\n",
      "Accuracy: 0.7000 | Precision: 0.2434 | Recall: 0.3499 | F1: 0.2871 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[596171 175024]\n",
      " [104588  56301]]\n",
      "\n",
      "== Umbral por Recall â‰¥ 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.17261212508743848,\n",
       " 'pre': 0.17261212508743848,\n",
       " 'rec': 1.0,\n",
       " 'f1': 0.2944061747179482,\n",
       " 'auc': 0.6024960911474394,\n",
       " 'thr': 0.05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 7) Entrenar LightGBM + mÃ©tricas/umbrales\n",
    "# ================================\n",
    "import time\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Peso para desbalance (cÃ¡lculo en TRAIN)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/ max(pos,1), 1.0)\n",
    "\n",
    "# Mejores hiperparÃ¡metros que te rindieron (puedes ajustarlos si deseas)\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=12000,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n",
    "\n",
    "# ---- MÃ©tricas helper ----\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_05 = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# BÃºsqueda de mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo:\", best)\n",
    "\n",
    "# Umbral operativo por objetivo de negocio (opcional)\n",
    "# - si quieres â‰¥ 0.30 de precisiÃ³n\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best[\"thr\"]  # heurÃ­stica simple\n",
    "report_metrics(y_valid, valid_proba, thr_prec, \"Umbral por PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "\n",
    "# - si quieres â‰¥ 0.70 de recall (recuperaciÃ³n de retrasos)\n",
    "# barrido rÃ¡pido hasta lograr >=0.70 recall\n",
    "thr_rec = best[\"thr\"]\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    rec = recall_score(y_valid, (valid_proba >= thr).astype(int), zero_division=0)\n",
    "    if rec >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "report_metrics(y_valid, valid_proba, thr_rec, \"Umbral por Recall â‰¥ 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73177a53",
   "metadata": {},
   "source": [
    "8) Umbrales operativos (precisiÃ³n mÃ­nima vs recall mÃ­nimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da54e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\lgbm_retrasos.pkl | best_iteration=1\n",
      "âœ… Target Encoding guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\target_encoding.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agregados histÃ³ricos guardados: 5 tablas.\n",
      "âœ… Orden de features guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\feature_order.json\n",
      "âœ… Metadatos guardados en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata.json\n",
      "âœ… Schema de entrada guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\input_schema.json\n",
      "\n",
      "ğŸ” Smoke test de recargaâ€¦\n",
      "Smoke AUC (mini valid): 0.6169\n",
      "âœ… Artefactos listos.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8) Guardar artefactos del modelo y metadatos\n",
    "# ============================================\n",
    "import os, json, time, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "# 8.1 Guardar el modelo LightGBM (con best_iteration en metadatos)\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "print(f\"âœ… Modelo guardado en: {model_path} | best_iteration={best_iter}\")\n",
    "\n",
    "# 8.2 Preparar y guardar mapeos de Target Encoding (si existen)\n",
    "#   - mappings: dict por columna {categoria -> media_suavizada}\n",
    "#   - defaults: dict por columna {default_global_mean}\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "if \"mappings\" in globals() and \"defaults\" in globals():\n",
    "    joblib.dump({\"mappings\": mappings, \"defaults\": defaults}, te_path)\n",
    "    print(f\"âœ… Target Encoding guardado en: {te_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No se encontraron 'mappings'/'defaults' para Target Encoding; se omite guardado TE.\")\n",
    "\n",
    "# 8.3 Guardar tablas de agregados histÃ³ricos (si tienes funciones y specs)\n",
    "#     Reconstituimos los agregados con v_train para congelar el 'histÃ³rico' sin fuga.\n",
    "agg_tables = {}\n",
    "if \"aggs_specs\" in globals() and \"v_train\" in globals() and \"build_agg\" in globals():\n",
    "    for keys, pref in aggs_specs:\n",
    "        try:\n",
    "            agg_df = build_agg(v_train, keys, target=\"RETRASADO_LLEGADA\", pref=pref, smooth=20)\n",
    "            # Guardar cada agregado como CSV individual\n",
    "            csv_path = os.path.join(ARTIF_DIR, f\"agg_{pref}.csv\")\n",
    "            agg_df.to_csv(csv_path, index=False)\n",
    "            agg_tables[pref] = csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  No se pudo construir/guardar el agregado '{pref}': {e}\")\n",
    "    print(f\"âœ… Agregados histÃ³ricos guardados: {len(agg_tables)} tablas.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No se encontraron 'aggs_specs'/'v_train'/'build_agg'; se omite guardado de agregados.\")\n",
    "\n",
    "# 8.4 Guardar orden de features finales del modelo\n",
    "feature_order = list(X_train_model.columns)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_order\": feature_order}, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "# 8.5 Guardar metadatos (umbrales, medias, info para inferencia)\n",
    "#     - umbral base 0.5 y umbral F1 Ã³ptimo (si existe 'best')\n",
    "#     - best_iteration\n",
    "#     - columnas categÃ³ricas originales usadas en TE (si existen)\n",
    "#     - global_mean para TE/aggregates (si existe)\n",
    "meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": model_path,\n",
    "    \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "    \"thresholds\": {\n",
    "        \"base_05\": 0.5,\n",
    "        \"best_f1\": float(best[\"thr\"]) if \"best\" in globals() and isinstance(best, dict) and \"thr\" in best else None\n",
    "    },\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"agg_tables\": agg_tables,  # dict {pref -> csv_path}\n",
    "    \"global_mean\": float(global_mean) if \"global_mean\" in globals() else None,\n",
    "    \"te_cols\": list(mappings.keys()) if \"mappings\" in globals() else []\n",
    "}\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Metadatos guardados en: {meta_path}\")\n",
    "\n",
    "# 8.6 (Opcional) Guardar un \"schema\" ligero de dtypes esperados para entrada cruda\n",
    "schema = {\n",
    "    \"required_raw_fields\": [\n",
    "        \"MONTH\",\"DAY_OF_WEEK\",\n",
    "        \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "        \"SCHEDULED_DEPARTURE\",  # para reconstruir MINUTO_DIA_SALIDA/SALIDA_SIN/COS en producciÃ³n si hace falta\n",
    "        \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"  # para DISTANCIA_HAV si no viene en la entrada\n",
    "    ],\n",
    "    \"notes\": \"Si la entrada ya trae SALIDA_SIN/SALIDA_COS/DISTANCIA_HAV, se usan directamente.\"\n",
    "}\n",
    "schema_path = os.path.join(ARTIF_DIR, \"input_schema.json\")\n",
    "with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Schema de entrada guardado en: {schema_path}\")\n",
    "\n",
    "# 8.7 Smoke test de recarga rÃ¡pida\n",
    "print(\"\\nğŸ” Smoke test de recargaâ€¦\")\n",
    "_loaded_model = joblib.load(model_path)\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    _feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Usamos un mini-slice de validaciÃ³n para probar predict_proba\n",
    "idx = np.random.RandomState(42).choice(len(X_valid_model), size=min(5000, len(X_valid_model)), replace=False)\n",
    "mini_Xv = X_valid_model.iloc[idx][_feature_order]\n",
    "mini_yv = y_valid.iloc[idx]\n",
    "\n",
    "proba = _loaded_model.predict_proba(mini_Xv, num_iteration=getattr(_loaded_model, \"best_iteration_\", None))[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Smoke AUC (mini valid): {roc_auc_score(mini_yv, proba):.4f}\")\n",
    "print(\"âœ… Artefactos listos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb15bf6",
   "metadata": {},
   "source": [
    "Paso 9 Â· Cargador de artefactos + funciones de preprocesamiento para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4da7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos cargados. Features esperadas: 23\n",
      "TE cols: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA'] | Agregados: ['AIR', 'DES', 'ORI', 'RUTA', 'RUTA_HORA']\n",
      "Umbrales -> base: 0.5 | best_f1: 0.2 | best_iter: 1\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9) Cargar artefactos y helpers\n",
    "# ================================\n",
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# 9.1 Cargar artefactos\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "feat_path  = os.path.join(ARTIF_DIR,  \"feature_order.json\")\n",
    "meta_path  = os.path.join(ARTIF_DIR,  \"metadata.json\")\n",
    "te_path    = os.path.join(ARTIF_DIR,  \"target_encoding.pkl\")\n",
    "\n",
    "model        = joblib.load(model_path)\n",
    "feature_order = json.load(open(feat_path, \"r\", encoding=\"utf-8\"))[\"feature_order\"]\n",
    "META          = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Target Encoding (si existÃ­a)\n",
    "TE = None\n",
    "if os.path.exists(te_path):\n",
    "    TE = joblib.load(te_path)  # dict {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = TE[\"mappings\"]\n",
    "    defaults = TE[\"defaults\"]\n",
    "    te_cols  = list(mappings.keys())\n",
    "else:\n",
    "    mappings, defaults, te_cols = {}, {}, []\n",
    "\n",
    "# Agregados histÃ³ricos (si existen)\n",
    "agg_tables = META.get(\"agg_tables\", {})  # {pref: path_csv}\n",
    "aggs = {pref: pd.read_csv(path) for pref, path in agg_tables.items() if os.path.exists(path)}\n",
    "\n",
    "# Umbrales\n",
    "thr_base  = META[\"thresholds\"][\"base_05\"] or 0.5\n",
    "thr_bestf = META[\"thresholds\"][\"best_f1\"] if META[\"thresholds\"][\"best_f1\"] is not None else thr_base\n",
    "\n",
    "best_iter = META.get(\"best_iteration\", None)\n",
    "global_mean = META.get(\"global_mean\", None)\n",
    "print(\"Artefactos cargados. Features esperadas:\", len(feature_order))\n",
    "print(\"TE cols:\", te_cols, \"| Agregados:\", list(aggs.keys()))\n",
    "print(\"Umbrales -> base:\", thr_base, \"| best_f1:\", thr_bestf, \"| best_iter:\", best_iter)\n",
    "\n",
    "# 9.2 Helpers para reconstruir features si vienen crudas (producciÃ³n)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # MINUTO_DIA_SALIDA + SALIDA_SIN/COS (si no vienen)\n",
    "    if \"SALIDA_SIN\" not in out.columns or \"SALIDA_COS\" not in out.columns:\n",
    "        if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "            hs = (out[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (out[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            out[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "        if \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "            out[\"SALIDA_SIN\"] = np.sin(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "            out[\"SALIDA_COS\"] = np.cos(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "\n",
    "    # DISTANCIA_HAV (si no viene) â€” requiere coords\n",
    "    needs_dist = \"DISTANCIA_HAV\" not in out.columns\n",
    "    has_coords = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns)\n",
    "    if needs_dist and has_coords:\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # Tipos bÃ¡sicos\n",
    "    if \"MONTH\" in out:        out[\"MONTH\"]        = out[\"MONTH\"].astype(\"int16\", errors=\"ignore\")\n",
    "    if \"DAY_OF_WEEK\" in out:  out[\"DAY_OF_WEEK\"]  = out[\"DAY_OF_WEEK\"].astype(\"int16\", errors=\"ignore\")\n",
    "    for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]:\n",
    "        if c in out:\n",
    "            out[c] = out[c].astype(\"object\")  # TE espera object/str para mapear\n",
    "\n",
    "    return out\n",
    "\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "        # Quitamos la cruda si no estaba en feature_order\n",
    "        if c not in feature_order and c in out.columns:\n",
    "            out.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        # Detectar las llaves del agregado por los nombres de columnas del CSV\n",
    "        keys = [c for c in agg_df.columns if c.endswith((\"_AIRLINE\",\"_DEST\",\"_ORIG\",\"_RUTA\")) or c in [\"AIRLINE\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]]\n",
    "        # Si no detecta, intenta heurÃ­sticas por prefijo\n",
    "        if not keys:\n",
    "            if pref == \"AIR\":  keys = [\"AIRLINE\"]\n",
    "            if pref == \"DES\":  keys = [\"DESTINATION_AIRPORT\"]\n",
    "            if pref == \"ORI\":  keys = [\"ORIGIN_AIRPORT\"]\n",
    "            if pref == \"RUTA\": keys = [\"RUTA\"]\n",
    "            if pref == \"RUTA_HORA\": keys = [\"RUTA\",\"HORA_SALIDA\"]\n",
    "        # Merge left\n",
    "        out = out.merge(agg_df, on=keys, how=\"left\")\n",
    "        # Completar nulos con valores razonables\n",
    "        rate_col = f\"{pref}_rate\"; n_col = f\"{pref}_n\"\n",
    "        if rate_col in out:\n",
    "            fill_rate = global_mean if global_mean is not None else out[rate_col].mean()\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out:\n",
    "            out[n_col]    = out[n_col].fillna(0).astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def align_features(df: pd.DataFrame, order: list[str]) -> pd.DataFrame:\n",
    "    # Crea cualquier falta con 0.0, y descarta extras\n",
    "    X = df.copy()\n",
    "    for c in order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0.0\n",
    "    X = X[order]\n",
    "    # Dtypes suaves para LightGBM\n",
    "    for c in X.select_dtypes(include=\"float\").columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    for c in X.select_dtypes(include=\"int\").columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d05a2",
   "metadata": {},
   "source": [
    "Paso 10 Â· FunciÃ³n de scoring (proba + clase) y ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ba6bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Artefactos cargados | Modelo: lgbm_retrasos.pkl | Features: 23 | Agregados: 5 | TE cols: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba_retraso</th>\n",
       "      <th>pred_clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proba_retraso  pred_clase\n",
       "0         0.2113           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10) Scoring / Inferencia: cargar artefactos y predecir\n",
    "# ============================================\n",
    "import os, json, joblib, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----- 10.1 Cargar artefactos guardados en el paso 8 -----\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# Modelo LightGBM\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Metadatos (umbrales, best_iteration, rutas de agregados, etc.)\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    META = json.load(f)\n",
    "\n",
    "best_iter     = META.get(\"best_iteration\", None)\n",
    "feature_order = None\n",
    "thr_base      = META.get(\"thresholds\", {}).get(\"base_05\", 0.5)\n",
    "thr_bestf     = META.get(\"thresholds\", {}).get(\"best_f1\", None)\n",
    "global_mean   = META.get(\"global_mean\", None)  # media global del target (para TE y agregados)\n",
    "\n",
    "# Orden de features (columna a columna, EXACTO como el modelo fue entrenado)\n",
    "feat_path = META.get(\"feature_order_path\", os.path.join(ARTIF_DIR, \"feature_order.json\"))\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Target Encoding (mappings y defaults)\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "mappings, defaults, te_cols = {}, {}, []\n",
    "if os.path.exists(te_path):\n",
    "    te_obj   = joblib.load(te_path)  # {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = te_obj.get(\"mappings\", {})\n",
    "    defaults = te_obj.get(\"defaults\", {})\n",
    "    te_cols  = list(mappings.keys())\n",
    "\n",
    "# Agregados histÃ³ricos (tablas CSV por prefijo)\n",
    "# META[\"agg_tables\"] = dict { \"AIR\": \".../agg_AIR.csv\", \"ORI\": \"...\", ... }\n",
    "aggs = {}\n",
    "for pref, csv_path in META.get(\"agg_tables\", {}).items():\n",
    "    try:\n",
    "        aggs[pref] = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ No se pudo cargar agregado {pref} desde {csv_path}: {e}\")\n",
    "\n",
    "print(f\"âœ… Artefactos cargados | Modelo: {os.path.basename(model_path)} | Features: {len(feature_order)} | Agregados: {len(aggs)} | TE cols: {len(te_cols)}\")\n",
    "\n",
    "# ----- 10.2 Utilidades de ingenierÃ­a para inferencia -----\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crea/asegura columnas derivadas necesarias en producciÃ³n (sin usar el target).\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Reconstruir HORA_SALIDA/MIN_SALIDA/MINUTO_DIA_SALIDA si es necesario\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "    if \"MIN_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"MIN_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59).astype(\"int16\")\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(out.columns):\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (out[\"HORA_SALIDA\"]*60 + out[\"MIN_SALIDA\"]).astype(\"int32\")\n",
    "\n",
    "    # SALIDA_SIN / SALIDA_COS (codificaciÃ³n cÃ­clica)\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_SIN\"] = np.sin(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_COS\"] = np.cos(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # DISTANCIA_HAV si faltara y existen coordenadas\n",
    "    need_geo = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}\n",
    "    if \"DISTANCIA_HAV\" not in out.columns and need_geo.issubset(out.columns):\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN / MONTH_COS (estacionalidad)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.3 Target Encoding (no elimina columnas crudas) -----\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica TE y deja las columnas crudas intactas (las filtramos al alinear).\"\"\"\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# ----- 10.4 Agregados histÃ³ricos (usa las llaves del CSV) -----\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une agregados guardados (prefijo -> CSV). Detecta llaves como todas las columnas\n",
    "    del CSV salvo {pref+'_rate', pref+'_n'}. Si falta 'HORA_SALIDA', la reconstruye\n",
    "    a partir de SCHEDULED_DEPARTURE.\n",
    "    \"\"\"\n",
    "    if not aggs:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        rate_col = f\"{pref}_rate\"\n",
    "        n_col    = f\"{pref}_n\"\n",
    "        metric_cols = {rate_col, n_col}\n",
    "        key_cols = [c for c in agg_df.columns if c not in metric_cols]\n",
    "\n",
    "        # Reconstruye HORA_SALIDA si el agregado lo requiere\n",
    "        if \"HORA_SALIDA\" in key_cols and \"HORA_SALIDA\" not in out.columns:\n",
    "            if \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "                out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Saltando agregado {pref}: falta HORA_SALIDA y no hay SCHEDULED_DEPARTURE.\")\n",
    "                continue\n",
    "\n",
    "        missing = [c for c in key_cols if c not in out.columns]\n",
    "        if missing:\n",
    "            print(f\"âš ï¸ Saltando agregado {pref}: faltan llaves {missing}.\")\n",
    "            continue\n",
    "\n",
    "        out = out.merge(agg_df, on=key_cols, how=\"left\")\n",
    "\n",
    "        if rate_col in out.columns:\n",
    "            fill_rate = (global_mean if global_mean is not None \n",
    "                         else out[rate_col].mean(skipna=True))\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out.columns:\n",
    "            out[n_col] = out[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.5 Alinear features al orden del modelo -----\n",
    "def align_features(df: pd.DataFrame, feature_order: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mantiene solo las columnas que el modelo espera (en su orden).\n",
    "    Rellena faltantes con 0.0 para flotantes y 0 para enteros.\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            # default general: 0.0\n",
    "            X[c] = 0.0\n",
    "    X = X[feature_order]\n",
    "\n",
    "    # NaNs -> 0.0 (float) / 0 (int)\n",
    "    for c in X.columns:\n",
    "        if pd.api.types.is_float_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "        elif pd.api.types.is_integer_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0).astype(X[c].dtype)\n",
    "        else:\n",
    "            # si quedÃ³ categÃ³rica/objeto por accidente, conviÃ©rtela a string vacÃ­a y luego codifica a 0.0\n",
    "            X[c] = X[c].astype(str).fillna(\"\")\n",
    "    return X\n",
    "\n",
    "# ----- 10.6 FunciÃ³n de scoring Ãºnica -----\n",
    "def score_retraso(df_raw: pd.DataFrame, threshold: float = None):\n",
    "    \"\"\"\n",
    "    df_raw: DataFrame crudo de entrada (puede traer SCHEDULED_DEPARTURE y coords).\n",
    "    threshold: None usa el umbral best_f1 (si existe) o 0.5.\n",
    "    Devuelve: (probabilidades, clases, X_final)\n",
    "    \"\"\"\n",
    "    thr = threshold if threshold is not None else (thr_bestf if thr_bestf is not None else thr_base)\n",
    "\n",
    "    # 1) IngenierÃ­a necesaria\n",
    "    df1 = add_engineered_columns(df_raw)\n",
    "\n",
    "    # 2) Agregados histÃ³ricos (usan llaves crudas, por eso VAN ANTES del TE)\n",
    "    df2 = apply_aggregates(df1)\n",
    "\n",
    "    # 3) Target Encoding (aÃ±ade *_TE y mantiene crudas)\n",
    "    df3 = apply_target_encoding(df2)\n",
    "\n",
    "    # 4) Alinear al orden de features del modelo\n",
    "    X = align_features(df3, feature_order)\n",
    "\n",
    "    # 5) Predecir\n",
    "    proba = model.predict_proba(X, num_iteration=best_iter)[:, 1]\n",
    "    yhat  = (proba >= thr).astype(int)\n",
    "    return proba, yhat, X\n",
    "\n",
    "# ----- 10.7 Ejemplo de inferencia -----\n",
    "# Ajusta este ejemplo con rutas reales ORIGEN/DESTINO/AIRLINE de tu dataset\n",
    "ejemplo = pd.DataFrame([dict(\n",
    "    MONTH=10, DAY_OF_WEEK=5,\n",
    "    AIRLINE=\"WN\",\n",
    "    ORIGIN_AIRPORT=\"LAX\", DESTINATION_AIRPORT=\"LAS\",\n",
    "    SCHEDULED_DEPARTURE=1425,                    # HHMM\n",
    "    ORIGEN_LAT=33.9425, ORIGEN_LON=-118.4081,    # LAX\n",
    "    DEST_LAT=36.0801,  DEST_LON=-115.1522        # LAS\n",
    ")])\n",
    "\n",
    "proba, clase, X_infer = score_retraso(ejemplo, threshold=None)  # usa best_f1 si existe\n",
    "pd.DataFrame({\n",
    "    \"proba_retraso\": proba.round(4),\n",
    "    \"pred_clase\": clase\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e9d8e",
   "metadata": {},
   "source": [
    "A) XGBoost (binario, desbalance con scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3abb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.59114\n",
      "[400]\tvalid-auc:0.60046\n",
      "[409]\tvalid-auc:0.60037\n",
      "AUC valid=0.6048 | best_iter=10 | tiempo=4.2 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58746\n",
      "[400]\tvalid-auc:0.60018\n",
      "[417]\tvalid-auc:0.60026\n",
      "AUC valid=0.6051 | best_iter=17 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58871\n",
      "[400]\tvalid-auc:0.59957\n",
      "[543]\tvalid-auc:0.59910\n",
      "AUC valid=0.6004 | best_iter=143 | tiempo=6.1 min\n",
      "\n",
      "=== XGB MEJOR ===\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42} | AUC: 0.6051 | best_iter: 17\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7214 | Precision: 0.2462 | Recall: 0.2977 | F1: 0.2695 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[624549 146646]\n",
      " [112995  47894]]\n",
      "\n",
      "== Mejor F1 (thr=0.420) ==\n",
      "Accuracy: 0.4981 | Precision: 0.2106 | Recall: 0.6939 | F1: 0.3231 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[352603 418592]\n",
      " [ 49248 111641]]\n",
      "â†’ Umbral F1 Ã³ptimo (XGB): {'thr': 0.42, 'f1': 0.32307175867647103}\n",
      "\n",
      "== PrecisiÃ³n â‰³ 0.30 (aprox) (thr=0.499) ==\n",
      "Accuracy: 0.7203 | Precision: 0.2458 | Recall: 0.3000 | F1: 0.2702 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[623093 148102]\n",
      " [112622  48267]]\n",
      "\n",
      "== Recall â‰¥ 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# XGBoost: entrenamiento\n",
    "# =========================\n",
    "import time, numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_model, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid_model, label=y_valid)\n",
    "\n",
    "grid = [\n",
    "    dict(eta=0.05, max_depth=8,   min_child_weight=5,  subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.05, max_depth=10,  min_child_weight=10, subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.03, max_depth=10,  min_child_weight=10, subsample=0.9, colsample_bytree=0.9),\n",
    "]\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"booster\": None, \"nrounds\": None}\n",
    "\n",
    "for p in grid:\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": p[\"eta\"],\n",
    "        \"max_depth\": p[\"max_depth\"],\n",
    "        \"min_child_weight\": p[\"min_child_weight\"],\n",
    "        \"subsample\": p[\"subsample\"],\n",
    "        \"colsample_bytree\": p[\"colsample_bytree\"],\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"nthread\": -1,\n",
    "        \"seed\": 42,\n",
    "        # \"tree_method\": \"hist\",      # CPU rÃ¡pido\n",
    "        # \"device\": \"cuda\"            # si tienes GPU\n",
    "    }\n",
    "    print(\"Probando XGB:\", params)\n",
    "    t0 = time.time()\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=400,\n",
    "        verbose_eval=400\n",
    "    )\n",
    "    t1 = time.time()\n",
    "    proba = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1))\n",
    "    auc  = roc_auc_score(y_valid, proba)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={booster.best_iteration} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best.update({\"auc\": auc, \"params\": params, \"booster\": booster, \"nrounds\": booster.best_iteration})\n",
    "\n",
    "print(\"=== XGB MEJOR ===\")\n",
    "print(best[\"params\"], \"| AUC:\", round(best[\"auc\"], 4), \"| best_iter:\", best[\"nrounds\"])\n",
    "\n",
    "# MÃ©tricas en umbrales (igual que LightGBM)\n",
    "valid_proba = best[\"booster\"].predict(dvalid, iteration_range=(0, best[\"nrounds\"]+1))\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo (XGB):\", best_f1)\n",
    "\n",
    "# Objetivos de negocio (opcionales)\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "_ = report_metrics(y_valid, valid_proba, thr_prec, \"PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "thr_rec = 0.05\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "_ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall â‰¥ 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258a578",
   "metadata": {},
   "source": [
    "B) CatBoost (usa mismas features numÃ©ricas) **NO SE USA DIO ERROR**\n",
    "\n",
    "PodrÃ­amos explotar categorÃ­as nativas, pero como se hizo Target Encoding, lo mÃ¡s estable es alimentar a CatBoost con el mismo set numÃ©rico que LGBM/XGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36f0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # CatBoost: entrenamiento\n",
    "# # =========================\n",
    "# import time, numpy as np\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "#     y_hat = (y_prob >= thr).astype(int)\n",
    "#     acc = accuracy_score(y_true, y_hat)\n",
    "#     pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "#     rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "#     f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "#     auc = roc_auc_score(y_true, y_prob)\n",
    "#     cm  = confusion_matrix(y_true, y_hat)\n",
    "#     print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "#     print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "#     print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "#     return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "# train_pool = Pool(X_train_model, label=y_train)\n",
    "# valid_pool = Pool(X_valid_model, label=y_valid)\n",
    "\n",
    "# grid = [\n",
    "#     dict(learning_rate=0.05, depth=8,  l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "#     dict(learning_rate=0.05, depth=10, l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "#     dict(learning_rate=0.03, depth=10, l2_leaf_reg=8.0, bagging_temperature=1.0),\n",
    "# ]\n",
    "\n",
    "# best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "# for p in grid:\n",
    "#     params = dict(\n",
    "#         loss_function=\"Logloss\",\n",
    "#         eval_metric=\"AUC\",\n",
    "#         iterations=12000,\n",
    "#         learning_rate=p[\"learning_rate\"],\n",
    "#         depth=p[\"depth\"],\n",
    "#         l2_leaf_reg=p[\"l2_leaf_reg\"],\n",
    "#         bagging_temperature=p[\"bagging_temperature\"],\n",
    "#         random_seed=42,\n",
    "#         od_type=\"Iter\",     # early stopping\n",
    "#         od_wait=400,\n",
    "#         verbose=400,\n",
    "#         class_weights=[1.0, scale_pos_weight],  # desbalance\n",
    "#         # task_type=\"GPU\", devices=\"0\",       # si tienes GPU\n",
    "#     )\n",
    "#     print(\"Probando CAT:\", params)\n",
    "#     t0 = time.time()\n",
    "#     model_cb = CatBoostClassifier(**params)\n",
    "#     model_cb.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "#     t1 = time.time()\n",
    "\n",
    "#     proba = model_cb.predict_proba(valid_pool)[:,1]\n",
    "#     auc   = roc_auc_score(y_valid, proba)\n",
    "#     print(f\"AUC valid={auc:.4f} | best_iter={model_cb.get_best_iteration()} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "#     if auc > best[\"auc\"]:\n",
    "#         best.update({\"auc\": auc, \"model\": model_cb, \"params\": params})\n",
    "\n",
    "# print(\"=== CAT MEJOR ===\")\n",
    "# print(best[\"params\"][\"learning_rate\"], best[\"params\"][\"depth\"], \"| AUC:\", round(best[\"auc\"],4),\n",
    "#       \"| best_iter:\", best[\"model\"].get_best_iteration())\n",
    "\n",
    "# valid_proba = best[\"model\"].predict_proba(valid_pool)[:,1]\n",
    "# _ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "# for thr in np.linspace(0.05, 0.5, 46):\n",
    "#     f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "#     if f1 > best_f1[\"f1\"]:\n",
    "#         best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "# _ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1 (CatBoost)\")\n",
    "# print(\"â†’ Umbral F1 Ã³ptimo (CAT):\", best_f1)\n",
    "\n",
    "# thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "# _ = report_metrics(y_valid, valid_proba, thr_prec, \"PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "# thr_rec = 0.05\n",
    "# for thr in np.linspace(0.05, 0.4, 71):\n",
    "#     if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "#         thr_rec = float(thr); break\n",
    "# _ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall â‰¥ 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198edf48",
   "metadata": {},
   "source": [
    "Random Forest (sklearn) â€” Entrenar, evaluar y guardar **Esta OpciÃ³n Saturo la memoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f92a4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Entrenando RF (base)â€¦\n",
      "RF AUC valid=0.6026 | tiempo=40.0 min\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8141 | Precision: 0.2983 | Recall: 0.0568 | F1: 0.0954 | ROC-AUC: 0.6026\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[749712  21483]\n",
      " [151756   9133]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.220) ==\n",
      "Accuracy: 0.5059 | Precision: 0.2103 | Recall: 0.6758 | F1: 0.3207 | ROC-AUC: 0.6026\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[362788 408407]\n",
      " [ 52157 108732]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.22000000000000003, 'f1': 0.32073011734028684}\n",
      "\n",
      "Probando RF: {'n_estimators': 300, 'max_depth': 24, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "RF AUC valid=0.6072 | tiempo=23.3 min\n",
      "\n",
      "Probando RF: {'n_estimators': 500, 'max_depth': None, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 134217728 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ~~~~~~~~~^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n    return self.function(*args, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n    tree._fit(\n    ~~~~~~~~~^\n        X,\n        ^^\n    ...<3 lines>...\n        missing_values_in_feature_mask=missing_values_in_feature_mask,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"sklearn/tree/_tree.pyx\", line 141, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn/tree/_tree.pyx\", line 256, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn/tree/_tree.pyx\", line 911, in sklearn.tree._tree.Tree._add_node\n  File \"sklearn/tree/_tree.pyx\", line 879, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn/tree/_utils.pyx\", line 29, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 134217728 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m params = base_params.copy(); params.update(cfg)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProbando RF:\u001b[39m\u001b[33m\"\u001b[39m, {k: params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmin_samples_leaf\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax_features\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m mdl, proba, auc = \u001b[43mtrain_eval_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > best_rf[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     66\u001b[39m     best_rf = {\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m: auc, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: mdl, \u001b[33m\"\u001b[39m\u001b[33mproba\u001b[39m\u001b[33m\"\u001b[39m: proba, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: params}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_eval_rf\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     27\u001b[39m rf = RandomForestClassifier(**params)\n\u001b[32m     28\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m secs = time.time() - t0\n\u001b[32m     31\u001b[39m proba = rf.predict_proba(X_valid_model)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: could not allocate 134217728 bytes"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Random Forest: entrenamiento\n",
    "# ============================\n",
    "import time, os, json, joblib, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# --- 1) Manejo del desbalance con class_weight ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "class_w = {0: 1.0, 1: float(scale_pos_weight)}   # alternativa: \"balanced_subsample\"\n",
    "\n",
    "# --- 2) ParÃ¡metros base (buen compromiso calidad/tiempo/memoria) ---\n",
    "base_params = dict(\n",
    "    n_estimators=400,          # subir a 600/800 si tienes mÃ¡s tiempo\n",
    "    max_depth=None,            # None = Ã¡rboles profundos; si RAM justa, prueba 18-24\n",
    "    min_samples_leaf=2,        # ayuda a generalizar y bajar varianza\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",       # tÃ­pico en RF\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight=class_w\n",
    ")\n",
    "\n",
    "def train_eval_rf(params):\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    rf.fit(X_train_model, y_train)\n",
    "    secs = time.time() - t0\n",
    "    proba = rf.predict_proba(X_valid_model)[:, 1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(f\"RF AUC valid={auc:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return rf, proba, auc\n",
    "\n",
    "print(\"â†’ Entrenando RF (base)â€¦\")\n",
    "rf, rf_proba, rf_auc = train_eval_rf(base_params)\n",
    "\n",
    "# --- 3) Barrido pequeÃ±o de umbral para F1 (puedes reutilizar tu lÃ³gica existente) ---\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best)\n",
    "\n",
    "# (Opcional) Si tu objetivo operativo es recall alto o precisiÃ³n â‰¥ 0.30,\n",
    "# puedes reutilizar tus funciones de umbrales de negocio con rf_proba:\n",
    "# _ = report_metrics(y_valid, rf_proba, thr_prec_calculado, \"RF PrecisiÃ³n â‰¥ 0.30\")\n",
    "# _ = report_metrics(y_valid, rf_proba, thr_rec_calculado, \"RF Recall â‰¥ 0.70\")\n",
    "\n",
    "# --- 4) Mini tuning rÃ¡pido (opcional, 2-3 pruebas sin reventar la RAM/tiempo) ---\n",
    "quick_grid = [\n",
    "    dict(n_estimators=300, max_depth=24, min_samples_leaf=2, max_features=\"sqrt\"),\n",
    "    dict(n_estimators=500, max_depth=None, min_samples_leaf=2, max_features=\"sqrt\"),\n",
    "    dict(n_estimators=400, max_depth=18, min_samples_leaf=3, max_features=\"sqrt\"),\n",
    "]\n",
    "best_rf = {\"auc\": rf_auc, \"model\": rf, \"proba\": rf_proba, \"params\": base_params}\n",
    "for cfg in quick_grid:\n",
    "    params = base_params.copy(); params.update(cfg)\n",
    "    print(\"\\nProbando RF:\", {k: params[k] for k in [\"n_estimators\",\"max_depth\",\"min_samples_leaf\",\"max_features\"]})\n",
    "    mdl, proba, auc = train_eval_rf(params)\n",
    "    if auc > best_rf[\"auc\"]:\n",
    "        best_rf = {\"auc\": auc, \"model\": mdl, \"proba\": proba, \"params\": params}\n",
    "\n",
    "print(\"\\n=== RF MEJOR ===\")\n",
    "print(best_rf[\"params\"], \"| AUC:\", best_rf[\"auc\"])\n",
    "\n",
    "# Recalcular umbral F1 con el mejor modelo\n",
    "best_thr = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (best_rf[\"proba\"] >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best_thr[\"f1\"]:\n",
    "        best_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_rf[\"proba\"], 0.5, \"RF(best) Base 0.5\")\n",
    "_ = report_metrics(y_valid, best_rf[\"proba\"], best_thr[\"thr\"], \"RF(best) Mejor F1\")\n",
    "print(\"â†’ RF(best) umbral F1 Ã³ptimo:\", best_thr)\n",
    "\n",
    "# ============================\n",
    "# Guardado de artefactos RF\n",
    "# ============================\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "rf_path = os.path.join(MODELS_DIR, \"rf_retrasos.pkl\")\n",
    "joblib.dump(best_rf[\"model\"], rf_path)\n",
    "print(f\"âœ… RF guardado en: {rf_path}\")\n",
    "\n",
    "# Guardar orden de features para inferencia (reutilizamos el que ya generaste con LGBM)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    # si no existe por alguna razÃ³n, lo creamos\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(X_train_model.columns)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Metadatos especÃ­ficos del RF\n",
    "rf_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": rf_path,\n",
    "    \"model_type\": \"random_forest\",\n",
    "    \"auc_valid\": float(best_rf[\"auc\"]),\n",
    "    \"best_thr_f1\": float(best_thr[\"thr\"]),\n",
    "    \"feature_order_path\": feat_path\n",
    "}\n",
    "rf_meta_path = os.path.join(ARTIF_DIR, \"metadata_rf.json\")\n",
    "with open(rf_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rf_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Metadatos RF guardados en: {rf_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec875f67",
   "metadata": {},
   "source": [
    "(Opcional) Probabilidades mejor calibradas\n",
    "\n",
    "Los bosques tienden a dar probabilidades poco calibradas. Si lo necesitas para umbrales finos/reportes, puedes calibrar con sigmoid sobre una muestra del validation (para no morir en memoria/tiempo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CalibraciÃ³n (sigmoid) opcional\n",
    "# ============================\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Usamos el mejor RF ya entrenado (prefit) y calibramos en un subset del valid\n",
    "cal_idx = np.random.RandomState(42).choice(len(X_valid_model), size=min(200_000, len(X_valid_model)), replace=False)\n",
    "X_cal = X_valid_model.iloc[cal_idx]\n",
    "y_cal = y_valid.iloc[cal_idx]\n",
    "\n",
    "cal = CalibratedClassifierCV(best_rf[\"model\"], method=\"sigmoid\", cv=\"prefit\")\n",
    "t0 = time.time()\n",
    "cal.fit(X_cal, y_cal)\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Calibrado en {(t1-t0):.1f}s\")\n",
    "\n",
    "cal_proba = cal.predict_proba(X_valid_model)[:,1]\n",
    "cal_auc   = roc_auc_score(y_valid, cal_proba)\n",
    "print(f\"AUC valid calibrado={cal_auc:.4f}\")\n",
    "\n",
    "# Re-barrido de umbral para F1 con probabilidades calibradas\n",
    "best_cal = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (cal_proba >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best_cal[\"f1\"]:\n",
    "        best_cal = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, cal_proba, 0.5, \"RF Calibrado Base 0.5\")\n",
    "_ = report_metrics(y_valid, cal_proba, best_cal[\"thr\"], \"RF Calibrado Mejor F1\")\n",
    "print(\"â†’ RF Calibrado umbral F1 Ã³ptimo:\", best_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888c06b",
   "metadata": {},
   "source": [
    "**Sustituye Radom Forest anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b453e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Entrenando tanda 1/4: +100 Ã¡rboles (total=100)\n",
      "â†’ Entrenando tanda 2/4: +100 Ã¡rboles (total=200)\n",
      "â†’ Entrenando tanda 3/4: +100 Ã¡rboles (total=300)\n",
      "â†’ Entrenando tanda 4/4: +100 Ã¡rboles (total=400)\n",
      "âœ“ RF AUC valid=0.6103 | tiempo=47.7 min | Ã¡rboles=400\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7723 | Precision: 0.2702 | Recall: 0.1878 | F1: 0.2216 | ROC-AUC: 0.6103\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[689606  81589]\n",
      " [130679  30210]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.310) ==\n",
      "Accuracy: 0.5029 | Precision: 0.2121 | Recall: 0.6926 | F1: 0.3248 | ROC-AUC: 0.6103\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[357344 413851]\n",
      " [ 49461 111428]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.31, 'f1': 0.32478343496053447}\n",
      "âœ… RF guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\rf_retrasos.pkl\n",
      "âœ… Metadatos RF guardados en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata_rf.json\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Random Forest: entrenamiento (memoria-seguro)\n",
    "# ============================\n",
    "import os, json, time, joblib, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# --- utilidades mÃ©tricas (reusa tus helpers si ya las tienes) ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def scan_best_f1(y_true, proba, lo=0.05, hi=0.5, steps=46):\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (proba >= thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# --- 0) Downcasting agresivo para ahorrar RAM ---\n",
    "def _downcast_df(df):\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        if np.issubdtype(df2[c].dtype, np.floating):\n",
    "            df2[c] = df2[c].astype(np.float32)\n",
    "        elif np.issubdtype(df2[c].dtype, np.integer):\n",
    "            # cuentas/aggregates -> int32 es suficiente\n",
    "            if str(df2[c].dtype) not in (\"int8\", \"int16\", \"int32\"):\n",
    "                df2[c] = df2[c].astype(np.int32)\n",
    "    return df2\n",
    "\n",
    "Xtr = _downcast_df(X_train_model)\n",
    "Xva = _downcast_df(X_valid_model)\n",
    "\n",
    "# --- 1) Desbalance via class_weight (mÃ¡s estable que sample_weight a esta escala) ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "class_w = {0: 1.0, 1: float(scale_pos_weight)}\n",
    "\n",
    "# --- 2) ParÃ¡metros memoria-seguros ---\n",
    "# Claves para bajar el pico:\n",
    "# - n_jobs=4 (menos paralelismo => menos matrices en RAM al mismo tiempo)\n",
    "# - bootstrap=True + max_samples<1.0 (cada Ã¡rbol entrena con una fracciÃ³n aleatoria)\n",
    "# - max_depth limitado y hojas mÃ­nimas mayores\n",
    "base_params = dict(\n",
    "    n_estimators=0,            # usamos warm_start, empezamos en 0\n",
    "    warm_start=True,\n",
    "    max_depth=22,              # si sigue alto el pico, prueba 18â€“20\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=6,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    max_samples=0.55,          # 55% de las filas por Ã¡rbol\n",
    "    n_jobs=4,                  # reduce el pico; sube a 6â€“8 si tienes mucha RAM\n",
    "    random_state=42,\n",
    "    class_weight=class_w\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(**base_params)\n",
    "\n",
    "# --- 3) Entrenamiento por tandas (evita picos de memoria gordos) ---\n",
    "# Puedes ajustar: 4 tandas x 100 Ã¡rboles = 400 Ã¡rboles finales\n",
    "batches = [100, 100, 100, 100]\n",
    "t0 = time.time()\n",
    "for i, add_trees in enumerate(batches, start=1):\n",
    "    rf.n_estimators += add_trees\n",
    "    print(f\"â†’ Entrenando tanda {i}/{len(batches)}: +{add_trees} Ã¡rboles (total={rf.n_estimators})\")\n",
    "    rf.fit(Xtr, y_train)  # warm_start mantiene Ã¡rboles previos\n",
    "\n",
    "secs = time.time() - t0\n",
    "proba_va = rf.predict_proba(Xva)[:, 1]\n",
    "auc_va   = roc_auc_score(y_valid, proba_va)\n",
    "print(f\"âœ“ RF AUC valid={auc_va:.4f} | tiempo={secs/60:.1f} min | Ã¡rboles={rf.n_estimators}\")\n",
    "\n",
    "# --- 4) Umbral F1 y reportes ---\n",
    "best = scan_best_f1(y_valid, proba_va)\n",
    "_ = report_metrics(y_valid, proba_va, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, proba_va, best[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best)\n",
    "\n",
    "# (Opcional) Otros objetivos de negocio:\n",
    "# thr_prec = 0.229  # si lo calculaste para PrecisiÃ³nâ‰ˆ0.30\n",
    "# _ = report_metrics(y_valid, proba_va, thr_prec, \"RF PrecisiÃ³n â‰³ 0.30\")\n",
    "# thr_rec  = 0.200  # si lo calculaste para Recallâ‰¥0.70\n",
    "# _ = report_metrics(y_valid, proba_va, thr_rec, \"RF Recall â‰¥ 0.70\")\n",
    "\n",
    "# --- 5) Guardado artefactos RF ---\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "rf_path = os.path.join(MODELS_DIR, \"rf_retrasos.pkl\")\n",
    "joblib.dump(rf, rf_path)\n",
    "print(f\"âœ… RF guardado en: {rf_path}\")\n",
    "\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(Xtr.columns)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "rf_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": rf_path,\n",
    "    \"model_type\": \"random_forest\",\n",
    "    \"auc_valid\": float(auc_va),\n",
    "    \"best_thr_f1\": float(best[\"thr\"]),\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"params\": {\n",
    "        **{k: v for k, v in base_params.items() if k != \"n_estimators\"},\n",
    "        \"n_estimators\": int(rf.n_estimators)\n",
    "    }\n",
    "}\n",
    "rf_meta_path = os.path.join(ARTIF_DIR, \"metadata_rf.json\")\n",
    "with open(rf_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rf_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Metadatos RF guardados en: {rf_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734538a",
   "metadata": {},
   "source": [
    "selector de modelos (LightGBM, XGBoost, Random Forest y ExtraTrees) que:\n",
    "\n",
    "Entrena cada modelo con una config base razonable (rÃ¡pida y estable para 5.2M filas).\n",
    "\n",
    "EvalÃºa ROC-AUC y hace un barrido de umbral para encontrar el mejor F1.\n",
    "\n",
    "Muestra un cuadro comparativo (AUC, F1@best_thr, tiempo).\n",
    "\n",
    "Selecciona el mejor por AUC (desempate por F1) y guarda artefactos compatibles con tu estructura actual:\n",
    "\n",
    "models/<mejor_modelo>.pkl\n",
    "\n",
    "artifacts/feature_order.json (si no existe)\n",
    "\n",
    "artifacts/summary_models.json (todos los resultados)\n",
    "\n",
    "artifacts/best_model.json (metadatos del ganador)\n",
    "\n",
    "Requisitos previos en tu notebook: tener listas las matrices X_train_model, X_valid_model, y_train, y_valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Selector de modelos: LGBM, XGBoost, RandomForest, ExtraTrees\n",
    "# ============================================================\n",
    "import os, json, time, joblib, numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "def ensure_dirs():\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "    MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "    ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "    return BASE_DIR, MODELS_DIR, ARTIF_DIR\n",
    "\n",
    "def scan_best_f1(y_true, proba, lo=0.05, hi=0.5, steps=46):\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        y_hat = (proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_true, y_hat, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "def quick_report(y_true, proba, thr, title=\"\"):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, proba)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    name: str\n",
    "    model: Any\n",
    "    auc: float\n",
    "    f1: float\n",
    "    best_thr: float\n",
    "    secs: float\n",
    "    extra: Optional[Dict[str, Any]] = None\n",
    "\n",
    "def pick_winner(results):\n",
    "    # ganador por AUC (desempate por F1)\n",
    "    results_sorted = sorted(results, key=lambda r: (r.auc, r.f1), reverse=True)\n",
    "    return results_sorted[0], results_sorted\n",
    "\n",
    "# ---------- Entrenadores ----------\n",
    "def train_lgbm(X_tr, y_tr, X_va, y_va):\n",
    "    import lightgbm as lgb\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=12000,\n",
    "        num_leaves=127,\n",
    "        min_child_samples=100,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=5.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw\n",
    "    )\n",
    "    print(\"\\nâ†’ Entrenando LightGBMâ€¦\")\n",
    "    mdl = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"LGBM Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"LGBM Mejor F1\")\n",
    "    print(f\"âœ“ LGBM AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min | best_iter={getattr(mdl,'best_iteration_',None)}\")\n",
    "    return ModelResult(\"lgbm\", mdl, auc, best[\"f1\"], best[\"thr\"], secs, extra={\"best_iter\": getattr(mdl,\"best_iteration_\",None)})\n",
    "\n",
    "def train_xgb(X_tr, y_tr, X_va, y_va):\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸  XGBoost no disponible, salto:\", e)\n",
    "        return None\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": 0.05,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 10,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"scale_pos_weight\": spw,\n",
    "        \"nthread\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    print(\"\\nâ†’ Entrenando XGBoostâ€¦\")\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=5000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=400,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"XGB Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(f\"âœ“ XGB AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min | best_iter={mdl.best_iteration}\")\n",
    "    return ModelResult(\"xgboost\", mdl, auc, best[\"f1\"], best[\"thr\"], secs, extra={\"best_iter\": mdl.best_iteration})\n",
    "\n",
    "def train_rf(X_tr, y_tr, X_va, y_va):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    class_w = {0: 1.0, 1: float(spw)}\n",
    "    params = dict(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=class_w\n",
    "    )\n",
    "    print(\"\\nâ†’ Entrenando RandomForestâ€¦\")\n",
    "    t0 = time.time()\n",
    "    mdl = RandomForestClassifier(**params).fit(X_tr, y_tr)\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"RF Mejor F1\")\n",
    "    print(f\"âœ“ RF AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return ModelResult(\"random_forest\", mdl, auc, best[\"f1\"], best[\"thr\"], secs)\n",
    "\n",
    "def train_extra(X_tr, y_tr, X_va, y_va):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    class_w = {0: 1.0, 1: float(spw)}\n",
    "    params = dict(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=class_w\n",
    "    )\n",
    "    print(\"\\nâ†’ Entrenando ExtraTreesâ€¦\")\n",
    "    t0 = time.time()\n",
    "    mdl = ExtraTreesClassifier(**params).fit(X_tr, y_tr)\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"ExtraTrees Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"ExtraTrees Mejor F1\")\n",
    "    print(f\"âœ“ ExtraTrees AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return ModelResult(\"extra_trees\", mdl, auc, best[\"f1\"], best[\"thr\"], secs)\n",
    "\n",
    "# ---------- Ejecutar todo ----------\n",
    "BASE_DIR, MODELS_DIR, ARTIF_DIR = ensure_dirs()\n",
    "\n",
    "results = []\n",
    "# LightGBM\n",
    "try:\n",
    "    results.append(train_lgbm(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸  Error LGBM:\", e)\n",
    "\n",
    "# XGBoost (si no estÃ¡ instalado, se salta)\n",
    "xgb_res = train_xgb(X_train_model, y_train, X_valid_model, y_valid)\n",
    "if xgb_res is not None:\n",
    "    results.append(xgb_res)\n",
    "\n",
    "# RandomForest\n",
    "try:\n",
    "    results.append(train_rf(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸  Error RF:\", e)\n",
    "\n",
    "# ExtraTrees\n",
    "try:\n",
    "    results.append(train_extra(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸  Error ExtraTrees:\", e)\n",
    "\n",
    "# Filtrar None por si alguno fallÃ³\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "# SelecciÃ³n del ganador\n",
    "winner, ranked = pick_winner(results)\n",
    "\n",
    "# ---------- Mostrar comparativa ----------\n",
    "import pandas as pd\n",
    "df_cmp = pd.DataFrame([{\n",
    "    \"modelo\": r.name,\n",
    "    \"AUC_valid\": round(r.auc, 6),\n",
    "    \"F1@best\": round(r.f1, 6),\n",
    "    \"best_thr\": round(r.best_thr, 4),\n",
    "    \"mins\": round(r.secs/60, 2),\n",
    "    **({\"best_iter\": r.extra.get(\"best_iter\")} if r.extra else {})\n",
    "} for r in ranked])\n",
    "print(\"\\n=== COMPARATIVA MODELOS (ordenada) ===\")\n",
    "display(df_cmp.sort_values([\"AUC_valid\",\"F1@best\"], ascending=False).reset_index(drop=True))\n",
    "\n",
    "print(f\"\\nğŸ† GANADOR: {winner.name} | AUC={winner.auc:.4f} | F1@best={winner.f1:.4f} | thr={winner.best_thr:.3f}\")\n",
    "\n",
    "# ---------- Guardar mejor modelo + metadatos ----------\n",
    "# 1) feature_order (si no existe)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(X_train_model.columns)}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "# 2) Modelo\n",
    "best_model_path = os.path.join(MODELS_DIR, f\"{winner.name}_best.pkl\")\n",
    "joblib.dump(winner.model, best_model_path)\n",
    "print(f\"âœ… Modelo ganador guardado en: {best_model_path}\")\n",
    "\n",
    "# 3) Summary de todos los modelos\n",
    "summary_path = os.path.join(ARTIF_DIR, \"summary_models.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df_cmp.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Resumen comparativo guardado en: {summary_path}\")\n",
    "\n",
    "# 4) Metadatos del ganador\n",
    "best_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"winner\": winner.name,\n",
    "    \"model_path\": best_model_path,\n",
    "    \"AUC_valid\": float(winner.auc),\n",
    "    \"F1_best\": float(winner.f1),\n",
    "    \"best_threshold\": float(winner.best_thr),\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"extra\": winner.extra or {}\n",
    "}\n",
    "best_meta_path = os.path.join(ARTIF_DIR, \"best_model.json\")\n",
    "with open(best_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Metadatos del ganador guardados en: {best_meta_path}\")\n",
    "\n",
    "# ---------- Nota de uso en inferencia ----------\n",
    "print(\"\\nâ„¹ï¸ Para inferencia, usa el 'feature_order.json' para alinear columnas y carga el modelo ganador:\")\n",
    "print(f\"   - Modelo: {best_model_path}\")\n",
    "print(f\"   - Metadatos: {best_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7a33a",
   "metadata": {},
   "source": [
    "###RevisiÃ³n 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af27c88",
   "metadata": {},
   "source": [
    "Respeta tus columnas existentes (no recalcula si ya vienen), hace split sin fuga, aplica Target Encoding, agregados histÃ³ricos, entrena LGBM, XGB y RF, compara, y guarda solo en models/ dos archivos .joblib:\n",
    "\n",
    "models/<mejor_modelo>_retrasos.joblib â†’ el modelo entrenado\n",
    "\n",
    "models/artifacts.joblib â†’ TODO lo adicional (TE, agregados, orden de features, umbrales, best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee87c42",
   "metadata": {},
   "source": [
    "1 â€” Imports, rutas y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087a5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1) Imports, rutas y utilidades\n",
    "# ================================\n",
    "import os, time, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MÃ©tricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Rutas del proyecto ----------\n",
    "PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cambia aquÃ­ si tu CSV estÃ¡ en otra ruta:\n",
    "CSV_PATH = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "# ---------- ConfiguraciÃ³n general ----------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Columnas esperadas del CSV (segÃºn tu lista)\n",
    "COLUMNS_EXPECTED = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"DEPARTURE_TIME\",\"DEPARTURE_DELAY\",\n",
    "    \"SCHEDULED_TIME\",\"DISTANCE\",\"SCHEDULED_ARRIVAL\",\"ARRIVAL_TIME\",\"ARRIVAL_DELAY\",\n",
    "    \"AIRLINE_NAME\",\"ORIGEN_AEROPUERTO\",\"ORIGEN_CIUDAD\",\"ORIGEN_ESTADO\",\"ORIGEN_LAT\",\"ORIGEN_LON\",\n",
    "    \"DEST_AEROPUERTO\",\"DEST_CIUDAD\",\"DEST_ESTADO\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"MOTIVO_RETRASO\",\"CANTIDAD_CAUSAS\",\"RETRASADO_LLEGADA\",\"RETRASADO_SALIDA\",\n",
    "    \"HORA_SALIDA\",\"HORA_LLEGADA\",\"MIN_SALIDA\",\"MIN_LLEGADA\",\"MINUTO_DIA_SALIDA\",\"MINUTO_DIA_LLEGADA\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\"LLEGADA_SIN\",\"LLEGADA_COS\",\"PERIODO_SALIDA\",\"PERIODO_LLEGADA\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eba5cd",
   "metadata": {},
   "source": [
    "2 â€” Carga del CSV + dtypes seguros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea396dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Cargado: (5231130, 41) | en 32.4s\n",
      "âœ“ Columnas clave presentes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2) Cargar flights_clean.csv y optimizar tipos numÃ©ricos\n",
    "#     - NO recalcula columnas si ya existen en el CSV\n",
    "# =====================================================\n",
    "def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(path_csv)\n",
    "    # Downcast numÃ©ricos cuando sea seguro\n",
    "    for c in df.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    print(f\"âœ“ Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "    return df\n",
    "\n",
    "df = load_flights(CSV_PATH)\n",
    "\n",
    "# ValidaciÃ³n soft de columnas clave\n",
    "faltantes = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "                         \"MINUTO_DIA_SALIDA\",\"SALIDA_SIN\",\"SALIDA_COS\",\"RETRASADO_LLEGADA\"] if c not in df.columns]\n",
    "if faltantes:\n",
    "    print(\"âš ï¸ Faltan columnas clave (se derivarÃ¡n si es posible):\", faltantes)\n",
    "else:\n",
    "    print(\"âœ“ Columnas clave presentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d3a80",
   "metadata": {},
   "source": [
    "3 â€” Ensamblado de features base (respetando las ya calculadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc6180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231130, 45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 3) Features base: RUTA, seÃ±ales trigonomÃ©tricas y\n",
    "#    distancia Haversine (si faltara DISTANCE_HAV)\n",
    "# ==================================================\n",
    "def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"RUTA\" not in out.columns:\n",
    "        out[\"RUTA\"] = (out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    return out\n",
    "\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Si ya existen, no recalcular\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        # HHMM -> minutos del dÃ­a\n",
    "        hh = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23)\n",
    "        mm = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59)\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (hh * 60 + mm).astype(\"int16\")\n",
    "\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "\n",
    "    # SeÃ±ales mensuales (Ãºtiles y muy baratas)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Distancia Haversine (si la quieres y no la tienes en tu CSV)\n",
    "def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"DISTANCIA_HAV\" in out.columns:\n",
    "        return out\n",
    "    if not {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns):\n",
    "        return out\n",
    "\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        p1 = np.radians(lat1); p2 = np.radians(lat2)\n",
    "        dlat = p2 - p1\n",
    "        dlon = np.radians(lon2) - np.radians(lon1)\n",
    "        a = np.sin(dlat/2)**2 + np.cos(p1)*np.cos(p2)*np.sin(dlon/2)**2\n",
    "        return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "    out[\"DISTANCIA_HAV\"] = haversine(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "df = add_route(df)\n",
    "df = ensure_time_features(df)\n",
    "df = ensure_distance(df)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522de6ef",
   "metadata": {},
   "source": [
    "4 â€” Split sin fuga + matrices base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef22fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 45) (932084, 45) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Split temporal sin fuga (train meses 1-9, valid 10-12)\n",
    "#    y definiciÃ³n de matrices base para modelado\n",
    "# ============================================================\n",
    "target_col = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# MÃ¡scaras de tiempo (2015)\n",
    "train_mask = (df[\"MONTH\"] >= 1) & (df[\"MONTH\"] <= 9)\n",
    "valid_mask = (df[\"MONTH\"] >= 10) & (df[\"MONTH\"] <= 12)\n",
    "\n",
    "v_train = df.loc[train_mask].copy()\n",
    "v_valid = df.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Shapes ->\", v_train.shape, v_valid.shape,\n",
    "      \"| rate train\", v_train[target_col].mean(),\n",
    "      \"| rate valid\", v_valid[target_col].mean())\n",
    "\n",
    "# Conjunto mÃ­nimo de features base (numÃ©ricas + derivadas)\n",
    "BASE_FEATS = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# Columnas categÃ³ricas para Target Encoding\n",
    "TE_COLS = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "\n",
    "# Construimos X/y de partida (solo numÃ©ricas; TE/Agregados luego)\n",
    "X_train = v_train[BASE_FEATS].copy()\n",
    "X_valid = v_valid[BASE_FEATS].copy()\n",
    "y_train = v_train[target_col].astype(\"int8\")\n",
    "y_valid = v_valid[target_col].astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71f7b2",
   "metadata": {},
   "source": [
    "5 â€” Target Encoding (sin fuga) y agregados histÃ³ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172cb1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 27) (932084, 27)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5) Target Encoding y Agregados (sin fuga)\n",
    "#     - TE: medias suavizadas por categorÃ­a\n",
    "#     - Agregs: tasas y conteos por llaves (AIR, ORI, DES, RUTA, RUTA_HORA)\n",
    "# =====================================================\n",
    "def target_encode_fit(df: pd.DataFrame, cols, target=target_col, smooth=20):\n",
    "    \"\"\"\n",
    "    Calcula TE por columna categÃ³rica usando SOLO TRAIN.\n",
    "    Devuelve:\n",
    "      - mappings: {col: {cat: valor_te}}\n",
    "      - defaults: {col: global_mean}\n",
    "    \"\"\"\n",
    "    mappings, defaults = {}, {}\n",
    "    global_mean = df[target].mean()\n",
    "    for c in cols:\n",
    "        g = df.groupby(c)[target]\n",
    "        stats = g.mean()\n",
    "        cnts  = g.size()\n",
    "        te = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "        mappings[c] = te.to_dict()\n",
    "        defaults[c] = float(global_mean)\n",
    "    return mappings, defaults, float(global_mean)\n",
    "\n",
    "def target_encode_apply(df: pd.DataFrame, mappings, defaults):\n",
    "    out = df.copy()\n",
    "    for c, mp in mappings.items():\n",
    "        col = c + \"_TE\"\n",
    "        out[col] = out[c].astype(str).map(mp).astype(\"float32\")\n",
    "        out[col].fillna(defaults[c], inplace=True)\n",
    "    return out\n",
    "\n",
    "def build_agg(df: pd.DataFrame, keys, target=target_col, pref=\"AIR\", smooth=20):\n",
    "    \"\"\"\n",
    "    Construye agregados suavizados por clave:\n",
    "      - <pref>_rate : tasa suavizada del target\n",
    "      - <pref>_n    : conteo\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "    global_mean = df[target].mean()\n",
    "    rate = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "    out = rate.rename(f\"{pref}_rate\").reset_index()\n",
    "    out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "    return out.astype({f\"{pref}_rate\":\"float32\", f\"{pref}_n\":\"float32\"})\n",
    "\n",
    "def apply_aggs(df: pd.DataFrame, agg_tables: dict):\n",
    "    out = df.copy()\n",
    "    for pref, (keys, table) in agg_tables.items():\n",
    "        out = out.merge(table, on=keys, how=\"left\")\n",
    "        out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(0).astype(\"float32\")\n",
    "        out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# --- Fit TE en TRAIN y aplicar a TRAIN/VALID ---\n",
    "mappings, defaults, global_mean = target_encode_fit(v_train, TE_COLS, target_col)\n",
    "X_train_te = target_encode_apply(v_train[TE_COLS + BASE_FEATS], mappings, defaults)\n",
    "X_valid_te = target_encode_apply(v_valid[TE_COLS + BASE_FEATS], mappings, defaults)\n",
    "\n",
    "# --- Construir agregados SOLO con TRAIN y aplicar a TRAIN/VALID ---\n",
    "aggs_specs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\")\n",
    "]\n",
    "agg_tables = {}\n",
    "for keys, pref in aggs_specs:\n",
    "    agg_tables[pref] = (keys, build_agg(v_train, keys, target_col, pref, smooth=20))\n",
    "\n",
    "X_train_full = apply_aggs(X_train_te, agg_tables)\n",
    "X_valid_full = apply_aggs(X_valid_te, agg_tables)\n",
    "\n",
    "print(\"Shapes ->\", X_train_full.shape, X_valid_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676ad4d",
   "metadata": {},
   "source": [
    "6 â€” Orden de features y helpers de mÃ©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d01af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6) Alinear columnas y helpers de mÃ©tricas\n",
    "# =========================================\n",
    "FEATURE_ORDER = list(X_train_full.columns)  # orden congelado para inferencia\n",
    "\n",
    "def align_features(df: pd.DataFrame, feature_order):\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    return X[feature_order]\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb2e87",
   "metadata": {},
   "source": [
    "7 â€” Entrenar LGBM, XGB y RF (parÃ¡metros seguros RAM/tiempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices numÃ©ricas | X_train: (4299046, 23) | X_valid: (932084, 23)\n",
      "Dtypes ejemplo: {'MONTH': dtype('int8'), 'DAY_OF_WEEK': dtype('int8'), 'SALIDA_SIN': dtype('float32'), 'SALIDA_COS': dtype('float32'), 'MONTH_SIN': dtype('float32'), 'MONTH_COS': dtype('float32'), 'DISTANCIA_HAV': dtype('float32'), 'MINUTO_DIA_SALIDA': dtype('int32')}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.639319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.602295\tvalid_0's binary_logloss: 0.583084\n",
      "[400]\tvalid_0's auc: 0.602473\tvalid_0's binary_logloss: 0.580826\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602496\tvalid_0's binary_logloss: 0.460413\n",
      "âœ“ LGBM entrenado en 225.7s | best_iter=1 | AUC valid=0.6025\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4275 | Precision: 0.2009 | Recall: 0.7782 | F1: 0.3194 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[273276 497919]\n",
      " [ 35688 125201]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.2, 'f1': 0.3193866396941872}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6051 | best_iter=17 | tiempo=5.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6048 | best_iter=10 | tiempo=4.3 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6004 | best_iter=143 | tiempo=6.2 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7214 | Precision: 0.2462 | Recall: 0.2977 | F1: 0.2695 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[624549 146646]\n",
      " [112995  47894]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.420) ==\n",
      "Accuracy: 0.4981 | Precision: 0.2106 | Recall: 0.6939 | F1: 0.3231 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[352603 418592]\n",
      " [ 49248 111641]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.42, 'f1': 0.32307175867647103}\n",
      "âœ“ RF entrenado en 18.9 min | AUC valid=0.6097\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7220 | Precision: 0.2525 | Recall: 0.3114 | F1: 0.2789 | ROC-AUC: 0.6097\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[622895 148300]\n",
      " [110783  50106]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.370) ==\n",
      "Accuracy: 0.5181 | Precision: 0.2136 | Recall: 0.6681 | F1: 0.3237 | ROC-AUC: 0.6097\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[375465 395730]\n",
      " [ 53397 107492]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.37, 'f1': 0.32371696900066405}\n",
      "\n",
      "=== MEJOR MODELO: RF | AUC=0.6097 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 â€” Entrenar LGBM, XGB y RF (NUMÃ‰RICO)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers mÃ©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos numÃ©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a int32\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")  # primero float para evitar NA issues\n",
    "            # si quisieras enteros estrictos: X[c] = X[c].fillna(0).astype(\"int32\")\n",
    "\n",
    "    # convertir objetos/categorÃ­as si quedara alguno (no deberÃ­a si TE ya se aplicÃ³)\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"âš ï¸  Columnas no numÃ©ricas detectadas, se codificarÃ¡n como categoryâ†’codes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar las matrices YA TRANSFORMADAS (TE + agregados) ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid, FEATURE_ORDER\n",
    "# Asegura que FEATURE_ORDER sea el de X_train_model:\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "X_train_num  = ensure_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_num  = ensure_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "print(\"âœ“ Matrices numÃ©ricas | X_train:\", X_train_num.shape, \"| X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", X_train_num.dtypes.head(8).to_dict())\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (clasificador)\n",
    "# =========================\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=12000,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "t0 = time.time()\n",
    "lgbm.fit(\n",
    "    X_train_num, y_train,\n",
    "    eval_set=[(X_valid_num, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    ")\n",
    "lgb_secs = time.time() - t0\n",
    "lgb_best_iter = getattr(lgbm, \"best_iteration_\", None)\n",
    "lgb_proba = lgbm.predict_proba(X_valid_num)[:,1]\n",
    "lgb_auc   = roc_auc_score(y_valid, lgb_proba)\n",
    "print(f\"âœ“ LGBM entrenado en {lgb_secs:.1f}s | best_iter={lgb_best_iter} | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "# mejor F1\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b132c9e",
   "metadata": {},
   "source": [
    "para nuevo entrenamiento LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c6e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sanity OK | X_train_model: (4299046, 27) | X_valid_model: (932084, 27)\n",
      "Dtypes (primeras): {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# Celda 6.9 â€” Sanity\n",
    "# ================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _to_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    # bool -> int8\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "    # pandas nullable ints -> float32 (evita NA issues)\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "    # object/category -> codes\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    for c in non_numeric:\n",
    "        X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "    # float homogenizado\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    # int homogenizado\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "# 1) Comprobar que las matrices base existen\n",
    "vars_needed = [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]\n",
    "missing = [v for v in vars_needed if v not in globals()]\n",
    "if missing:\n",
    "    # Si faltan, intenta reconstruir desde X_train_full / X_valid_full (numÃ©ricas ya alineadas)\n",
    "    alt_needed = [\"X_train_full\",\"X_valid_full\",\"y_train\",\"y_valid\"]\n",
    "    alt_missing = [v for v in alt_needed if v not in globals()]\n",
    "    if alt_missing:\n",
    "        raise RuntimeError(\n",
    "            f\"Faltan variables previas del pipeline: {missing} \"\n",
    "            f\"y tampoco estÃ¡n disponibles alternativas {alt_missing}. \"\n",
    "            f\"Ejecuta las celdas de TE + agregados + split antes de la Celda 7.\"\n",
    "        )\n",
    "    # Reconstruir â€œ_modelâ€ a partir de las numÃ©ricas completas\n",
    "    X_train_model = _to_numeric_frame(X_train_full)\n",
    "    X_valid_model = _to_numeric_frame(X_valid_full)\n",
    "\n",
    "# 2) Asegurar orden de columnas consistente\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "# 3) Forzar numÃ©rico para esta celda (igual que harÃ¡ la celda 7)\n",
    "X_train_model = _to_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_model = _to_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "# 4) Validaciones rÃ¡pidas\n",
    "assert set(X_train_model.columns) == set(X_valid_model.columns), \"DesalineaciÃ³n de columnas entre train y valid\"\n",
    "assert len(X_train_model) == len(y_train), \"X_train_model y y_train con longitudes distintas\"\n",
    "assert len(X_valid_model) == len(y_valid), \"X_valid_model y y_valid con longitudes distintas\"\n",
    "\n",
    "print(\"âœ“ Sanity OK | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Dtypes (primeras):\", {c: str(X_train_model.dtypes[c]) for c in X_train_model.columns[:8]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1ca42",
   "metadata": {},
   "source": [
    "Similar a la celda anterior pero con cambios, Celda 7 â€” Entrenar LGBM, XGB y RF (parÃ¡metros seguros RAM/tiempo) lista para copiar y pegar. Asume que ya ejecutaste las celdas previas que generan X_train_full, X_valid_full, y_train, y_valid, FEATURE_ORDER, align_features(...) y (opcional) report_metrics(...). La celda incluye salvaguardas y comentarios en cada bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f125fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices numÃ©ricas | X_train: (4299046, 27) | X_valid: (932084, 27)\n",
      "Dtypes ejemplo: {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592065\tvalid_0's binary_logloss: 0.59038\n",
      "[400]\tvalid_0's auc: 0.591969\tvalid_0's binary_logloss: 0.597104\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.594861\tvalid_0's binary_logloss: 0.46068\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5949 | best_iter=2 | tiempo=1.9 min | params={lr:0.05, leaves:63, ff:0.8, bf:0.8}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592298\tvalid_0's binary_logloss: 0.585581\n",
      "[400]\tvalid_0's auc: 0.592428\tvalid_0's binary_logloss: 0.591143\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.59538\tvalid_0's binary_logloss: 0.460127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5954 | best_iter=2 | tiempo=2.8 min | params={lr:0.03, leaves:127, ff:0.8, bf:0.8}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.593665\tvalid_0's binary_logloss: 0.590183\n",
      "[400]\tvalid_0's auc: 0.593732\tvalid_0's binary_logloss: 0.603358\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.595565\tvalid_0's binary_logloss: 0.46019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5956 | best_iter=2 | tiempo=3.0 min | params={lr:0.03, leaves:95, ff:0.7, bf:0.9}\n",
      "âœ“ LGBM (mejor de 3) | AUC valid=0.5956\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5956\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2871 | Precision: 0.1860 | Recall: 0.9273 | F1: 0.3099 | ROC-AUC: 0.5956\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[118376 652819]\n",
      " [ 11692 149197]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19, 'f1': 0.30988934526251294}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6062 | best_iter=2 | tiempo=4.5 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6049 | best_iter=2 | tiempo=4.1 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6014 | best_iter=4 | tiempo=4.7 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7045 | Precision: 0.2430 | Recall: 0.3365 | F1: 0.2822 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[602549 168646]\n",
      " [106742  54147]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4385 | Precision: 0.2030 | Recall: 0.7698 | F1: 0.3213 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284898 486297]\n",
      " [ 37031 123858]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.48, 'f1': 0.3212734941196611}\n",
      "âœ“ RF entrenado en 19.6 min | AUC valid=0.6024\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7208 | Precision: 0.2515 | Recall: 0.3123 | F1: 0.2786 | ROC-AUC: 0.6024\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[621633 149562]\n",
      " [110642  50247]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.380) ==\n",
      "Accuracy: 0.5442 | Precision: 0.2164 | Recall: 0.6258 | F1: 0.3216 | ROC-AUC: 0.6024\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[406560 364635]\n",
      " [ 60211 100678]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.38, 'f1': 0.32155119274611066}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6062 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 â€” Entrenar LGBM, XGB y RF (NUMÃ‰RICO)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers mÃ©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime mÃ©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos numÃ©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a float32 para evitar NA issues\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # convertir objetos/categorÃ­as si quedara alguno\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"âš ï¸  Columnas no numÃ©ricas detectadas, se codificarÃ¡n como categoryâ†’codes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar las matrices YA TRANSFORMADAS (TE + agregados) ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid\n",
    "# Asegura que FEATURE_ORDER sea el de X_train_model:\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "X_train_num  = ensure_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_num  = ensure_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "print(\"âœ“ Matrices numÃ©ricas | X_train:\", X_train_num.shape, \"| X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", {c: str(X_train_num.dtypes[c]) for c in list(X_train_num.columns[:8])})\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (clasificador) â€” mini-tuning rÃ¡pido (3 configs)\n",
    "# =========================\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,\n",
    "    min_child_samples=100,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    # tips de eficiencia\n",
    "    max_bin=128,\n",
    "    force_col_wise=True,  # reduce overhead con muchas columnas\n",
    ")\n",
    "\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.05, num_leaves=63,  feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "    dict(learning_rate=0.03, num_leaves=127, feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "    dict(learning_rate=0.03, num_leaves=95,  feature_fraction=0.70, bagging_fraction=0.90, bagging_freq=1),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_train_num, y_train,\n",
    "        eval_set=[(X_valid_num, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(X_valid_num)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1 (LGBM)\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4dcb8",
   "metadata": {},
   "source": [
    "versiÃ³n reemplazo de la Celda 7 que aplica exactamente tus sugerencias para cerrar con LightGBM: mÃ¡s capacidad (num_leaves â†‘, lr â†“), max_bin=255, min_child_samples mÃ¡s bajo y features de interacciÃ³n baratas (relaciÃ³n distancia/tiempo, abs(DEPARTURE_DELAY) y buckets de 30â€™).\n",
    "La celda es autosuficiente: si alguna columna no existe, la crea; si ya existe, la respeta. XGB y RF se mantienen para comparar, pero si quieres solo LGBM, te indico quÃ© lÃ­neas comentar al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015ca4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices numÃ©ricas | X_train: (4299046, 28)  | X_valid: (932084, 28)\n",
      "Dtypes ejemplo: {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592299\tvalid_0's binary_logloss: 0.583984\n",
      "[400]\tvalid_0's auc: 0.592228\tvalid_0's binary_logloss: 0.595969\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.591258\tvalid_0's binary_logloss: 0.459871\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5913 | best_iter=4 | tiempo=3.2 min | params={lr:0.02, leaves:255, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.594793\tvalid_0's binary_logloss: 0.591906\n",
      "[400]\tvalid_0's auc: 0.594567\tvalid_0's binary_logloss: 0.605517\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.5917\tvalid_0's binary_logloss: 0.459739\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5917 | best_iter=4 | tiempo=4.5 min | params={lr:0.02, leaves:191, mcs:60, ff:0.8, bf:0.9}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.594958\tvalid_0's binary_logloss: 0.59879\n",
      "[400]\tvalid_0's auc: 0.595052\tvalid_0's binary_logloss: 0.606772\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.593671\tvalid_0's binary_logloss: 0.45971\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5937 | best_iter=3 | tiempo=3.4 min | params={lr:0.025, leaves:223, mcs:80, ff:0.8, bf:0.8}\n",
      "âœ“ LGBM (mejor de 3) | AUC valid=0.5937\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5937\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5012 | Precision: 0.2075 | Recall: 0.6701 | F1: 0.3169 | ROC-AUC: 0.5937\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[359365 411830]\n",
      " [ 53073 107816]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19999999999999996, 'f1': 0.3168565907704967}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.5960 | best_iter=8 | tiempo=6.1 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.5947 | best_iter=8 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.5921 | best_iter=1 | tiempo=5.5 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7183 | Precision: 0.2435 | Recall: 0.3000 | F1: 0.2688 | ROC-AUC: 0.5960\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[621275 149920]\n",
      " [112621  48268]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.440) ==\n",
      "Accuracy: 0.4837 | Precision: 0.2063 | Recall: 0.6994 | F1: 0.3186 | ROC-AUC: 0.5960\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[338320 432875]\n",
      " [ 48367 112522]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.44, 'f1': 0.3186301300039927}\n",
      "âœ“ RF entrenado en 19.4 min | AUC valid=0.6019\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7197 | Precision: 0.2505 | Recall: 0.3131 | F1: 0.2783 | ROC-AUC: 0.6019\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[620448 150747]\n",
      " [110519  50370]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.380) ==\n",
      "Accuracy: 0.5425 | Precision: 0.2160 | Recall: 0.6275 | F1: 0.3213 | ROC-AUC: 0.6019\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404669 366526]\n",
      " [ 59926 100963]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.37999999999999995, 'f1': 0.3213447956484791}\n",
      "\n",
      "=== MEJOR MODELO: RF | AUC=0.6019 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 â€” Entrenar LGBM, XGB y RF (NUMÃ‰RICO, con interacciones)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers mÃ©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime mÃ©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos numÃ©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a float32 para evitar NA issues\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # convertir objetos/categorÃ­as si quedara alguno\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"âš ï¸  Columnas no numÃ©ricas detectadas, se codificarÃ¡n como categoryâ†’codes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar matrices YA transformadas (TE + agregados) y aÃ±adir interacciones baratas ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "def add_interactions(X: np.ndarray.__class__, base_cols: list[str]) -> tuple:\n",
    "    \"\"\"\n",
    "    AÃ±ade, si es posible y no existen aÃºn:\n",
    "      - DIST_T_P: DISTANCIA_HAV / (SCHEDULED_TIME + 1e-3)\n",
    "      - ABS_DEP_DELAY: abs(DEPARTURE_DELAY)  (si estÃ¡ disponible)\n",
    "      - BUCKET_30_SALIDA: floor(MINUTO_DIA_SALIDA/30)*30\n",
    "      - BUCKET_30_LLEGADA: floor(MINUTO_DIA_LLEGADA/30)*30\n",
    "    Retorna (X_modificado, nuevas_columnas_agregadas_en_orden).\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    new_cols = []\n",
    "\n",
    "    def maybe_add(col_name, series, dtype=None):\n",
    "        nonlocal X, new_cols\n",
    "        if col_name not in X.columns:\n",
    "            if dtype is not None:\n",
    "                X[col_name] = series.astype(dtype)\n",
    "            else:\n",
    "                X[col_name] = series\n",
    "            new_cols.append(col_name)\n",
    "\n",
    "    # 1) DIST_T_P = DISTANCIA_HAV / SCHEDULED_TIME\n",
    "    if \"DISTANCIA_HAV\" in X.columns and \"SCHEDULED_TIME\" in X.columns:\n",
    "        eps = 1e-3\n",
    "        maybe_add(\"DIST_T_P\", (X[\"DISTANCIA_HAV\"].astype(\"float32\") / (X[\"SCHEDULED_TIME\"].astype(\"float32\") + eps)), \"float32\")\n",
    "\n",
    "    # 2) ABS_DEP_DELAY = abs(DEPARTURE_DELAY)\n",
    "    if \"DEPARTURE_DELAY\" in X.columns:\n",
    "        maybe_add(\"ABS_DEP_DELAY\", X[\"DEPARTURE_DELAY\"].astype(\"float32\").abs(), \"float32\")\n",
    "\n",
    "    # 3) Buckets de 30' (enteros)\n",
    "    if \"MINUTO_DIA_SALIDA\" in X.columns:\n",
    "        maybe_add(\"BUCKET_30_SALIDA\", ((X[\"MINUTO_DIA_SALIDA\"].astype(\"int32\") // 30) * 30), \"int32\")\n",
    "    if \"MINUTO_DIA_LLEGADA\" in X.columns:\n",
    "        maybe_add(\"BUCKET_30_LLEGADA\", ((X[\"MINUTO_DIA_LLEGADA\"].astype(\"int32\") // 30) * 30), \"int32\")\n",
    "\n",
    "    # Casting final homogÃ©neo\n",
    "    X = ensure_numeric_frame(X)\n",
    "    return X, new_cols\n",
    "\n",
    "# Aplicar interacciones a train y valid (simÃ©trico)\n",
    "X_train_ext, added_cols_train = add_interactions(X_train_model, FEATURE_ORDER)\n",
    "X_valid_ext, _               = add_interactions(X_valid_model, FEATURE_ORDER)\n",
    "\n",
    "# Actualizar FEATURE_ORDER preservando orden previo + nuevas columnas en cola\n",
    "FEATURE_ORDER = list(X_train_model.columns) + [c for c in added_cols_train if c not in X_train_model.columns]\n",
    "\n",
    "# Ensamblar matrices numÃ©ricas finales en el mismo orden de features\n",
    "X_train_num = ensure_numeric_frame(X_train_ext[FEATURE_ORDER])\n",
    "X_valid_num = ensure_numeric_frame(X_valid_ext[FEATURE_ORDER])\n",
    "\n",
    "print(\"âœ“ Matrices numÃ©ricas | X_train:\", X_train_num.shape, \" | X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", {c: str(X_train_num.dtypes[c]) for c in list(X_train_num.columns[:8])})\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM â€” enfoque principal (mÃ¡s capacidad + bins finos + min_child_samples bajo)\n",
    "# =========================\n",
    "# Sugerencias aplicadas:\n",
    "#   - num_leaves â†‘ (hasta 255)\n",
    "#   - learning_rate â†“ (0.02)\n",
    "#   - max_bin = 255 (mÃ¡s granularidad en histogramas)\n",
    "#   - min_child_samples = 60â€“80\n",
    "#   - force_col_wise para mejor rendimiento con muchas columnas\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,         # con early_stopping, se corta antes\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# probamos 3 configuraciones centradas en la consigna\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.02, num_leaves=255, min_child_samples=70, feature_fraction=0.85, bagging_fraction=0.85, bagging_freq=1),\n",
    "    dict(learning_rate=0.02, num_leaves=191, min_child_samples=60, feature_fraction=0.80, bagging_fraction=0.90, bagging_freq=1),\n",
    "    dict(learning_rate=0.025, num_leaves=223, min_child_samples=80, feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_train_num, y_train,\n",
    "        eval_set=[(X_valid_num, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(X_valid_num)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p['min_child_samples']}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1 (LGBM)\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador) â€” (se mantiene para comparar)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro) â€” (se mantiene para comparar)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor (para reporte)\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f891",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dc0f8",
   "metadata": {},
   "source": [
    "1 â€” Imports, rutas y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a78806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1) Imports, rutas y utilidades\n",
    "# ================================\n",
    "import os, time, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier  # (no lo usamos directo, pero mantenemos compatibilidad)\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MÃ©tricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Rutas del proyecto ----------\n",
    "PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
    "ARTIF_DIR    = PROJECT_ROOT / \"artifacts\"\n",
    "for d in (MODELS_DIR, ARTIF_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cambia aquÃ­ si tu CSV estÃ¡ en otra ruta:\n",
    "CSV_PATH = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "# ---------- ConfiguraciÃ³n general ----------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Columnas esperadas del CSV (segÃºn tu lista)\n",
    "COLUMNS_EXPECTED = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"DEPARTURE_TIME\",\"DEPARTURE_DELAY\",\n",
    "    \"SCHEDULED_TIME\",\"DISTANCE\",\"SCHEDULED_ARRIVAL\",\"ARRIVAL_TIME\",\"ARRIVAL_DELAY\",\n",
    "    \"AIRLINE_NAME\",\"ORIGEN_AEROPUERTO\",\"ORIGEN_CIUDAD\",\"ORIGEN_ESTADO\",\"ORIGEN_LAT\",\"ORIGEN_LON\",\n",
    "    \"DEST_AEROPUERTO\",\"DEST_CIUDAD\",\"DEST_ESTADO\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"MOTIVO_RETRASO\",\"CANTIDAD_CAUSAS\",\"RETRASADO_LLEGADA\",\"RETRASADO_SALIDA\",\n",
    "    \"HORA_SALIDA\",\"HORA_LLEGADA\",\"MIN_SALIDA\",\"MIN_LLEGADA\",\"MINUTO_DIA_SALIDA\",\"MINUTO_DIA_LLEGADA\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\"LLEGADA_SIN\",\"LLEGADA_COS\",\"PERIODO_SALIDA\",\"PERIODO_LLEGADA\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987d351",
   "metadata": {},
   "source": [
    "2 â€” Cargar CSV y optimizar tipos (no recalcula si ya existen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2311a2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Cargado: (5231130, 41) | en 40.2s\n",
      "âœ“ Columnas clave presentes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2) Cargar flights_clean.csv y optimizar tipos numÃ©ricos\n",
    "#     - NO recalcula columnas si ya existen en el CSV\n",
    "# =====================================================\n",
    "def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(path_csv)\n",
    "    # Downcast numÃ©ricos cuando sea seguro\n",
    "    for c in df.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    print(f\"âœ“ Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "    return df\n",
    "\n",
    "df = load_flights(CSV_PATH)\n",
    "\n",
    "# ValidaciÃ³n soft de columnas clave\n",
    "faltantes = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "                         \"MINUTO_DIA_SALIDA\",\"SALIDA_SIN\",\"SALIDA_COS\",\"RETRASADO_LLEGADA\"] if c not in df.columns]\n",
    "if faltantes:\n",
    "    print(\"âš ï¸ Faltan columnas clave (se derivarÃ¡n si es posible):\", faltantes)\n",
    "else:\n",
    "    print(\"âœ“ Columnas clave presentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fedcff",
   "metadata": {},
   "source": [
    "3 â€” Features base (RUTA, trigonomÃ©tricas, Haversine) solo si faltan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3a368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231130, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 3) Features base: RUTA, seÃ±ales trigonomÃ©tricas y\n",
    "#    distancia Haversine (si faltara DISTANCE_HAV)\n",
    "# ==================================================\n",
    "def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"RUTA\" not in out.columns:\n",
    "        out[\"RUTA\"] = (out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    return out\n",
    "\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Si ya existen, no recalcular\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        hh = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23)\n",
    "        mm = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59)\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (hh * 60 + mm).astype(\"int16\")\n",
    "\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "\n",
    "    # SeÃ±ales mensuales (baratas)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Distancia Haversine (si la quieres y no la tienes en tu CSV)\n",
    "def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"DISTANCIA_HAV\" in out.columns:\n",
    "        return out\n",
    "    if not {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns):\n",
    "        return out\n",
    "\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        p1 = np.radians(lat1); p2 = np.radians(lat2)\n",
    "        dlat = p2 - p1\n",
    "        dlon = np.radians(lon2) - np.radians(lon1)\n",
    "        a = np.sin(dlat/2)**2 + np.cos(p1)*np.cos(p2)*np.sin(dlon/2)**2\n",
    "        return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "    out[\"DISTANCIA_HAV\"] = haversine(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "df = add_route(df)\n",
    "df = ensure_time_features(df)\n",
    "df = ensure_distance(df)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a19329",
   "metadata": {},
   "source": [
    "4 â€” Split temporal + matrices base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e3b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 45) (932084, 45) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n",
      "BASE_FEATS: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA']\n",
      "TE_COLS:    ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Split temporal sin fuga (train 1-9, valid 10-12)\n",
    "#    y definiciÃ³n de matrices base para modelado\n",
    "# ============================================================\n",
    "target_col = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# MÃ¡scaras de tiempo (2015)\n",
    "train_mask = (df[\"MONTH\"] >= 1) & (df[\"MONTH\"] <= 9)\n",
    "valid_mask = (df[\"MONTH\"] >= 10) & (df[\"MONTH\"] <= 12)\n",
    "\n",
    "v_train = df.loc[train_mask].copy()\n",
    "v_valid = df.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Shapes ->\", v_train.shape, v_valid.shape,\n",
    "      \"| rate train\", v_train[target_col].mean(),\n",
    "      \"| rate valid\", v_valid[target_col].mean())\n",
    "\n",
    "# Conjunto mÃ­nimo de features base (numÃ©ricas + derivadas)\n",
    "BASE_FEATS = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\" if \"DISTANCIA_HAV\" in df.columns else \"DISTANCE\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# Columnas categÃ³ricas para Target Encoding\n",
    "TE_COLS = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in df.columns]\n",
    "\n",
    "# Construimos X/y de partida\n",
    "X_train = v_train[BASE_FEATS].copy()\n",
    "X_valid = v_valid[BASE_FEATS].copy()\n",
    "y_train = v_train[target_col].astype(\"int8\")\n",
    "y_valid = v_valid[target_col].astype(\"int8\")\n",
    "\n",
    "print(\"BASE_FEATS:\", BASE_FEATS)\n",
    "print(\"TE_COLS:   \", TE_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a7146",
   "metadata": {},
   "source": [
    "5 â€” Target Encoding + agregados (sin fuga) + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af61a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TE guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\onehot_encoder.joblib\n",
      "Shapes -> (4299046, 27) (932084, 27)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5) Target Encoding y Agregados (sin fuga)\n",
    "#     - TE: medias suavizadas por categorÃ­a\n",
    "#     - Agregs: tasas y conteos por llaves (AIR, DES, ORI, RUTA, RUTA_HORA)\n",
    "# =====================================================\n",
    "import joblib\n",
    "\n",
    "def target_encode_fit(df_train: pd.DataFrame, cols, target=target_col, smooth=200):\n",
    "    mappings, defaults = {}, {}\n",
    "    global_mean = df_train[target].mean()\n",
    "    for c in cols:\n",
    "        g = df_train.groupby(c)[target]\n",
    "        stats = g.mean()\n",
    "        cnts  = g.size()\n",
    "        te = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "        mappings[c] = te.to_dict()\n",
    "        defaults[c] = float(global_mean)\n",
    "    return mappings, defaults, float(global_mean)\n",
    "\n",
    "def target_encode_apply(df_part: pd.DataFrame, cols, mappings, defaults):\n",
    "    out = df_part.copy()\n",
    "    for c in cols:\n",
    "        col_te = f\"{c}_TE\"\n",
    "        out[col_te] = out[c].astype(str).map(mappings[c]).fillna(defaults[c]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "def build_agg(df_train: pd.DataFrame, keys, target=target_col, pref=\"AIR\", smooth=20):\n",
    "    g = df_train.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "    global_mean = df_train[target].mean()\n",
    "    rate = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "    out = rate.rename(f\"{pref}_rate\").reset_index()\n",
    "    out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "    return out.astype({f\"{pref}_rate\":\"float32\", f\"{pref}_n\":\"float32\"})\n",
    "\n",
    "def apply_aggs(df_part: pd.DataFrame, agg_tables: dict):\n",
    "    out = df_part.copy()\n",
    "    for pref, (keys, table) in agg_tables.items():\n",
    "        out = out.merge(table, on=keys, how=\"left\")\n",
    "        out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(0).astype(\"float32\")\n",
    "        out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# --- FIT TE solo con TRAIN ---\n",
    "mappings, defaults, global_mean = target_encode_fit(v_train, TE_COLS, target_col, smooth=200)\n",
    "\n",
    "# --- Aplicar TE a TRAIN/VALID (agregando columnas *_TE) ---\n",
    "X_train_te_base = target_encode_apply(v_train[TE_COLS + BASE_FEATS], TE_COLS, mappings, defaults)\n",
    "X_valid_te_base = target_encode_apply(v_valid[TE_COLS + BASE_FEATS], TE_COLS, mappings, defaults)\n",
    "\n",
    "# --- Agregados SOLO con TRAIN y aplicar a ambos ---\n",
    "aggs_specs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\")\n",
    "]\n",
    "agg_tables = {}\n",
    "for keys, pref in aggs_specs:\n",
    "    agg_tables[pref] = (keys, build_agg(v_train, keys, target_col, pref, smooth=20))\n",
    "\n",
    "X_train_full = apply_aggs(X_train_te_base, agg_tables)\n",
    "X_valid_full = apply_aggs(X_valid_te_base, agg_tables)\n",
    "\n",
    "# Orden de features congelado\n",
    "FEATURE_ORDER = list(X_train_full.columns)\n",
    "\n",
    "# Guardar el \"encoder\" (payload TE + orden features) con el nombre que ya usabas\n",
    "ENCODER_PATH = MODELS_DIR / \"onehot_encoder.joblib\"\n",
    "joblib.dump(\n",
    "    {\"type\":\"target_encoding\",\"mappings\":mappings,\"defaults\":defaults,\n",
    "     \"global_mean\":global_mean,\"cols\":TE_COLS,\"feature_order\":FEATURE_ORDER},\n",
    "    ENCODER_PATH\n",
    ")\n",
    "print(f\"âœ“ TE guardado en: {ENCODER_PATH}\")\n",
    "\n",
    "# Guardar tambiÃ©n feature_order.json (opcional para inspecciÃ³n)\n",
    "with open(ARTIF_DIR / \"feature_order.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_order\": FEATURE_ORDER}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Shapes ->\", X_train_full.shape, X_valid_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faad45c",
   "metadata": {},
   "source": [
    "6 â€” Sanity (construir X_train_model si faltara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d81610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sanity OK | X_train_model: (4299046, 27) | X_valid_model: (932084, 27)\n",
      "Dtypes (primeras): {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6) Alinear columnas y helpers de mÃ©tricas\n",
    "# =========================================\n",
    "def align_features(df: pd.DataFrame, feature_order):\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    return X[feature_order]\n",
    "\n",
    "def _to_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    # bool -> int8\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "    # pandas nullable ints -> float32 (evita NA issues en LightGBM)\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "    # object/category -> codes (por si queda algo no numÃ©rico)\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    for c in non_numeric:\n",
    "        X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "# Si no existen matrices _model, se construyen desde X_train_full / X_valid_full\n",
    "vars_needed = [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]\n",
    "missing = [v for v in vars_needed if v not in globals()]\n",
    "if missing:\n",
    "    alt_needed = [\"X_train_full\",\"X_valid_full\",\"y_train\",\"y_valid\"]\n",
    "    alt_missing = [v for v in alt_needed if v not in globals()]\n",
    "    if alt_missing:\n",
    "        raise RuntimeError(\n",
    "            f\"Faltan variables: {missing} y tampoco estÃ¡n {alt_missing}. \"\n",
    "            f\"Ejecuta las celdas 4 y 5 antes de esta.\"\n",
    "        )\n",
    "    X_train_model = _to_numeric_frame(align_features(X_train_full, FEATURE_ORDER))\n",
    "    X_valid_model = _to_numeric_frame(align_features(X_valid_full, FEATURE_ORDER))\n",
    "\n",
    "# Asegurar orden y tipos\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "X_train_model = _to_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_model = _to_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "# Chequeos\n",
    "assert set(X_train_model.columns) == set(X_valid_model.columns), \"DesalineaciÃ³n de columnas entre train y valid\"\n",
    "assert len(X_train_model) == len(y_train), \"X_train_model y y_train con longitudes distintas\"\n",
    "assert len(X_valid_model) == len(y_valid), \"X_valid_model y y_valid con longitudes distintas\"\n",
    "\n",
    "print(\"âœ“ Sanity OK | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Dtypes (primeras):\", {c: str(X_train_model.dtypes[c]) for c in X_train_model.columns[:8]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a01c6",
   "metadata": {},
   "source": [
    "6.1 â€” (Opcional) Matrices con categÃ³ricas nativas para LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb19303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LGBM categÃ³ricas nativas listas: (4299046, 13) (932084, 13)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6.1) (Opcional) CategÃ³ricas nativas para LGBM (sin TE)\n",
    "#      â€” Solo para LGBM; XGB/RF usarÃ¡n las matrices numÃ©ricas TE\n",
    "# ============================================================\n",
    "# Reconstruimos la base sin TE (solo columnas originales de BASE_FEATS + TE_COLS)\n",
    "X_train_base_for_lgb = v_train[TE_COLS + BASE_FEATS].copy()\n",
    "X_valid_base_for_lgb = v_valid[TE_COLS + BASE_FEATS].copy()\n",
    "for c in TE_COLS:\n",
    "    X_train_base_for_lgb[c] = X_train_base_for_lgb[c].astype(\"category\")\n",
    "    X_valid_base_for_lgb[c] = X_valid_base_for_lgb[c].astype(\"category\")\n",
    "\n",
    "LGBM_CATEGORICAL_FEATURES = TE_COLS[:]  # se pasa esta lista a LightGBM\n",
    "print(\"âœ“ LGBM categÃ³ricas nativas listas:\", X_train_base_for_lgb.shape, X_valid_base_for_lgb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693105a4",
   "metadata": {},
   "source": [
    "6.6 enriquecer matrices LGBM con agregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e90fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LGBM FULL shapes: (4299046, 27) (932084, 27)\n",
      "âœ“ Ejemplo dtypes (primeras): {'AIRLINE': 'category', 'ORIGIN_AIRPORT': 'category', 'DESTINATION_AIRPORT': 'category', 'RUTA': 'category', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32', 'MONTH_SIN': 'float32', 'MONTH_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 6.6 â€” Enriquecer LGBM con AGGs/TE numÃ©ricos\n",
    "#  - Mantener categÃ³ricas nativas sÃ³lo en: AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA\n",
    "#  - AÃ±adir columnas numÃ©ricas de X_train_full/X_valid_full (rates, counts, TE)\n",
    "# =============================================\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Partimos de las bases para LGBM (ya tipadas como 'category')\n",
    "X_train_lgb_full = X_train_base_for_lgb.copy()\n",
    "X_valid_lgb_full = X_valid_base_for_lgb.copy()\n",
    "\n",
    "# 2) Identificamos columnas numÃ©ricas interesantes en los datasets \"full\" (TE + agregados)\n",
    "#    Regla: tomamos todas las columnas de X_train_full que NO estÃ©n en la base categÃ³rica\n",
    "#    ni sean las 4 categÃ³ricas nativas (para no romper dtypes)\n",
    "base_cols_cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "base_cols_all  = list(X_train_base_for_lgb.columns)  # incluye tambiÃ©n numÃ©ricas base (MONTH, DAY_OF_WEEK, etc.)\n",
    "\n",
    "cand_extra = [c for c in X_train_full.columns\n",
    "              if c not in base_cols_cats and c not in base_cols_all]\n",
    "\n",
    "# 3) Filtramos solo numÃ©ricas reales (rates, counts, TE), evitando objetos\n",
    "num_extra = [c for c in cand_extra if str(X_train_full[c].dtype) not in (\"object\",)]\n",
    "\n",
    "# 4) (Opcional) Si quieres ser aÃºn mÃ¡s explÃ­cito: quedarte sÃ³lo con *_rate, *_n y *_TE\n",
    "# num_extra = [c for c in num_extra if c.endswith((\"_rate\",\"_n\",\"_TE\"))]\n",
    "\n",
    "# 5) Anexamos esas columnas numÃ©ricas a las matrices LGBM\n",
    "X_train_lgb_full = pd.concat([X_train_lgb_full.reset_index(drop=True),\n",
    "                              X_train_full[num_extra].reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_valid_lgb_full = pd.concat([X_valid_lgb_full.reset_index(drop=True),\n",
    "                              X_valid_full[num_extra].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 6) Sanidad: asegurarnos de que las 4 categÃ³ricas siguen siendo 'category'\n",
    "for c in base_cols_cats:\n",
    "    if c in X_train_lgb_full.columns:\n",
    "        X_train_lgb_full[c] = X_train_lgb_full[c].astype('category')\n",
    "    if c in X_valid_lgb_full.columns:\n",
    "        X_valid_lgb_full[c] = X_valid_lgb_full[c].astype('category')\n",
    "\n",
    "# 7) Redefinimos quÃ© usarÃ¡ LGBM:\n",
    "X_train_base_for_lgb = X_train_lgb_full\n",
    "X_valid_base_for_lgb = X_valid_lgb_full\n",
    "\n",
    "# 8) Lista de categÃ³ricas nativas se mantiene igual:\n",
    "LGBM_CATEGORICAL_FEATURES = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "\n",
    "print(\"âœ“ LGBM FULL shapes:\", X_train_base_for_lgb.shape, X_valid_base_for_lgb.shape)\n",
    "print(\"âœ“ Ejemplo dtypes (primeras):\", {c: str(X_train_base_for_lgb.dtypes[c]) for c in list(X_train_base_for_lgb.columns[:10])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e560d",
   "metadata": {},
   "source": [
    "Entrenar LGBM, XGB y RF (Numerico/CategÃ³rico) + guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecef9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices | LGBM: (4299046, 13)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.610325\tvalid_0's binary_logloss: 0.546958\n",
      "[400]\tvalid_0's auc: 0.607027\tvalid_0's binary_logloss: 0.546569\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580286\tvalid_0's binary_logloss: 0.460432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5803 | best_iter=2 | tiempo=5.7 min | params={lr:0.02, leaves:255, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.486863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.611638\tvalid_0's binary_logloss: 0.548258\n",
      "[400]\tvalid_0's auc: 0.608812\tvalid_0's binary_logloss: 0.547344\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580037\tvalid_0's binary_logloss: 0.460504\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5800 | best_iter=2 | tiempo=4.6 min | params={lr:0.02, leaves:191, mcs:60, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.609563\tvalid_0's binary_logloss: 0.547503\n",
      "[400]\tvalid_0's auc: 0.606852\tvalid_0's binary_logloss: 0.546267\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.576525\tvalid_0's binary_logloss: 0.460466\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5765 | best_iter=1 | tiempo=4.8 min | params={lr:0.025, leaves:223, mcs:80, ff:0.85, bf:0.85}\n",
      "âœ“ LGBM (mejor de 3) | AUC valid=0.5803\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2951 | Precision: 0.1868 | Recall: 0.9200 | F1: 0.3106 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[127033 644162]\n",
      " [ 12871 148018]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19, 'f1': 0.31061339735108373}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=5.3 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6047 | best_iter=2 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6012 | best_iter=4 | tiempo=4.8 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "âœ“ RF entrenado en 21.1 min | AUC valid=0.6027\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7145 | Precision: 0.2498 | Recall: 0.3265 | F1: 0.2831 | ROC-AUC: 0.6027\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[613436 157759]\n",
      " [108353  52536]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.370) ==\n",
      "Accuracy: 0.5233 | Precision: 0.2133 | Recall: 0.6554 | F1: 0.3219 | ROC-AUC: 0.6027\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[382309 388886]\n",
      " [ 55446 105443]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.37, 'f1': 0.3218562371607618}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "âœ… Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "âœ… Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (NUMÃ‰RICO/CAT) + guardado\n",
    "# ============================================\n",
    "import joblib\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime mÃ©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === SelecciÃ³n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # True: LGBM con categÃ³ricas nativas; False: LGBM con TE numÃ©rico\n",
    "\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"  # ya es numÃ©rico (TE)\n",
    "\n",
    "# XGB/RF con TE numÃ©ricas\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"âœ“ Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM â€” mini-tuning (3 configs mejoradas)\n",
    "# =========================\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,\n",
    "    min_child_samples=70,      # 60â€“80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,               # â†‘ resoluciÃ³n histogramas\n",
    "    feature_fraction=0.85,     # NO usar colsample_bytree simultÃ¡neo\n",
    "    bagging_fraction=0.85,\n",
    "    bagging_freq=1,\n",
    ")\n",
    "\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.02, num_leaves=255),\n",
    "    dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "    dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE numÃ©ricas)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE numÃ©ricas)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"âœ… Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, ademÃ¡s guardamos con tu nombre histÃ³rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"âœ… Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos mÃ­nimos (opcional)\n",
    "meta = {\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee49e7",
   "metadata": {},
   "source": [
    "otro 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices | LGBM: (4299046, 13)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580037\tvalid_0's binary_logloss: 0.460504\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5800 | best_iter=2 | tiempo=1.0 min | params={lr:0.02, leaves:191, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580286\tvalid_0's binary_logloss: 0.460432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5803 | best_iter=2 | tiempo=1.0 min | params={lr:0.02, leaves:255, mcs:60, ff:0.85, bf:0.85}\n",
      "âœ“ LGBM (mejor) | AUC valid=0.5803\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2951 | Precision: 0.1868 | Recall: 0.9200 | F1: 0.3106 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[127033 644162]\n",
      " [ 12871 148018]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19, 'f1': 0.31061339735108373}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=1.4 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "âœ“ RF entrenado en 8.2 min | AUC valid=0.6052\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.6686 | Precision: 0.2393 | Recall: 0.4224 | F1: 0.3055 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[555221 215974]\n",
      " [ 92933  67956]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.410) ==\n",
      "Accuracy: 0.5272 | Precision: 0.2139 | Recall: 0.6503 | F1: 0.3220 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[386720 384475]\n",
      " [ 56255 104634]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.41, 'f1': 0.32195175985156876}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "âœ… Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "âœ… Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (modos FAST/BALANCED/FULL)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---------- Configura el modo ----------\n",
    "# \"FAST\":     solo LGBM nativo (rÃ¡pido)\n",
    "# \"BALANCED\": LGBM + XGB (grid pequeÃ±o) + RF ligero   â† recomendado\n",
    "# \"FULL\":     LGBM + XGB (grid 3) + RF grande (lento)\n",
    "MODE = \"BALANCED\"\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === SelecciÃ³n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # LGBM con categÃ³ricas nativas\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    # fallback a TE numÃ©rico si prefieres\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"\n",
    "\n",
    "# XGB/RF usan TE numÃ©rico\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"âœ“ Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ======== util: barrido de umbral F1 ========\n",
    "import numpy as np\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (categÃ³ricas nativas)\n",
    "# =========================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Ajustes que ayudan con categÃ³ricas de alta cardinalidad\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=3000,            # â†“ porque early_stopping corta antes\n",
    "    min_child_samples=70,         # 60â€“80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    feature_fraction=0.85,\n",
    "    bagging_fraction=0.85,\n",
    "    bagging_freq=1,\n",
    "    # regularizadores de categÃ³ricas:\n",
    "    min_data_per_group=100,       # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=191)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=191),\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "        dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=120), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# MÃ©tricas LGBM\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE numÃ©ricas)\n",
    "# =========================\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=3000,        # â†“ por early stopping\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=120\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    xgb_grid = []  # no corre XGB\n",
    "elif MODE == \"BALANCED\":\n",
    "    xgb_grid = [\n",
    "        xgb_base,  # una sola config razonable\n",
    "    ]\n",
    "else:  # FULL\n",
    "    xgb_grid = [\n",
    "        xgb_base,\n",
    "        {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "        {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "    ]\n",
    "\n",
    "best_xgb = {\"auc\":-1}\n",
    "if xgb_grid:\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "else:\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE numÃ©ricas)\n",
    "# ==================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf_cfg = None  # no corre RF\n",
    "elif MODE == \"BALANCED\":\n",
    "    rf_cfg = dict(n_estimators=120, max_depth=14)   # mucho mÃ¡s rÃ¡pido, similar AUC al grande\n",
    "else:  # FULL\n",
    "    rf_cfg = dict(n_estimators=300, max_depth=18)   # tu configuraciÃ³n pesada\n",
    "\n",
    "if rf_cfg:\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs = time.time()-t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "else:\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"âœ… Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, ademÃ¡s guardamos con tu nombre histÃ³rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"âœ… Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos mÃ­nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1b57a",
   "metadata": {},
   "source": [
    "verificaciones rÃ¡pidas (sin re-entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebcf25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM usado con categÃ³ricas nativas: True\n",
      "CategÃ³ricas registradas: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA']\n",
      "\n",
      "Dtypes categÃ³ricas en Xtr_lgb:\n",
      "{'AIRLINE': 'category', 'ORIGIN_AIRPORT': 'category', 'DESTINATION_AIRPORT': 'category', 'RUTA': 'category'}\n",
      "\n",
      "LGBM best_iteration_: 2\n",
      "\n",
      "Top-15 features por gain (LGBM):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MINUTO_DIA_SALIDA</td>\n",
       "      <td>574523.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORIGIN_AIRPORT</td>\n",
       "      <td>269991.314453</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESTINATION_AIRPORT</td>\n",
       "      <td>200796.485474</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUTA</td>\n",
       "      <td>179082.137115</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MONTH</td>\n",
       "      <td>157465.047974</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SALIDA_SIN</td>\n",
       "      <td>111745.900391</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SALIDA_COS</td>\n",
       "      <td>55363.924652</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>43713.608673</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MONTH_COS</td>\n",
       "      <td>12062.956177</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DISTANCIA_HAV</td>\n",
       "      <td>4638.095947</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRLINE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MONTH_SIN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HORA_SALIDA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature           gain  split\n",
       "11    MINUTO_DIA_SALIDA  574523.000000      2\n",
       "1        ORIGIN_AIRPORT  269991.314453     96\n",
       "2   DESTINATION_AIRPORT  200796.485474     81\n",
       "3                  RUTA  179082.137115    157\n",
       "4                 MONTH  157465.047974     63\n",
       "6            SALIDA_SIN  111745.900391     20\n",
       "7            SALIDA_COS   55363.924652     25\n",
       "5           DAY_OF_WEEK   43713.608673     46\n",
       "9             MONTH_COS   12062.956177     10\n",
       "10        DISTANCIA_HAV    4638.095947      8\n",
       "0               AIRLINE       0.000000      0\n",
       "8             MONTH_SIN       0.000000      0\n",
       "12          HORA_SALIDA       0.000000      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen modelos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1*</th>\n",
       "      <th>thr*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.310613</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.606032</td>\n",
       "      <td>0.320829</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.605190</td>\n",
       "      <td>0.321952</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  modelo       AUC       F1*  thr*\n",
       "0   LGBM  0.580286  0.310613  0.19\n",
       "1    XGB  0.606032  0.320829  0.48\n",
       "2     RF  0.605190  0.321952  0.41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buckets de prob (LGBM): [low, high, n, tasa_real]\n",
      "(np.float64(0.0), np.float64(0.1), 0, None)\n",
      "(np.float64(0.1), np.float64(0.2), 789696, 0.16292092146851447)\n",
      "(np.float64(0.2), np.float64(0.30000000000000004), 142388, 0.22636036744669494)\n",
      "(np.float64(0.30000000000000004), np.float64(0.4), 0, None)\n",
      "(np.float64(0.4), np.float64(0.5), 0, None)\n",
      "(np.float64(0.5), np.float64(0.6000000000000001), 0, None)\n",
      "(np.float64(0.6000000000000001), np.float64(0.7000000000000001), 0, None)\n",
      "(np.float64(0.7000000000000001), np.float64(0.8), 0, None)\n",
      "(np.float64(0.8), np.float64(0.9), 0, None)\n",
      "(np.float64(0.9), np.float64(1.0), 0, None)\n"
     ]
    }
   ],
   "source": [
    "# ===== Auditar la corrida actual (sin reentrenar) =====\n",
    "print(\"LGBM usado con categÃ³ricas nativas:\", True)\n",
    "print(\"CategÃ³ricas registradas:\", LGBM_CATEGORICAL_FEATURES)\n",
    "\n",
    "# 1) Verifica dtypes de las categÃ³ricas (deben ser 'category' u 'object'):\n",
    "print(\"\\nDtypes categÃ³ricas en Xtr_lgb:\")\n",
    "print({c: str(Xtr_lgb[c].dtype) for c in LGBM_CATEGORICAL_FEATURES})\n",
    "\n",
    "# 2) Â¿CuÃ¡ntas iteraciones aprendiÃ³ LGBM?\n",
    "print(\"\\nLGBM best_iteration_:\", getattr(lgbm, \"best_iteration_\", None))\n",
    "\n",
    "# 3) Importancias de LGBM (top 15):\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\": lgbm.feature_name_,\n",
    "        \"gain\": lgbm.booster_.feature_importance(importance_type=\"gain\"),\n",
    "        \"split\": lgbm.booster_.feature_importance(importance_type=\"split\"),\n",
    "    }).sort_values(\"gain\", ascending=False)\n",
    "    print(\"\\nTop-15 features por gain (LGBM):\")\n",
    "    display(fi.head(15))\n",
    "except Exception as e:\n",
    "    print(\"No fue posible extraer importancias:\", e)\n",
    "\n",
    "# 4) ComparaciÃ³n resumida de AUC y mejor F1/umbral\n",
    "def _summary_row(name, proba, best):\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "    if proba is None:\n",
    "        return {\"modelo\": name, \"AUC\": None, \"F1*\": None, \"thr*\": None}\n",
    "    return {\n",
    "        \"modelo\": name,\n",
    "        \"AUC\": float(roc_auc_score(y_valid, proba)),\n",
    "        \"F1*\": float(best.get(\"f1\", None)),\n",
    "        \"thr*\": float(best.get(\"thr\", None)),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "rows.append(_summary_row(\"LGBM\", lgb_proba, best_lgb_thr))\n",
    "rows.append(_summary_row(\"XGB\",  best_xgb.get(\"proba\"), best_xgb_thr))\n",
    "rows.append(_summary_row(\"RF\",   (rf_proba if 'rf_proba' in globals() else None), best_rf_thr))\n",
    "import pandas as pd\n",
    "print(\"\\nResumen modelos:\")\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "# 5) Sanidad de distribuciÃ³n de probabilidades (calibraciÃ³n gruesa)\n",
    "import numpy as np\n",
    "def _prob_buckets(y, p, k=10):\n",
    "    bins = np.linspace(0,1,k+1)\n",
    "    idx = np.digitize(p, bins)-1\n",
    "    out = []\n",
    "    for b in range(k):\n",
    "        mask = (idx==b)\n",
    "        if mask.sum()==0:\n",
    "            out.append((bins[b], bins[b+1], 0, None))\n",
    "        else:\n",
    "            out.append((bins[b], bins[b+1], int(mask.sum()), float(y[mask].mean())))\n",
    "    return out\n",
    "\n",
    "print(\"\\nBuckets de prob (LGBM): [low, high, n, tasa_real]\")\n",
    "for row in _prob_buckets(y_valid.values, lgb_proba, k=10):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c5623",
   "metadata": {},
   "source": [
    "otro 7 v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d6ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices | LGBM: (4299046, 27)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588349\tvalid_0's binary_logloss: 0.459685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5883 | best_iter=4 | tiempo=1.5 min | params={lr:0.02, leaves:255, mcs:70, ff:0.9, bf:0.9}\n",
      "âœ“ LGBM (mejor) | AUC valid=0.5883\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5395 | Precision: 0.2115 | Recall: 0.6114 | F1: 0.3143 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404475 366720]\n",
      " [ 62517  98372]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19999999999999996, 'f1': 0.31429707930432393}\n",
      "\n",
      "=== MEJOR MODELO: LGBM | AUC=0.5883 ===\n",
      "âœ… Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "âœ… Copia LGBM (compatibilidad): D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_regressor_default.joblib\n",
      "âœ… Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (modos FAST/BALANCED/FULL)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---------- Configura el modo ----------\n",
    "# \"FAST\":     solo LGBM nativo (rÃ¡pido)\n",
    "# \"BALANCED\": LGBM + XGB (grid pequeÃ±o) + RF ligero   â† recomendado\n",
    "# \"FULL\":     LGBM + XGB (grid 3) + RF grande (lento)\n",
    "MODE = \"FAST\"\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === SelecciÃ³n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # LGBM con categÃ³ricas nativas\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    # fallback a TE numÃ©rico si prefieres\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"\n",
    "\n",
    "# XGB/RF usan TE numÃ©rico\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"âœ“ Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ======== util: barrido de umbral F1 ========\n",
    "import numpy as np\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (categÃ³ricas nativas)\n",
    "# =========================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Ajustes que ayudan con categÃ³ricas de alta cardinalidad\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=3000,            # â†“ porque early_stopping corta antes\n",
    "    min_child_samples=70,         # 60â€“80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    feature_fraction=0.90,\n",
    "    bagging_fraction=0.90,\n",
    "    bagging_freq=1,\n",
    "    # regularizadores de categÃ³ricas:\n",
    "    min_data_per_group=100,       # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=255)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=191),\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "        dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=120), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# MÃ©tricas LGBM\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE numÃ©ricas)\n",
    "# =========================\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=3000,        # â†“ por early stopping\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=120\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    xgb_grid = []  # no corre XGB\n",
    "elif MODE == \"BALANCED\":\n",
    "    xgb_grid = [\n",
    "        xgb_base,  # una sola config razonable\n",
    "    ]\n",
    "else:  # FULL\n",
    "    xgb_grid = [\n",
    "        xgb_base,\n",
    "        {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "        {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "    ]\n",
    "\n",
    "best_xgb = {\"auc\":-1}\n",
    "if xgb_grid:\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "else:\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE numÃ©ricas)\n",
    "# ==================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf_cfg = None  # no corre RF\n",
    "elif MODE == \"BALANCED\":\n",
    "    rf_cfg = dict(n_estimators=120, max_depth=14)   # mucho mÃ¡s rÃ¡pido, similar AUC al grande\n",
    "else:  # FULL\n",
    "    rf_cfg = dict(n_estimators=300, max_depth=18)   # tu configuraciÃ³n pesada\n",
    "\n",
    "if rf_cfg:\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs = time.time()-t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "else:\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# =========================\n",
    "# D) SelecciÃ³n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"âœ… Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, ademÃ¡s guardamos con tu nombre histÃ³rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"âœ… Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos mÃ­nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255a9a2",
   "metadata": {},
   "source": [
    "0tro 7 v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49442fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices | LGBM: (4299046, 27)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.589119\tvalid_0's binary_logloss: 0.563532\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588349\tvalid_0's binary_logloss: 0.459685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5883 | best_iter=4 | tiempo=2.3 min | params={lr:0.02, leaves:255, mcs:60, ff:0.9, bf:0.9}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.506626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.589839\tvalid_0's binary_logloss: 0.56256\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588188\tvalid_0's binary_logloss: 0.459791\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5882 | best_iter=4 | tiempo=2.9 min | params={lr:0.02, leaves:191, mcs:70, ff:0.9, bf:0.9}\n",
      "âœ“ LGBM (mejor) | AUC valid=0.5883\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5395 | Precision: 0.2115 | Recall: 0.6114 | F1: 0.3143 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404475 366720]\n",
      " [ 62517  98372]]\n",
      "â†’ LGBM umbral F1 Ã³ptimo: {'thr': 0.19999999999999996, 'f1': 0.31429707930432393}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=2.5 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "â†’ XGB umbral F1 Ã³ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "âœ“ RF entrenado en 7.5 min | AUC valid=0.6052\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.6686 | Precision: 0.2393 | Recall: 0.4224 | F1: 0.3055 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[555221 215974]\n",
      " [ 92933  67956]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.410) ==\n",
      "Accuracy: 0.5272 | Precision: 0.2139 | Recall: 0.6503 | F1: 0.3220 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[386720 384475]\n",
      " [ 56255 104634]]\n",
      "â†’ RF umbral F1 Ã³ptimo: {'thr': 0.41, 'f1': 0.32195175985156876}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "âœ… Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "âœ… Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (LGBM con CAT nativas + AGGs)\n",
    "#     Modos: FAST / BALANCED / FULL\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) ConfiguraciÃ³n del experimento\n",
    "# -----------------------------\n",
    "# \"FAST\":     Solo LGBM (rÃ¡pido) para iterar.\n",
    "# \"BALANCED\": LGBM + XGB (1 cfg) + RF ligero  â† recomendado\n",
    "# \"FULL\":     LGBM + XGB (3 cfg) + RF grande  (mÃ¡s lento)\n",
    "MODE = \"BALANCED\"\n",
    "\n",
    "# LGBM usarÃ¡ categÃ³ricas nativas + columnas numÃ©ricas de agregados/TE\n",
    "USE_LGBM_NATIVE_CATS = True\n",
    "\n",
    "# -----------------------------\n",
    "# 1) SelecciÃ³n y sanidad de matrices\n",
    "# -----------------------------\n",
    "# Requisitos de celdas previas:\n",
    "#  - X_train_base_for_lgb, X_valid_base_for_lgb (con ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA'] como 'category'\n",
    "#    y ademÃ¡s anexadas las columnas numÃ©ricas de agregados/TE)  â† generado en la celda 6.6\n",
    "#  - LGBM_CATEGORICAL_FEATURES = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "#  - X_train_model, X_valid_model, FEATURE_ORDER (matrices numÃ©ricas TE/AGG para XGB/RF)\n",
    "#  - y_train, y_valid\n",
    "#  - MODELS_DIR, ARTIF_DIR (rutas de guardado)\n",
    "\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    # LGBM verÃ¡ categÃ³ricas nativas + numÃ©ricas adicionales\n",
    "    Xtr_lgb = X_train_base_for_lgb.copy()\n",
    "    Xva_lgb = X_valid_base_for_lgb.copy()\n",
    "    try:\n",
    "        lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "    except NameError:\n",
    "        lgb_cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "        # Forzar dtype 'category' por si acaso\n",
    "        for c in lgb_cats:\n",
    "            if c in Xtr_lgb.columns: Xtr_lgb[c] = Xtr_lgb[c].astype('category')\n",
    "            if c in Xva_lgb.columns: Xva_lgb[c] = Xva_lgb[c].astype('category')\n",
    "else:\n",
    "    # Fallback: todo numÃ©rico (TE) para LGBM\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"  # ya no hay categÃ³ricas nativas\n",
    "\n",
    "# XGB/RF usan las matrices numÃ©ricas (TE/AGG) alineadas\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"âœ“ Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# Peso por desbalance (clase 1 retraso ~18-19%)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos, 1), 1.0)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utilidades de mÃ©tricas\n",
    "# -----------------------------\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime mÃ©tricas para un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    \"\"\"Barrido simple de umbral para maximizar F1 (Ãºtil en clases desbalanceadas).\"\"\"\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LightGBM (categÃ³ricas nativas + AGGs)\n",
    "# -----------------------------\n",
    "import lightgbm as lgb\n",
    "\n",
    "# HiperparÃ¡metros base (diseÃ±ados para entrenar \"de verdad\" con early stopping)\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=4000,           # suficiente; early_stopping corta antes si no mejora\n",
    "    min_child_samples=60,        # 60â€“80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,                 # histograma mÃ¡s fino\n",
    "    feature_fraction=0.90,       # â†‘ variaciÃ³n por columna\n",
    "    bagging_fraction=0.90,       # â†‘ variaciÃ³n por fila\n",
    "    bagging_freq=1,\n",
    "    # Regularizadores para categÃ³ricas de alta cardinalidad:\n",
    "    min_data_per_group=100,      # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "# PequeÃ±o espacio de bÃºsqueda segÃºn el modo\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=191, min_child_samples=60)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=70),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02,  num_leaves=255, min_child_samples=60),\n",
    "        dict(learning_rate=0.02,  num_leaves=191, min_child_samples=70),\n",
    "        dict(learning_rate=0.015, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    \"\"\"Entrena LGBM con eval AUC y early stopping; retorna modelo, proba, auc y tiempo.\"\"\"\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs  = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:, 1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"âœ“ LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# MÃ©tricas LGBM base y con umbral Ã³ptimo F1\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ LGBM umbral F1 Ã³ptimo:\", best_lgb_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) XGBoost (TE numÃ©ricas)\n",
    "# -----------------------------\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    \"\"\"Entrena XGB con eval AUC y early stopping; retorna modelo, proba, auc y tiempo.\"\"\"\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=4000 if MODE==\"FULL\" else 3000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    secs  = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "else:\n",
    "    if MODE == \"BALANCED\":\n",
    "        xgb_grid = [xgb_base]  # 1 configuraciÃ³n razonable\n",
    "    else:  # FULL\n",
    "        xgb_grid = [\n",
    "            xgb_base,\n",
    "            {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "            {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9},\n",
    "        ]\n",
    "    best_xgb = {\"auc\": -1}\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"â†’ XGB umbral F1 Ã³ptimo:\", best_xgb_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Random Forest (TE numÃ©ricas)\n",
    "# -----------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "else:\n",
    "    rf_cfg = (dict(n_estimators=120, max_depth=14) if MODE==\"BALANCED\"\n",
    "              else dict(n_estimators=300, max_depth=18))\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs  = time.time() - t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:, 1]\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"âœ“ RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"â†’ RF umbral F1 Ã³ptimo:\", best_rf_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) SelecciÃ³n del mejor y guardado\n",
    "# -----------------------------\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb.get(\"auc\", -1), best_xgb.get(\"model\", None), best_xgb.get(\"proba\", None), locals().get(\"best_xgb_thr\", {\"thr\":0.5,\"f1\":-1})),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar best model en tu estructura\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"âœ… Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Compatibilidad histÃ³rica si el mejor fue LGBM\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"âœ… Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos mÃ­nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,                      # orden de columnas numÃ©ricas (XGB/RF)\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS), # bandera informativa\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ… Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951189e9",
   "metadata": {},
   "source": [
    "otro 7 v4 con parametros anteriores que dan uac > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e684659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrices numÃ©ricas | X_train: (4299046, 27)  | X_valid: (932084, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.592492\tvalid_0's binary_logloss: 0.596723\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.594905\tvalid_0's binary_logloss: 0.46069\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "âœ… LGBM listo | AUC valid=0.5949 | best_iter=2 | tiempo=1.5 min\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5949\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4012 | Precision: 0.1978 | Recall: 0.8079 | F1: 0.3178 | ROC-AUC: 0.5949\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[243967 527228]\n",
      " [ 30912 129977]]\n",
      "â†’ Umbral F1 Ã³ptimo: {'thr': 0.19999999999999996, 'f1': 0.31775566132987165}\n",
      "ğŸ’¾ Modelo guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "ğŸ“ Metadatos guardados: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata_lgbm.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) LGBM \"WINNER\" â€” lr=0.05, leaves=63, mcs=100, ff=0.8, bf=0.8\n",
    "#     (replica la corrida que te dio AUC ~0.6025)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ---- Utilidades de reporte ----\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.60, steps=56):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# ---- Asegurar rutas de artefactos ----\n",
    "try:\n",
    "    ARTIF_DIR\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "    ARTIF_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- SelecciÃ³n de matrices (prioriza numÃ©ricas con AGGs/TE) ----\n",
    "use_numeric = 'X_train_model' in globals() and 'X_valid_model' in globals()\n",
    "if use_numeric:\n",
    "    Xtr, Xva = X_train_model, X_valid_model\n",
    "    cats = None  # ya es todo numÃ©rico\n",
    "    print(f\"âœ“ Matrices numÃ©ricas | X_train: {Xtr.shape}  | X_valid: {Xva.shape}\")\n",
    "else:\n",
    "    # fallback a las categÃ³ricas nativas\n",
    "    Xtr, Xva = Xtr_lgb.copy(), Xva_lgb.copy()\n",
    "    cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "    for c in cats:\n",
    "        if c in Xtr.columns: Xtr[c] = Xtr[c].astype('category')\n",
    "        if c in Xva.columns: Xva[c] = Xva[c].astype('category')\n",
    "    print(f\"âœ“ Matrices LGBM (cats nativas) | X_train: {Xtr.shape}  | X_valid: {Xva.shape}\")\n",
    "\n",
    "# ---- Pos/Neg para desbalance ----\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ---- ParÃ¡metros ganadores (los que te dieron ~0.6025) ----\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=100,\n",
    "    n_estimators=3000,           # early_stopping cortarÃ¡ mucho antes\n",
    "    feature_fraction=0.80,\n",
    "    bagging_fraction=0.80,\n",
    "    bagging_freq=1,\n",
    "    max_bin=255,\n",
    "    # regularizadores Ãºtiles con alta cardinalidad (no estorban en numÃ©rico)\n",
    "    min_data_per_group=100,\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    "    # desbalance\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    # estabilidad/performance\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    # en datasets grandes suele ir mejor col-wise\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# ---- Entrenamiento ----\n",
    "t0 = time.time()\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "fit_kwargs = dict(\n",
    "    X=Xtr, y=y_train,\n",
    "    eval_set=[(Xva, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "if cats is not None:\n",
    "    fit_kwargs[\"categorical_feature\"] = cats\n",
    "\n",
    "lgbm.fit(**fit_kwargs)\n",
    "secs = time.time() - t0\n",
    "\n",
    "proba = lgbm.predict_proba(Xva)[:,1]\n",
    "auc   = roc_auc_score(y_valid, proba)\n",
    "print(f\"âœ… LGBM listo | AUC valid={auc:.4f} | best_iter={getattr(lgbm,'best_iteration_',None)} | tiempo={secs/60:.1f} min\")\n",
    "\n",
    "# ---- Reportes: base 0.5 y mejor F1 ----\n",
    "_ = report_metrics(y_valid, proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_thr = best_f1_threshold(y_valid, proba, lo=0.05, hi=0.60, steps=56)\n",
    "_ = report_metrics(y_valid, proba, best_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo:\", best_thr)\n",
    "\n",
    "# ---- Guardado de modelo + metadatos ----\n",
    "best_model_path = MODELS_DIR / \"lgbm_retrasos.joblib\"\n",
    "joblib.dump(lgbm, best_model_path)\n",
    "print(f\"ğŸ’¾ Modelo guardado en: {best_model_path}\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"lgbm\",\n",
    "    \"auc_valid\": float(auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_count\": int(Xtr.shape[1]),\n",
    "    \"used_numeric_matrix\": bool(use_numeric),\n",
    "    \"categorical_features\": (cats or []),\n",
    "    \"params\": {\n",
    "        **{k: (float(v) if isinstance(v, (np.floating,)) else v) for k, v in lgb_params.items()},\n",
    "        \"scale_pos_weight\": float(scale_pos_weight)\n",
    "    },\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata_lgbm.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"ğŸ“ Metadatos guardados:\", ARTIF_DIR / \"metadata_lgbm.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def371e",
   "metadata": {},
   "source": [
    "Celda 7 (patch LGBM con 23 features y sin IDs crudos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc080da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LGBM numÃ©rico (filtrado) | X_train: (4299046, 23) | X_valid: (932084, 23)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 3531\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.592105\tvalid_0's binary_logloss: 0.596356\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.592992\tvalid_0's binary_logloss: 0.460485\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "âœ… LGBM listo | AUC valid=0.5930 | best_iter=1 | tiempo=1.3 min\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5930\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4339 | Precision: 0.1987 | Recall: 0.7515 | F1: 0.3143 | ROC-AUC: 0.5930\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[283500 487695]\n",
      " [ 39975 120914]]\n",
      "â†’ Umbral F1 Ã³ptimo: {'thr': 0.19999999999999996, 'f1': 0.31426722356653297}\n",
      "ğŸ’¾ Modelo guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "ğŸ“ Metadatos guardados: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata_lgbm.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) LGBM WINNER (numÃ©rico TE+aggs) - 23 features (sin IDs crudos)\n",
    "# ============================================\n",
    "import time, json, joblib, numpy as np, lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.60, steps=56):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# --- Asegurar rutas ---\n",
    "try:\n",
    "    ARTIF_DIR\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "    ARTIF_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Whitelist de 23 features (SIN ids crudos) ---\n",
    "FEATS_BASE = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "              \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"]\n",
    "FEATS_TE   = [\"AIRLINE_TE\",\"ORIGIN_AIRPORT_TE\",\"DESTINATION_AIRPORT_TE\",\"RUTA_TE\"]\n",
    "FEATS_AGG  = [\"AIR_rate\",\"AIR_n\",\"DES_rate\",\"DES_n\",\"ORI_rate\",\"ORI_n\",\n",
    "              \"RUTA_rate\",\"RUTA_n\",\"RUTA_HORA_rate\",\"RUTA_HORA_n\"]\n",
    "FEATS_23   = FEATS_BASE + FEATS_TE + FEATS_AGG\n",
    "\n",
    "# --- SelecciÃ³n segura de matrices numÃ©ricas y filtrado de columnas ---\n",
    "assert 'X_train_model' in globals() and 'X_valid_model' in globals(), \\\n",
    "    \"No encuentro X_train_model / X_valid_model. Asegura ejecutar TE+aggs antes.\"\n",
    "missing_cols = [c for c in FEATS_23 if c not in X_train_model.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"Faltan columnas esperadas en X_train_model: {missing_cols}\")\n",
    "\n",
    "Xtr = X_train_model[FEATS_23].copy()\n",
    "Xva = X_valid_model[FEATS_23].copy()\n",
    "print(f\"âœ“ LGBM numÃ©rico (filtrado) | X_train: {Xtr.shape} | X_valid: {Xva.shape}\")\n",
    "\n",
    "# --- Desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# --- HiperparÃ¡metros ganadores ---\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=100,\n",
    "    n_estimators=3000,\n",
    "    feature_fraction=0.80,\n",
    "    bagging_fraction=0.80,\n",
    "    bagging_freq=1,\n",
    "    max_bin=255,\n",
    "    min_data_per_group=100,\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# --- Entrenar ---\n",
    "t0 = time.time()\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "lgbm.fit(\n",
    "    Xtr, y_train,\n",
    "    eval_set=[(Xva, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "secs  = time.time() - t0\n",
    "proba = lgbm.predict_proba(Xva)[:,1]\n",
    "auc   = roc_auc_score(y_valid, proba)\n",
    "\n",
    "print(f\"âœ… LGBM listo | AUC valid={auc:.4f} | best_iter={getattr(lgbm,'best_iteration_',None)} | tiempo={secs/60:.1f} min\")\n",
    "_ = report_metrics(y_valid, proba, 0.5,               \"LGBM Base 0.5\")\n",
    "best_thr = best_f1_threshold(y_valid, proba, 0.05, 0.60, 56)\n",
    "_ = report_metrics(y_valid, proba, best_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo:\", best_thr)\n",
    "\n",
    "# --- Guardar modelo + metadatos ---\n",
    "best_model_path = MODELS_DIR / \"lgbm_retrasos.joblib\"\n",
    "joblib.dump(lgbm, best_model_path)\n",
    "print(f\"ğŸ’¾ Modelo guardado en: {best_model_path}\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"lgbm\",\n",
    "    \"auc_valid\": float(auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_list\": FEATS_23,\n",
    "    \"feature_count\": int(Xtr.shape[1]),\n",
    "    \"used_numeric_matrix\": True,\n",
    "    \"categorical_features\": [],\n",
    "    \"params\": {**lgb_params, \"scale_pos_weight\": float(scale_pos_weight)},\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata_lgbm.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"ğŸ“ Metadatos guardados:\", ARTIF_DIR / \"metadata_lgbm.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c974b3",
   "metadata": {},
   "source": [
    "Celda 7B (comparativa rÃ¡pida XGB/RF + tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d918848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ XGB | AUC=0.5972 | best_iter=9 | t=2.1m | F1*=0.319 @thr=0.440\n",
      "âœ“ RF  | AUC=0.5978 | t=21.4m | F1*=0.319 @thr=0.300\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "=== Resumen modelos (AUC, F1*, thr*) ===\n",
      " LGBM  AUC=0.5930  F1*=0.314  thr*=0.200  (acc=0.434, pre=0.199, rec=0.752)\n",
      "  XGB  AUC=0.5972  F1*=0.319  thr*=0.440  (acc=0.498, pre=0.208, rec=0.682)\n",
      "   RF  AUC=0.5978  F1*=0.319  thr*=0.300  (acc=0.525, pre=0.212, rec=0.644)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7B) Comparativa rÃ¡pida: XGB y RF sobre las MISMAS 23 features\n",
    "# ============================================\n",
    "import time, numpy as np, json, joblib\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.80, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "def quick_report(name, y_true, proba):\n",
    "    thr = best_f1_threshold(y_true, proba)\n",
    "    yhat = (proba >= thr[\"thr\"]).astype(int)\n",
    "    return dict(\n",
    "        modelo=name,\n",
    "        auc=float(roc_auc_score(y_true, proba)),\n",
    "        f1=float(thr[\"f1\"]),\n",
    "        thr=float(thr[\"thr\"]),\n",
    "        acc=float(accuracy_score(y_true, yhat)),\n",
    "        pre=float(precision_score(y_true, yhat, zero_division=0)),\n",
    "        rec=float(recall_score(y_true, yhat, zero_division=0)),\n",
    "    )\n",
    "\n",
    "# Matrices de 23 features (mismas que usÃ³ LGBM arriba)\n",
    "Xtr_23 = X_train_model[FEATS_23]\n",
    "Xva_23 = X_valid_model[FEATS_23]\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ---- XGBoost (setup probado) ----\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "dtr = DMatrix(Xtr_23, label=y_train)\n",
    "dva = DMatrix(Xva_23, label=y_valid)\n",
    "t0 = time.time()\n",
    "xgbm = xgb_train(\n",
    "    xgb_params, dtr,\n",
    "    num_boost_round=3000,\n",
    "    evals=[(dva, \"valid\")],\n",
    "    verbose_eval=False,\n",
    "    early_stopping_rounds=120\n",
    ")\n",
    "xgb_secs = time.time()-t0\n",
    "xgb_proba = xgbm.predict(dva, iteration_range=(0, xgbm.best_iteration+1))\n",
    "xgb_res = quick_report(\"XGB\", y_valid, xgb_proba)\n",
    "print(f\"âœ“ XGB | AUC={xgb_res['auc']:.4f} | best_iter={xgbm.best_iteration} | t={xgb_secs/60:.1f}m | F1*={xgb_res['f1']:.3f} @thr={xgb_res['thr']:.3f}\")\n",
    "\n",
    "# ---- Random Forest (ligero probado) ----\n",
    "rf_params = dict(\n",
    "    n_estimators=300, max_depth=24,\n",
    "    min_samples_leaf=2, min_samples_split=4,\n",
    "    max_features=\"sqrt\", bootstrap=True,\n",
    "    n_jobs=-1, random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(Xtr_23, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(Xva_23)[:,1]\n",
    "rf_res = quick_report(\"RF\", y_valid, rf_proba)\n",
    "print(f\"âœ“ RF  | AUC={rf_res['auc']:.4f} | t={rf_secs/60:.1f}m | F1*={rf_res['f1']:.3f} @thr={rf_res['thr']:.3f}\")\n",
    "\n",
    "# ---- Resumen incluyendo LGBM entrenado arriba ----\n",
    "try:\n",
    "    lgb_proba_ = lgbm.predict_proba(Xva_23)[:,1]  # reutiliza objeto de Celda 7\n",
    "    lgb_res = quick_report(\"LGBM\", y_valid, lgb_proba_)\n",
    "except Exception:\n",
    "    lgb_res = {\"modelo\":\"LGBM\",\"auc\":np.nan,\"f1\":np.nan,\"thr\":np.nan,\"acc\":np.nan,\"pre\":np.nan,\"rec\":np.nan}\n",
    "\n",
    "resumen = [lgb_res, xgb_res, rf_res]\n",
    "print(\"\\n=== Resumen modelos (AUC, F1*, thr*) ===\")\n",
    "for r in resumen:\n",
    "    print(f\"{r['modelo']:>5}  AUC={r['auc']:.4f}  F1*={r['f1']:.3f}  thr*={r['thr']:.3f}  (acc={r['acc']:.3f}, pre={r['pre']:.3f}, rec={r['rec']:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
