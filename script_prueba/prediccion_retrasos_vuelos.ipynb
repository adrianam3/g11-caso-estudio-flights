{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0a1e6",
   "metadata": {},
   "source": [
    "\n",
    "# PredicciÃ³n de Retrasos de Vuelos en la Industria AÃ©rea âœˆï¸\n",
    "\n",
    "**Objetivo:** Construir un modelo que **prediga si un vuelo sufrirÃ¡ un retraso de llegada mayor a 15 minutos** (`RETRASADO_LLEGADA` = 1).  \n",
    "Este notebook sigue la estructura de `machine_learning.ipynb`, con secciones de carga de datos, EDA, preparaciÃ³n, entrenamiento con **LightGBM**, evaluaciÃ³n y conclusiones.\n",
    "\n",
    "**Dataset de entrada:** `data/processed/flights_clean.csv` (resultado del pipeline de limpieza e ingenierÃ­a de caracterÃ­sticas).  \n",
    "**TamaÃ±o esperado:** ~5.2M filas (podrÃ­a requerir >8GB RAM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481317d",
   "metadata": {},
   "source": [
    "## 1. Importaciones y configuraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc87b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"LightGBM no estÃ¡ instalado. Instala con: pip install lightgbm\") from e\n",
    "\n",
    "# ConfiguraciÃ³n visual\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb56eb",
   "metadata": {},
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133005",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eafda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.read_csv(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59569525",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feab64",
   "metadata": {},
   "source": [
    "## 3. InspecciÃ³n rÃ¡pida de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463af07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v.info(memory_usage='deep', show_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b78f3",
   "metadata": {},
   "source": [
    "## 4. DistribuciÃ³n de retrasos (llegada > 15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert \"RETRASADO_LLEGADA\" in v.columns, \"No existe la columna RETRASADO_LLEGADA en el dataset limpio.\"\n",
    "\n",
    "conteo = v[\"RETRASADO_LLEGADA\"].value_counts().sort_index()\n",
    "porc = (conteo / conteo.sum() * 100).round(2)\n",
    "\n",
    "print(\"ğŸ“Š DistribuciÃ³n de vuelos segÃºn retraso en llegada (>15 min):\\n\")\n",
    "print(f\"A tiempo (0): {conteo.get(0,0):,} vuelos ({porc.get(0,0):.2f}%)\")\n",
    "print(f\"Retrasados (1): {conteo.get(1,0):,} vuelos ({porc.get(1,0):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509f74a",
   "metadata": {},
   "source": [
    "## 5. SelecciÃ³n de variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf346a",
   "metadata": {},
   "source": [
    "\n",
    "Usaremos variables **categÃ³ricas y de tiempo** ya generadas en el pipeline:\n",
    "\n",
    "- `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT` (categÃ³ricas)\n",
    "- `MONTH`, `DAY_OF_WEEK` (tiempo)\n",
    "- CodificaciÃ³n cÃ­clica: `SALIDA_SIN`, `SALIDA_COS` (derivadas de la hora programada de salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\"\n",
    "]\n",
    "\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "# Liberar memoria del dataframe original si es necesario\n",
    "del v\n",
    "gc.collect()\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c69ea",
   "metadata": {},
   "source": [
    "## 6. CodificaciÃ³n de variables categÃ³ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430587be",
   "metadata": {},
   "source": [
    "\n",
    "Para eficiencia con >5M de filas, usamos **Label Encoding** para `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT`.  \n",
    "LightGBM maneja bien etiquetas enteras y permite splits por categorÃ­a, especialmente cuando las variables no son ordinales reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef32330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Guardar encoders en memoria (opcional: persistir a disco si se desea)\n",
    "print(\"âœ… CategÃ³ricas codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8235487",
   "metadata": {},
   "source": [
    "## 7. DivisiÃ³n Train/Test (estratificada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2506a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0e536",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento: LightGBM (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = dict(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",  # âœ… Compensa desbalance (18/82 aprox.)\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"âœ… Modelo entrenado en {t1 - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9c69f",
   "metadata": {},
   "source": [
    "## 9. EvaluaciÃ³n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {prec:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1-score:   {f1:.4f}\")\n",
    "print(f\"ROC-AUC:    {roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8211b",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31077444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC = {roc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC - Retrasos de Llegada\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575726a",
   "metadata": {},
   "source": [
    "### Matriz de confusiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8431adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233e1f",
   "metadata": {},
   "source": [
    "## 10. Importancia de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = lgb.plot_importance(model, max_num_features=20, importance_type=\"gain\")\n",
    "plt.title(\"Importancia de variables (LightGBM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75011e51",
   "metadata": {},
   "source": [
    "## 11. FunciÃ³n de predicciÃ³n (para integraciÃ³n futura con API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preparar_entrada(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute):\n",
    "    \"\"\"\n",
    "    Prepara un diccionario con las features necesarias para predicciÃ³n.\n",
    "    - scheduled_hour: 0-23\n",
    "    - scheduled_minute: 0-59\n",
    "    \"\"\"\n",
    "    minuto_dia = scheduled_hour * 60 + scheduled_minute\n",
    "    salida_sin = math.sin(2 * math.pi * minuto_dia / (24*60))\n",
    "    salida_cos = math.cos(2 * math.pi * minuto_dia / (24*60))\n",
    "    row = {\n",
    "        \"AIRLINE\": encoders[\"AIRLINE\"].transform([str(airline)])[0],\n",
    "        \"ORIGIN_AIRPORT\": encoders[\"ORIGIN_AIRPORT\"].transform([str(origin)])[0],\n",
    "        \"DESTINATION_AIRPORT\": encoders[\"DESTINATION_AIRPORT\"].transform([str(destination)])[0],\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def predecir_probabilidad_delay(sample_dict):\n",
    "    \"\"\"Recibe un diccionario de features y devuelve probabilidad de retraso en llegada (>15 min).\"\"\"\n",
    "    df = pd.DataFrame([sample_dict])[list(model.feature_name_)]\n",
    "    proba = model.predict_proba(df)[:, 1][0]\n",
    "    return float(proba)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo = preparar_entrada(\"AA\", \"JFK\", \"LAX\", 5, 4, 14, 30)\n",
    "print(\"Probabilidad de retraso (ejemplo):\", round(predecir_probabilidad_delay(ejemplo), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53002b4",
   "metadata": {},
   "source": [
    "## 12. Conclusiones y siguientes pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91e733",
   "metadata": {},
   "source": [
    "\n",
    "- El modelo **LightGBM** con `class_weight=\"balanced\"` maneja correctamente el desbalance (~18% retrasos).\n",
    "- Las variables de ubicaciÃ³n (aeropuerto y aerolÃ­nea) y la codificaciÃ³n cÃ­clica de la hora suelen aportar poder predictivo.\n",
    "- Para producciÃ³n:\n",
    "  - serializar `model` y `encoders` con `joblib`,\n",
    "  - crear un endpoint `/flights/predict-delay` con FastAPI,\n",
    "  - validar en datos recientes y monitorear mÃ©tricas.\n",
    "\n",
    "**Mejoras posibles:**\n",
    "- Agregar variable de **distancia Haversine**.\n",
    "- Agregar **mes/dÃ­a** como seno/coseno (estacionalidad).\n",
    "- RegularizaciÃ³n y **bÃºsqueda de hiperparÃ¡metros** (Optuna).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cf1c",
   "metadata": {},
   "source": [
    "## 13. (Opcional) Guardado de modelo y encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from joblib import dump\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "# dump(model, \"models/lgbm_delay_model.joblib\")\n",
    "# dump(encoders, \"models/label_encoders.joblib\")\n",
    "# print(\"âœ… Modelo y encoders guardados en carpeta models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3b12a",
   "metadata": {},
   "source": [
    "### 14. Resumen: \n",
    "Resumo lo que muestran tus capturas y quÃ© harÃ­a para mejorarlo rÃ¡pido:\n",
    "\n",
    "Lectura rÃ¡pida de resultados\n",
    "\n",
    "Accuracy ~0.783: por debajo del baseline â€œtodo a tiempoâ€ (â‰ˆ0.815). Normal cuando forzamos a detectar mÃ¡s retrasos.\n",
    "\n",
    "Precision â‰ˆ 0.63 | Recall â‰ˆ 0.64 | F1 â‰ˆ 0.63 | ROC-AUC â‰ˆ 0.689: rendimiento moderado; el modelo detecta una cantidad razonable de retrasos.\n",
    "\n",
    "Importancia de variables: domina SALIDA_SIN (hora del dÃ­a), luego AIRLINE, MONTH, aeropuertos, DAY_OF_WEEK, SALIDA_COS. Tiene sentido: hora, aerolÃ­nea, estacionalidad y aeropuertos pesan mucho.\n",
    "\n",
    "Nota: si la matriz de confusiÃ³n no cuadra con tus mÃ©tricas, recuerda que confusion_matrix(y_test, y_pred) retorna [[TN, FP],[FN, TP]]. Verifica que estÃ©s leyendo TP = [1,1] y FP = [0,1], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a80dcd",
   "metadata": {},
   "source": [
    "###  RevisiÃ³n 2  - Revsiones dicionales post resultados.. \n",
    "\n",
    "a continuaciÃ³n (en orden de impacto vs. esfuerzo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87803653",
   "metadata": {},
   "source": [
    "1) Nuevas features: Haversine + estacionalidad\n",
    "\n",
    "Pega esta celda despuÃ©s de cargar v o (si ya tenÃ­as X, y) vuelve a crear X con estas columnas nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ca2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "# Si aÃºn tienes el DF completo como 'v', Ãºsalo. Si ya lo liberaste, vuelve a cargar flights_clean y rehaz lo mÃ­nimo.\n",
    "try:\n",
    "    v\n",
    "except NameError:\n",
    "    import os, time\n",
    "    DATA_PATH = \"data/processed/flights_clean.csv\"\n",
    "    t0 = time.time()\n",
    "    v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    print(\"Re-cargado v:\", v.shape, f\"en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Chequeo de columnas necesarias\n",
    "req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "       \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\"]\n",
    "falta = [c for c in req if c not in v.columns]\n",
    "if falta:\n",
    "    raise ValueError(\"Faltan columnas en v: \" + \", \".join(falta))\n",
    "\n",
    "# 1) Distancia Haversine\n",
    "v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# 2) Estacionalidad anual del mes (seno/coseno)\n",
    "v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# 3) (Opcional) Minuto crudo del dÃ­a si lo tienes; si no, lo calculo rÃ¡pido\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Requiere HORA_SALIDA y MIN_SALIDA; si no estÃ¡n, se puede derivar de SCHEDULED_DEPARTURE HHMM\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(np.int16)\n",
    "    else:\n",
    "        if \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "            hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# === SelecciÃ³n de variables actualizada ===\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\n",
    "    \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "]\n",
    "\n",
    "# (Opcional) aÃ±adir \"MINUTO_DIA_SALIDA\" si existe:\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "X.dtypes, X.shape, y.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655b525",
   "metadata": {},
   "source": [
    "2) CodificaciÃ³n categÃ³rica igual que antes (LabelEncoder)\n",
    "\n",
    "Si ya lo tenÃ­as, puedes saltarla; si no, ejÃ©cÃºtala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"Codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ead0fb",
   "metadata": {},
   "source": [
    "3) Split y entrenamiento con early stopping\n",
    "\n",
    "Entrenamos con validaciÃ³n en el set de test y paramos cuando deje de mejorar AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f580ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usar scale_pos_weight, ver mÃ¡s abajo\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iteration_={model.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77873373",
   "metadata": {},
   "source": [
    "ponderaciÃ³n explÃ­cita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903de63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = y_train.sum(); neg = len(y_train)-pos\n",
    "spw = neg / pos\n",
    "# usar scale_pos_weight=spw y quitar class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65c187",
   "metadata": {},
   "source": [
    "4) MÃ©tricas a umbral 0.5 y bÃºsqueda del mejor umbral\n",
    "\n",
    "Calcula mÃ©tricas en 0.5, luego busca mejor F1 y mejor recall con precisiÃ³n mÃ­nima (p. ej. â‰¥0.60). Imprime confusiones y compara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# MÃ©tricas al umbral 0.5\n",
    "y_pred05 = (y_proba>=0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"Umbral 0.5\")\n",
    "\n",
    "# BÃºsqueda de mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "\n",
    "# BÃºsqueda max recall con precisiÃ³n mÃ­nima (ajusta min_prec segun negocio)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"prec\":0}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_test, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_test, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":thr, \"rec\":rec, \"prec\":pre}\n",
    "\n",
    "y_pred_rec = (y_proba>=best_rec[\"thr\"]).astype(int)\n",
    "rec_res = metricas(y_test, y_pred_rec, y_proba, f\"Max Recall con Prec â‰¥ {min_prec:.2f} (thr={best_rec['thr']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2e2a7",
   "metadata": {},
   "source": [
    "5) (Opcional) Curva Precision-Recall para elegir umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65519f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b425c",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 3 -  nueva revisiÃ³n "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932e89d",
   "metadata": {},
   "source": [
    "Despues de los resultados anteriores, se sugiere los siguiente\n",
    "aplicar Target Encoding con K-Fold (sin fuga) a RUTA, AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT. Incluyen: creaciÃ³n de RUTA, codificaciÃ³n KFold con smoothing, split estratificado, entrenamiento LightGBM con early stopping, comparaciÃ³n de mÃ©tricas y guardado de mapeos para usar en producciÃ³n/API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4713d",
   "metadata": {},
   "source": [
    "Notas finales\n",
    "\n",
    "El Target Encoding K-Fold suele subir AUC/F1 sobre tus resultados actuales, especialmente con RUTA_TE.\n",
    "\n",
    "El smoothing controla el â€œoverfitâ€ en categorÃ­as raras. Para rutas con pocos vuelos, empuja la media hacia la global. Puedes probar 20, 50, 100.\n",
    "\n",
    "Para producciÃ³n, ideal: precalcular DISTANCIA_HAV a partir de lat/lon del origen/destino (que ya tienes en catÃ¡logos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f25528",
   "metadata": {},
   "source": [
    "0) (Opcional) Asegurar features base\n",
    "\n",
    "Si ya calculaste DISTANCIA_HAV, MONTH_SIN/COS, etc., puedes saltar esta celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# RUTA (origen_destino)\n",
    "v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# Distancia (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# (opcional) minuto del dÃ­a\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "v.shape, v.columns[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876fc30",
   "metadata": {},
   "source": [
    "1) Funciones de Target Encoding K-Fold (sin fuga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76725cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_target_encode(train_df, col, target, n_splits=5, smoothing=50, random_state=42):\n",
    "    \"\"\"\n",
    "    KFold target encoding sin fuga: devuelve\n",
    "      enc_train: serie con el encoding para el train (via KFold)\n",
    "      mapping: dict {categoria: media_suavizada} usando TODO el train (para aplicar en test/producciÃ³n)\n",
    "      default: media global del target (para categorÃ­as nuevas)\n",
    "    smoothing: mayor valor => mÃ¡s peso de la media global si hay pocos datos por categorÃ­a\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = float(train_df[target].mean())\n",
    "\n",
    "    enc_train = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, train_df[target]):\n",
    "        tr, val = train_df.iloc[tr_idx], train_df.iloc[val_idx]\n",
    "        stats = tr.groupby(col)[target].mean()\n",
    "        cnts  = tr[col].value_counts()\n",
    "        smooth = (cnts*stats + smoothing*global_mean) / (cnts + smoothing)\n",
    "        enc_train.iloc[val_idx] = val[col].map(smooth).fillna(global_mean).astype(np.float32)\n",
    "\n",
    "    # mapping final con TODO el train (para test/producciÃ³n)\n",
    "    full_stats = train_df.groupby(col)[target].mean()\n",
    "    full_cnts  = train_df[col].value_counts()\n",
    "    mapping = ((full_cnts*full_stats + smoothing*global_mean)/(full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc_train, mapping, global_mean\n",
    "\n",
    "def aplicar_target_encoding_test(test_df, col, mapping, default):\n",
    "    return test_df[col].map(mapping).fillna(default).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321751c",
   "metadata": {},
   "source": [
    "2) PreparaciÃ³n de train/test y generaciÃ³n de *_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cols_te = [\"RUTA\", \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# Variables numÃ©ricas base (sÃºmalas a gusto)\n",
    "num_features = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\"\n",
    "]\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    num_features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "# Split estratificado ANTES del encoding para evitar fuga\n",
    "train_df, test_df = train_test_split(v, test_size=0.2, stratify=v[target], random_state=42)\n",
    "\n",
    "# Generar encodings en train y mappings para test\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    tr_enc, mapping, default = kfold_target_encode(train_df, c, target, n_splits=5, smoothing=50, random_state=42)\n",
    "    train_df[f\"{c}_TE\"] = tr_enc\n",
    "    test_df[f\"{c}_TE\"]  = aplicar_target_encoding_test(test_df, c, mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Conjunto final de features\n",
    "features = num_features + [f\"{c}_TE\" for c in cols_te]\n",
    "\n",
    "X_train = train_df[features].copy()\n",
    "y_train = train_df[target].astype(int).copy()\n",
    "X_test  = test_df[features].copy()\n",
    "y_test  = test_df[target].astype(int).copy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a74ce",
   "metadata": {},
   "source": [
    "3) Entrenamiento LightGBM con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usa scale_pos_weight = (neg/pos)\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_te = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model_te.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Entrenado TE en {t1-t0:.1f}s | best_iteration_={model_te.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcbcfc",
   "metadata": {},
   "source": [
    "4) MÃ©tricas a 0.5 + bÃºsqueda de mejor umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model_te.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# Umbral 0.5\n",
    "y_pred05 = (y_proba >= 0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"TE - Umbral 0.5\")\n",
    "\n",
    "# Mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"TE - Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3be42",
   "metadata": {},
   "source": [
    "5) (Opcional) Guardar modelo + mapeos + umbral\n",
    "\n",
    "Ãštil para tu endpoint /flights/predict-delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "dump(model_te, \"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"mappings\": {k: {str(kk): float(vv) for kk, vv in mp.items()} for k, mp in mappings.items()},\n",
    "               \"defaults\": defaults}, f)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1[\"thr\"])  # o fija el que defina negocio\n",
    "with open(\"models/umbral.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "print(\"âœ… Guardados: modelo, mappings y umbral.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4d5dc",
   "metadata": {},
   "source": [
    "6) (ProducciÃ³n) Aplicar TE a un nuevo registro\n",
    "\n",
    "Ejemplo de funciÃ³n para preparar features a partir de un vuelo nuevo (usa mapeos TE; categorÃ­as no vistas â†’ media global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db72026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json\n",
    "import numpy as np\n",
    "\n",
    "# Cargar mappings en producciÃ³n:\n",
    "# from joblib import load; model_te = load(\"models/lgbm_delay_te.joblib\")\n",
    "# mappings = json.load(open(\"models/te_mappings.json\"))[\"mappings\"]\n",
    "# defaults = json.load(open(\"models/te_mappings.json\"))[\"defaults\"]\n",
    "# thr = json.load(open(\"models/umbral.json\"))[\"threshold\"]\n",
    "\n",
    "def preparar_features_api(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute,\n",
    "                          mappings=mappings, defaults=defaults):\n",
    "    # cÃ­clicos de hora\n",
    "    minuto_dia = scheduled_hour*60 + scheduled_minute\n",
    "    salida_sin = math.sin(2*math.pi*minuto_dia/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*minuto_dia/(24*60))\n",
    "    # cÃ­clicos de mes\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    fila = {\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos,\n",
    "        \"MONTH_SIN\": month_sin,\n",
    "        \"MONTH_COS\": month_cos,\n",
    "        # DISTANCIA_HAV deberÃ­a venir precalculada en back si tienes lat/lon; si no, dejar 0 o estimar\n",
    "        \"DISTANCIA_HAV\": 0.0,  \n",
    "        \"RUTA_TE\": float(mappings[\"RUTA\"].get(ruta, defaults[\"RUTA\"])),\n",
    "        \"AIRLINE_TE\": float(mappings[\"AIRLINE\"].get(airline, defaults[\"AIRLINE\"])),\n",
    "        \"ORIGIN_AIRPORT_TE\": float(mappings[\"ORIGIN_AIRPORT\"].get(origin, defaults[\"ORIGIN_AIRPORT\"])),\n",
    "        \"DESTINATION_AIRPORT_TE\": float(mappings[\"DESTINATION_AIRPORT\"].get(destination, defaults[\"DESTINATION_AIRPORT\"]))\n",
    "    }\n",
    "    return pd.DataFrame([fila])[list(model_te.feature_name_)]\n",
    "\n",
    "def predecir_prob_retraso_api(df_features, modelo=model_te, thr=UMBRAL_OPERATIVO):\n",
    "    proba = float(modelo.predict_proba(df_features)[:,1][0])\n",
    "    return {\"prob_delay\": proba, \"delayed\": int(proba >= thr)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62839ad",
   "metadata": {},
   "source": [
    "## revisiÃ³n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fce5d",
   "metadata": {},
   "source": [
    "modelo champion (LightGBM + LabelEncoder) con validaciÃ³n temporal, tuning ligero, elecciÃ³n de umbral y guardado de artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de3547",
   "metadata": {},
   "source": [
    "1) Carga optimizada desde CSV (solo columnas + dtypes chicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da44dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- FunciÃ³n Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# --- Chequeos mÃ­nimos (ya existen en tu CSV segÃºn lo que enviaste) ---\n",
    "cols_req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\n",
    "            \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\",\n",
    "            \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\"]\n",
    "faltan = [c for c in cols_req if c not in v.columns]\n",
    "if faltan:\n",
    "    raise ValueError(\"Faltan columnas base en v: \" + \", \".join(faltan))\n",
    "\n",
    "# 1) Distancia Haversine (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"],\n",
    "                                      v[\"DEST_LAT\"],   v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# 2) Estacionalidad del mes (seno/coseno) (si no existen)\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# 3) Minuto del dÃ­a de salida (si no existe)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Ya tienes HORA_SALIDA y MIN_SALIDA en tu CSV\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "\n",
    "# (Opcional) ruta texto (Ãºtil para anÃ¡lisis; para modelar, mejor encoding posterior)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Dtypes compactos para ahorro de RAM\n",
    "v[\"RETRASADO_LLEGADA\"] = v[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "v[\"MONTH\"] = v[\"MONTH\"].astype(\"int8\")\n",
    "v[\"DAY_OF_WEEK\"] = v[\"DAY_OF_WEEK\"].astype(\"int8\")\n",
    "v[\"DISTANCIA_HAV\"] = v[\"DISTANCIA_HAV\"].astype(\"float32\")\n",
    "v[\"SALIDA_SIN\"] = v[\"SALIDA_SIN\"].astype(\"float32\")\n",
    "v[\"SALIDA_COS\"] = v[\"SALIDA_COS\"].astype(\"float32\")\n",
    "v[\"MONTH_SIN\"] = v[\"MONTH_SIN\"].astype(\"float32\")\n",
    "v[\"MONTH_COS\"] = v[\"MONTH_COS\"].astype(\"float32\")\n",
    "\n",
    "print(\"âœ… Columnas derivadas listas. v.shape:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d3c7",
   "metadata": {},
   "source": [
    "SelecciÃ³n de variables (features) y salida X, y\n",
    "\n",
    "Incluye las nuevas columnas; deja listas para el siguiente paso (split temporal + encoding + fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de entrada para el modelo\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# CategÃ³ricas crudas (luego se codifican con LabelEncoder o category.codes)\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# NumÃ©ricas/derivadas\n",
    "num_cols = [\n",
    "    \"MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\", \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\", \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\"\n",
    "]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "\n",
    "# Validar presencia\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(\"Faltan columnas para features/target: \" + \", \".join(missing))\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(int)\n",
    "\n",
    "print(\"âœ… Features y target listos.\")\n",
    "print(\"X.shape:\", X.shape, \"| y.mean (rate retraso):\", y.mean().round(4))\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811818a7",
   "metadata": {},
   "source": [
    "3) Split temporal (train: meses 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ba8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos v[\"MONTH\"] para crear los masks y luego cortamos X,y sin hacer copias grandes\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask]\n",
    "y_train = y.loc[train_mask].astype(\"int8\")\n",
    "\n",
    "X_valid = X.loc[valid_mask]\n",
    "y_valid = y.loc[valid_mask].astype(\"int8\")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Valid:\", X_valid.shape,\n",
    "      \"| Rate train:\", float(y_train.mean()),\n",
    "      \"| Rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b634f",
   "metadata": {},
   "source": [
    "4) CodificaciÃ³n de categÃ³ricas (LabelEncoder) solo con train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc885a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "# Ajuste en train y transformaciÃ³n consistente en valid\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[c] = le.fit_transform(X_train[c].astype(str))\n",
    "    X_valid[c] = le.transform(X_valid[c].astype(str))\n",
    "    encoders[c] = le\n",
    "\n",
    "# Tipos compactos\n",
    "X_train = X_train.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "X_valid = X_valid.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "\n",
    "X_train.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595cf824",
   "metadata": {},
   "source": [
    "5) Entrenamiento LightGBM con early stopping (validaciÃ³n temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb, time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    # class_weight=\"balanced\",   # o comenta y usa scale_pos_weight=scale_pos_weight\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28654e3",
   "metadata": {},
   "source": [
    "6) MÃ©tricas a 0.5 y bÃºsqueda de umbral (mejor F1 y â€œprecisiÃ³n mÃ­nimaâ€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# 6.1 Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# 6.2 Mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e93e34",
   "metadata": {},
   "source": [
    "(Opcional si Operaciones pide precisiÃ³n mÃ­nima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 MÃ¡ximo Recall con PrecisiÃ³n mÃ­nima (ajusta min_prec)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"pre\":0}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_valid, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_valid, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":float(thr), \"rec\":float(rec), \"pre\":float(pre)}\n",
    "if best_rec[\"rec\"] > 0:\n",
    "    best_rec_res = report_metrics(y_valid, valid_proba, best_rec[\"thr\"], f\"Max Recall con Prec â‰¥ {min_prec:.2f}\")\n",
    "    best_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391b2dc",
   "metadata": {},
   "source": [
    "7) Guardar artefactos (modelo + encoders + umbral + metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])  # o fija el que defina negocio\n",
    "\n",
    "dump(model, \"models/lgbm_delay_champion.joblib\")\n",
    "dump(encoders, \"models/label_encoders.joblib\")\n",
    "with open(\"models/threshold.json\",\"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train.columns),\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"note\": \"LightGBM + LabelEncoder + temporal split (features derivadas activadas)\"\n",
    "}\n",
    "with open(\"models/metadata.json\",\"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ… Guardados: modelo, encoders, threshold y metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac669c",
   "metadata": {},
   "source": [
    "8) (Opcional) Curvas ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as sk_auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_valid, valid_proba)\n",
    "plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_valid, valid_proba):.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',c='grey'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC - ValidaciÃ³n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_valid, valid_proba)\n",
    "prauc = sk_auc(rec, prec)\n",
    "plt.figure(); plt.plot(rec, prec, label=f\"PR AUC={prauc:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall - ValidaciÃ³n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7517c08",
   "metadata": {},
   "source": [
    "### **RevisiÃ³n 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2daa7",
   "metadata": {},
   "source": [
    "guion completo para pasar de flights_clean.csv a un modelo LightGBM con Target Encoding KFold (TE) en AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT y RUTA, con split temporal (1â€“9 vs 10â€“12), bÃºsqueda de umbral operativo, y guardado de artefactos (modelo, mappings TE, threshold, metadata) + una funciÃ³n de predicciÃ³n para producciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9a80e",
   "metadata": {},
   "source": [
    "Paso 0 Â· Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")\n",
    "# print(\"Leyendo:\", os.path.abspath(DATA_PATH))\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "print(\"ğŸ“„ Usando archivo:\", DATA_PATH)\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"âœ… Cargado {v.shape} en {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff90a",
   "metadata": {},
   "source": [
    "Paso 1 Â· Carga del CSV (solo columnas Ãºtiles + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que vamos a usar en el pipeline\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# Ver quÃ© hay realmente en el archivo y cargar sÃ³lo lo que exista\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "if missing: print(\"âš ï¸ Faltan en CSV (no pasa nada, se calcularÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"HORA_SALIDA\":\"int8\",\"MIN_SALIDA\":\"int8\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ff53d",
   "metadata": {},
   "source": [
    "Paso 2 Â· Derivar columnas que faltan (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# Distancia Haversine\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH\" in v.columns and \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto del dÃ­a de salida\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "        ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "\n",
    "# Ruta (texto)\n",
    "if \"RUTA\" not in v.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# NormalizaciÃ³n de tipos\n",
    "to_float32 = [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]\n",
    "for c in to_float32:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "for c in [\"MONTH\",\"DAY_OF_WEEK\",\"RETRASADO_LLEGADA\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"int8\")\n",
    "\n",
    "print(\"âœ… Derivadas listas | columnas:\", len(v.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09913e8",
   "metadata": {},
   "source": [
    "Paso 3 Â· Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "missing_feats = [c for c in cat_cols + num_cols if c not in v.columns]\n",
    "if missing_feats: print(\"âš ï¸ Faltaron features (se omiten):\", missing_feats)\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(\"int8\")\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303c6b9",
   "metadata": {},
   "source": [
    "Paso 4 Â· Split temporal (train 1â€“9, valid 10â€“12) + copias seguras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598da970",
   "metadata": {},
   "source": [
    "Paso 5 Â· Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 5 (FIX) Â· Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA) ===\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kfold_target_encode(x_col: pd.Series, y: pd.Series, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - x_col: Series (columna categÃ³rica) indexada igual que y\n",
    "      - y:     Series binaria (0/1) indexada igual que x_col\n",
    "    Devuelve:\n",
    "      - enc_train: Series con el encoding para cada fila del train (out-of-fold)\n",
    "      - mapping:   dict categorÃ­a -> valor TE (para aplicar en valid/test/producciÃ³n)\n",
    "      - global_mean: float con la media global del target (fallback)\n",
    "    \"\"\"\n",
    "    assert x_col.index.equals(y.index), \"x_col y y deben estar alineados por Ã­ndice\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=y.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(x_col, y):\n",
    "        # construimos df temporal con col + target para este fold\n",
    "        df_tr = pd.DataFrame({\"col\": x_col.iloc[tr_idx].astype(str), \"y\": y.iloc[tr_idx].astype(int)})\n",
    "        df_val = pd.DataFrame({\"col\": x_col.iloc[val_idx].astype(str)})\n",
    "\n",
    "        stats = df_tr.groupby(\"col\")[\"y\"].mean()\n",
    "        cnts  = df_tr.groupby(\"col\")[\"y\"].size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = df_val[\"col\"].map(smoothed).fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar fuera del train)\n",
    "    df_full = pd.DataFrame({\"col\": x_col.astype(str), \"y\": y.astype(int)})\n",
    "    stats_f = df_full.groupby(\"col\")[\"y\"].mean()\n",
    "    cnts_f  = df_full.groupby(\"col\")[\"y\"].size()\n",
    "    mapping = ((stats_f*cnts_f + global_mean*smoothing) / (cnts_f + smoothing)).astype(\"float32\").to_dict()\n",
    "\n",
    "    return enc.astype(\"float32\"), mapping, global_mean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "# --- columnas a codificar (sÃ³lo las que existan en X_train) ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "\n",
    "# Trabajamos sobre copias para evitar warnings\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    enc_train, mapping, default = kfold_target_encode(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_train\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Modelo us\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c27e0",
   "metadata": {},
   "source": [
    "Ese NameError indica que no se creÃ³ X_train_model/X_valid_model (normalmente porque la celda del Paso 5 â€“ Target Encoding no se ejecutÃ³ o fallÃ³ a mitad). Deja una celda puente antes del entrenamiento que:\n",
    "\n",
    "Verifica que corriste los pasos 3â€“5.\n",
    "\n",
    "Si hay TE, arma X_train_model/X_valid_model.\n",
    "\n",
    "Da mensajes claros si falta algo.\n",
    "\n",
    "Pega y ejecuta esto justo antes del Paso 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95170f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Puente de seguridad antes de entrenar (Paso 6) ---\n",
    "# Requiere X_train, X_valid, y_train, y_valid de los pasos 3â€“4\n",
    "assert 'X_train' in globals() and 'X_valid' in globals(), \"Falta correr el Paso 3â€“4 (definir X_train/X_valid).\"\n",
    "assert 'y_train' in globals() and 'y_valid' in globals(), \"Falta correr el Paso 3â€“4 (definir y_train/y_valid).\"\n",
    "\n",
    "# Columnas categÃ³ricas originales que se codifican con TE\n",
    "cols_te_base = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "cols_te = [c for c in cols_te_base if c in X_train.columns or f\"{c}_TE\" in X_train.columns]\n",
    "\n",
    "# Â¿existen columnas TE? (AIRLINE_TE, etc.)\n",
    "te_cols_present = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" in X_train.columns]\n",
    "if len(te_cols_present) != len(cols_te):\n",
    "    faltan = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" not in X_train.columns]\n",
    "    raise ValueError(\n",
    "        \"Faltan columnas de Target Encoding. \"\n",
    "        f\"No se ejecutÃ³ (o fallÃ³) el Paso 5.\\nFaltan: {faltan}\"\n",
    "    )\n",
    "\n",
    "# Construir matrices finales: todas las numÃ©ricas + columnas _TE; quitamos las categÃ³ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te_base if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te_base if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ… Listo para entrenar\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas ejemplo:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983568",
   "metadata": {},
   "source": [
    "Paso 6 Â· Entrenamiento LightGBM con early stopping (balanceado por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=5.0,\n",
    "    scale_pos_weight=scale_pos_weight,  # en vez de class_weight\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59d392",
   "metadata": {},
   "source": [
    "Paso 7 Â· MÃ©tricas (0.5, mejor F1) y umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_res = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (bÃºsqueda gruesa)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9520168",
   "metadata": {},
   "source": [
    "Paso 8 Â· Guardar artefactos (modelo, mappings TE, defaults y threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "# Guardamos mappings y defaults de TE\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train_model.columns),\n",
    "    \"cols_te\": cols_te,\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"scale_pos_weight\": float(scale_pos_weight),\n",
    "    \"note\": \"LightGBM + KFold Target Encoding (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)\"\n",
    "}\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ… Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0cbfe",
   "metadata": {},
   "source": [
    "Paso 9 Â· (ProducciÃ³n) FunciÃ³n de predicciÃ³n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar artefactos (en tu API/servicio)\n",
    "from joblib import load\n",
    "import json, numpy as np, math\n",
    "\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f: te_mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f: te_defaults = json.load(f)\n",
    "with open(\"models/threshold.json\",\"r\",encoding=\"utf-8\") as f: UMBRAL = json.load(f)[\"threshold\"]\n",
    "\n",
    "def prep_features_for_inference(\n",
    "    month:int, day_of_week:int,\n",
    "    airline:str, origin:str, destination:str,\n",
    "    scheduled_hour:int, scheduled_minute:int,\n",
    "    origen_lat:float, origen_lon:float, dest_lat:float, dest_lon:float\n",
    "):\n",
    "    # cÃ­clicos\n",
    "    salida_min = int(scheduled_hour)*60 + int(scheduled_minute)\n",
    "    salida_sin = math.sin(2*math.pi*salida_min/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*salida_min/(24*60))\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    # haversine\n",
    "    def haversine_km(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1 = math.radians(lat1); lon1 = math.radians(lon1)\n",
    "        lat2 = math.radians(lat2); lon2 = math.radians(lon2)\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist_hav = haversine_km(origen_lat, origen_lon, dest_lat, dest_lon)\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    # aplicar TE (map -> valor; si no existe categorÃ­a, usar default global)\n",
    "    def te(col, val):\n",
    "        mp = te_mappings[col]; default = te_defaults[col]\n",
    "        return float(mp.get(str(val), default))\n",
    "\n",
    "    row = {\n",
    "        \"MONTH\": np.int8(month),\n",
    "        \"DAY_OF_WEEK\": np.int8(day_of_week),\n",
    "        \"SALIDA_SIN\": np.float32(salida_sin),\n",
    "        \"SALIDA_COS\": np.float32(salida_cos),\n",
    "        \"MONTH_SIN\": np.float32(month_sin),\n",
    "        \"MONTH_COS\": np.float32(month_cos),\n",
    "        \"DISTANCIA_HAV\": np.float32(dist_hav),\n",
    "        \"MINUTO_DIA_SALIDA\": np.int16(salida_min),\n",
    "        # TE\n",
    "        \"AIRLINE_TE\": np.float32(te(\"AIRLINE\", airline)),\n",
    "        \"ORIGIN_AIRPORT_TE\": np.float32(te(\"ORIGIN_AIRPORT\", origin)),\n",
    "        \"DESTINATION_AIRPORT_TE\": np.float32(te(\"DESTINATION_AIRPORT\", destination)),\n",
    "        \"RUTA_TE\": np.float32(te(\"RUTA\", ruta)),\n",
    "    }\n",
    "\n",
    "    # El orden de columnas debe coincidir con X_train_model.columns\n",
    "    cols = list(c for c in model.feature_name_)\n",
    "    Xrow = np.array([[row[c] for c in cols]], dtype=np.float32)\n",
    "    return Xrow\n",
    "\n",
    "def predict_delay_prob(\n",
    "    month, day_of_week, airline, origin, destination,\n",
    "    scheduled_hour, scheduled_minute,\n",
    "    origen_lat, origen_lon, dest_lat, dest_lon\n",
    "):\n",
    "    Xrow = prep_features_for_inference(\n",
    "        month, day_of_week, airline, origin, destination,\n",
    "        scheduled_hour, scheduled_minute, origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    )\n",
    "    proba = float(model.predict_proba(Xrow)[0,1])\n",
    "    delayed = proba >= UMBRAL\n",
    "    return {\"prob_delay\": round(proba,4), \"delayed\": bool(delayed), \"threshold\": UMBRAL}\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# predict_delay_prob(7, 4, \"DL\", \"ATL\", \"JFK\", 14, 30, 33.64, -84.43, 40.64, -73.78)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b547da0",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bb55d",
   "metadata": {},
   "source": [
    "0. Encabezado y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75909967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0 Â· Imports, ruta y helpers\n",
    "import os, time, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb26ea",
   "metadata": {},
   "source": [
    "1. Carga del CSV (eficiente: usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1 Â· Carga del CSV (solo columnas Ãºtiles + dtypes compactos)\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")  # ajusta si lo tienes en otra ruta\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# leer solo el header para ver quÃ© hay\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"Columnas cargadas:\", present, \"\\nFaltantes (se derivan si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ… Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae384338",
   "metadata": {},
   "source": [
    "2. Derivar columnas que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2 Â· DerivaciÃ³n de features faltantes\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# Minuto del dÃ­a (si no vino ya)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# Ruta (texto)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"âœ… Derivadas OK | columnas:\", len(v.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b124f86",
   "metadata": {},
   "source": [
    "3. Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb86f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3 Â· SelecciÃ³n de variables\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3b6a",
   "metadata": {},
   "source": [
    "4. Split temporal (train: meses 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4 Â· Split temporal (evita fuga)\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dd98a",
   "metadata": {},
   "source": [
    "5. Target Encoding KFold (sin fuga) para categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Target Encoding KFold ROBUSTO (sin fuga) + armado de matrices\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) PRERREQUISITOS / ALINEACIONES ---\n",
    "# Deben existir de pasos previos:\n",
    "#   v, X, y, X_train, X_valid, y_train, y_valid\n",
    "for obj_name in [\"X_train\", \"X_valid\", \"y_train\", \"y_valid\"]:\n",
    "    assert obj_name in globals(), f\"Falta {obj_name}. Ejecuta los pasos previos.\"\n",
    "\n",
    "# Asegurar que y_train/y_valid estÃ¡n alineados con X_train/X_valid\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_valid = y_valid.loc[X_valid.index]\n",
    "\n",
    "# Crear RUTA si no existe\n",
    "if \"RUTA\" not in X_train.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(X_train.columns):\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    X_train[\"RUTA\"] = (X_train[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_train[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    X_valid[\"RUTA\"] = (X_valid[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_valid[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Columnas a codificar (solo si existen)\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "print(\"TE sobre:\", cols_te)\n",
    "\n",
    "# --- 1) Funciones TE robustas (operan con Series) ---\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42) -> tuple[pd.Series, dict, float]:\n",
    "    \"\"\"\n",
    "    s: Serie categÃ³rica (mismo Ã­ndice que y)\n",
    "    y: Serie binaria 0/1 (mismo Ã­ndice que s)\n",
    "    Devuelve:\n",
    "      enc     -> Serie con el encoding KFold para s (alineada a s.index)\n",
    "      mapping -> dict valor_categoria -> encoding_final (con TODO el train)\n",
    "      gmean   -> media global (fallback)\n",
    "    \"\"\"\n",
    "    # AlineaciÃ³n defensiva por Ã­ndice\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx]\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    # NormalizaciÃ³n de tipos\n",
    "    s = s.astype(\"string\")  # evita NaNs tipo objeto raros\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    # Mapping final con TODO el train (para producciÃ³n/valid)\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float) -> pd.Series:\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "# --- 2) Ejecutar TE ---\n",
    "mappings, defaults = {}, {}\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid[f\"{c}_TE\"] = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "print(\"âœ… TE aplicado sin fuga.\")\n",
    "print(\"Ejemplo TE:\", {k: list(v)[:2] if hasattr(v, \"__iter__\") else v for k,v in list(mappings.items())[:1]})\n",
    "\n",
    "# --- 3) Construir matrices finales: quitamos las categorÃ­as crudas ---\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"Listo para entrenar:\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas (primeras 12):\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3435a",
   "metadata": {},
   "source": [
    "6. Entrenamiento LightGBM (early stopping, balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Chequeos rÃ¡pidos\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "print(\"Shapes:\", X_train_model.shape, X_valid_model.shape)\n",
    "\n",
    "# Balanceo por proporciÃ³n de clases (pos/neg)\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    # usa uno u otro balanceo (recomiendo este):\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5051f6",
   "metadata": {},
   "source": [
    "7) MÃ©tricas base (0.5) + bÃºsqueda de mejor umbral (por F1) y matriz de confusiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (bÃºsqueda simple)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627403",
   "metadata": {},
   "source": [
    "### revisiÃ³n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf6512",
   "metadata": {},
   "source": [
    "agregados histÃ³ricos + reentrenamiento con LGBM y comparaciÃ³n con el modelo anterior\n",
    "\n",
    "agregar seÃ±ales histÃ³ricas sin fuga (congestiÃ³n por ruta y hora), re-entrenar LightGBM y comparar contra tu baseline. \n",
    "\n",
    "EstÃ¡ pensado para ejecutarlo despuÃ©s de tu pipeline anterior (ya se debe tener v, X_train, X_valid, y_train, y_valid, X_train_model, X_valid_model listos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee88653",
   "metadata": {},
   "source": [
    "1) Agregados histÃ³ricos (solo con TRAIN 1â€“9) â†’ sin fuga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 1: Carga eficiente con tus nombres + fallbacks seguros ===\n",
    "import time, numpy as np, pandas as pd\n",
    "\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "\n",
    "# Fallbacks de carga (para poder derivar luego):\n",
    "# 1) Si no viene RETRASADO_LLEGADA, intenta traer ARRIVAL_DELAY para crearlo.\n",
    "if \"RETRASADO_LLEGADA\" not in present and \"ARRIVAL_DELAY\" in header:\n",
    "    present.append(\"ARRIVAL_DELAY\")\n",
    "\n",
    "# 2) Si no vienen SALIDA_SIN/COS, con SCHEDULED_DEPARTURE podemos derivarlas (ya estÃ¡ en present).\n",
    "# 3) Si faltan lat/lon y existen equivalentes en tu archivo, aÃ±ade aquÃ­ sus alias si aplica.\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"ARRIVAL_DELAY\":\"float32\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(\"â†’ Cargado:\", v.shape, \"| en\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# === Derivaciones mÃ­nimas para completar faltantes ===\n",
    "\n",
    "# Target desde ARRIVAL_DELAY si no vino RETRASADO_LLEGADA\n",
    "if \"RETRASADO_LLEGADA\" not in v.columns:\n",
    "    if \"ARRIVAL_DELAY\" not in v.columns:\n",
    "        raise ValueError(\"No hay RETRASADO_LLEGADA ni ARRIVAL_DELAY para derivarlo.\")\n",
    "    v[\"RETRASADO_LLEGADA\"] = (v[\"ARRIVAL_DELAY\"] > 15).astype(\"int8\")\n",
    "\n",
    "# SALIDA_SIN/COS desde SCHEDULED_DEPARTURE si faltan\n",
    "if (\"SALIDA_SIN\" not in v.columns or \"SALIDA_COS\" not in v.columns) and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    minuto = (hs*60 + ms).astype(\"int16\")\n",
    "    v[\"SALIDA_SIN\"] = np.sin(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "    v[\"SALIDA_COS\"] = np.cos(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "\n",
    "# Distancia Haversine (opcional) si mÃ¡s adelante la quieres usar y tienes lat/lon\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(v[\"ORIGEN_LAT\"]); lon1 = np.radians(v[\"ORIGEN_LON\"])\n",
    "    lat2 = np.radians(v[\"DEST_LAT\"]);   lon2 = np.radians(v[\"DEST_LON\"])\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    v[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes (opcional)\n",
    "if \"MONTH\" in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# RUTA (Ãºtil para Target Encoding)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(f\"âœ… Listo: rate={v['RETRASADO_LLEGADA'].mean():.4f} | cols={len(v.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e5160",
   "metadata": {},
   "source": [
    "Paso 2 Â· Split temporal (train=meses 1â€“9, valid=10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el DataFrame v cargado en el Paso 1\n",
    "assert \"MONTH\" in v.columns and \"RETRASADO_LLEGADA\" in v.columns, \"Faltan MONTH o RETRASADO_LLEGADA.\"\n",
    "\n",
    "# Variables base (ajusta si quieres usar mÃ¡s adelante otras)\n",
    "# Si ya creaste RUTA en el Paso 1, se usarÃ¡; si no existe, la generamos aquÃ­.\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in v.columns]\n",
    "num_cols = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "                        \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"] if c in v.columns]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’\",\n",
    "      \"X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b3ff8",
   "metadata": {},
   "source": [
    "Paso 3 Â· Target Encoding KFold (sin fuga) para categÃ³ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1dfa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42):\n",
    "    # AlineaciÃ³n defensiva\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx].astype(\"string\")\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = cat_cols[:]  # todas las categÃ³ricas disponibles\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "# Copias para no tocar los originales\n",
    "X_train_te = X_train.copy()\n",
    "X_valid_te = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train_te[c], y_train,\n",
    "                                                          n_splits=5, smoothing=50, seed=42)\n",
    "    X_train_te[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid_te[f\"{c}_TE\"]  = apply_te(X_valid_te[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Matrices finales de entrenamiento (quitamos las columnas categÃ³ricas crudas)\n",
    "X_train_model = X_train_te.drop(columns=cols_te).copy()\n",
    "X_valid_model = X_valid_te.drop(columns=cols_te).copy()\n",
    "\n",
    "print(\"âœ… TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7828e",
   "metadata": {},
   "source": [
    "(Opcional fuerte) Paso 4 Â· Agregados histÃ³ricos RUTAÃ—HORA (sin fuga)\n",
    "\n",
    "Si quieres mejorar precisiÃ³n/recall, agrega estadÃ­sticos histÃ³ricos usando solo train (meses 1â€“9) y mapea a train/valid. Si no lo necesitas ahora, salta al paso 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "assert \"SCHEDULED_DEPARTURE\" in v.columns, \"Requiere SCHEDULED_DEPARTURE para obtener HORA_SALIDA.\"\n",
    "HORA_all = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "RUTA_all = v[\"RUTA\"].astype(str) if \"RUTA\" in v.columns else (\n",
    "    v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    ")\n",
    "\n",
    "train_mask_all = v[\"MONTH\"].between(1, 9)\n",
    "\n",
    "aux = pd.DataFrame({\n",
    "    \"RUTA\": RUTA_all[train_mask_all].values,\n",
    "    \"HORA_SALIDA\": HORA_all[train_mask_all].values,\n",
    "    \"RETRASADO_LLEGADA\": v.loc[train_mask_all, \"RETRASADO_LLEGADA\"].values\n",
    "})\n",
    "# Si existe ARRIVAL_DELAY, incluimos media de retraso\n",
    "if \"ARRIVAL_DELAY\" in v.columns:\n",
    "    aux[\"ARRIVAL_DELAY\"] = v.loc[train_mask_all, \"ARRIVAL_DELAY\"].values\n",
    "\n",
    "agg_dict = {\"rate_delay\": (\"RETRASADO_LLEGADA\",\"mean\"),\n",
    "            \"n\": (\"RETRASADO_LLEGADA\",\"size\")}\n",
    "if \"ARRIVAL_DELAY\" in aux.columns:\n",
    "    agg_dict[\"mean_arr_delay\"] = (\"ARRIVAL_DELAY\",\"mean\")\n",
    "\n",
    "agg_ruta_hora = (aux\n",
    "    .groupby([\"RUTA\",\"HORA_SALIDA\"], observed=True)\n",
    "    .agg(**agg_dict)\n",
    "    .reset_index())\n",
    "\n",
    "g_rate = float(agg_ruta_hora[\"rate_delay\"].mean())\n",
    "g_n    = 0.0\n",
    "g_mean_arr = float(agg_ruta_hora[\"mean_arr_delay\"].mean()) if \"mean_arr_delay\" in agg_ruta_hora.columns else 0.0\n",
    "\n",
    "def add_route_hour_stats(X_in, idx):\n",
    "    tmp = pd.DataFrame({\n",
    "        \"RUTA\": RUTA_all.loc[idx].astype(str).values,\n",
    "        \"HORA_SALIDA\": HORA_all.loc[idx].astype(\"int16\").values\n",
    "    }, index=idx)\n",
    "    merged = tmp.merge(agg_ruta_hora, on=[\"RUTA\",\"HORA_SALIDA\"], how=\"left\")\n",
    "    X_in[\"RUTA_HORA_rate\"] = merged[\"rate_delay\"].fillna(g_rate).astype(\"float32\").values\n",
    "    X_in[\"RUTA_HORA_n\"]    = merged[\"n\"].fillna(g_n).astype(\"float32\").values\n",
    "    if \"mean_arr_delay\" in merged.columns:\n",
    "        X_in[\"RUTA_HORA_mean_arr\"] = merged[\"mean_arr_delay\"].fillna(g_mean_arr).astype(\"float32\").values\n",
    "    return X_in\n",
    "\n",
    "X_train_model = add_route_hour_stats(X_train_model.copy(), X_train_model.index)\n",
    "X_valid_model = add_route_hour_stats(X_valid_model.copy(), X_valid_model.index)\n",
    "\n",
    "print(\"âœ” Agregados aÃ±adidos:\",\n",
    "      [c for c in X_train_model.columns if c.startswith(\"RUTA_HORA_\")],\n",
    "      \"| tiempo:\", f\"{time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2668c",
   "metadata": {},
   "source": [
    "Paso 5 Â· Entrenamiento con LightGBM (early stopping + balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd192744",
   "metadata": {},
   "source": [
    "Paso 6 Â· MÃ©tricas y selecciÃ³n de umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (puedes restringir por precisiÃ³n mÃ­nima si tu negocio lo requiere)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], f\"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c1e68",
   "metadata": {},
   "source": [
    "Paso 7 Â· Guardado de artefactos (modelo + TE + columnas + umbral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Umbral operativo (elige el que te convenga: best[\"thr\"] o un umbral de negocio)\n",
    "UMBRAL_OPERATIVO = float(best[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:{str(kk):float(vv) for kk,vv in v.items()} for k,v in mappings.items()}, f)\n",
    "with open(\"models/te_defaults.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:float(v) for k,v in defaults.items()}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cat_encoded=cols_te,\n",
    "    split=\"temporal: train 1-9, valid 10-12\",\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    auc_valid=float(auc_val),\n",
    "    threshold=float(UMBRAL_OPERATIVO),\n",
    "    notes=\"LightGBM con TE KFold (sin fuga)\" + (\" + agregados RUTAÃ—HORA\" if any(c.startswith(\"RUTA_HORA_\") for c in X_train_model.columns) else \"\")\n",
    ")\n",
    "with open(\"models/metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Guardado: models/lgbm_delay.joblib, te_mappings.json, te_defaults.json, metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91db1cb",
   "metadata": {},
   "source": [
    "Paso 8 Â· FunciÃ³n de inferencia (para API / dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from joblib import load\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    defaults = json.load(f)\n",
    "with open(\"models/metadata.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "FEATURES = meta[\"features\"]\n",
    "THRESHOLD = float(meta[\"threshold\"])\n",
    "\n",
    "def infer_delay(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"df_input: columnas mÃ­nimas: AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, MONTH, DAY_OF_WEEK, SCHEDULED_DEPARTURE (+ lat/lon si quieres DISTANCIA_HAV).\n",
    "       Devuelve: proba_delay y pred (0/1) usando THRESHOLD guardado.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in df.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(df.columns):\n",
    "        df[\"RUTA\"] = df[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + df[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # Derivar MINUTO_DIA_SALIDA / SALIDA_SIN/COS si hace falta\n",
    "    if \"MINUTO_DIA_SALIDA\" not in df.columns and \"SCHEDULED_DEPARTURE\" in df.columns:\n",
    "        hs = (df[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "        ms = (df[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "        df[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "    if \"SALIDA_SIN\" not in df.columns and \"MINUTO_DIA_SALIDA\" in df.columns:\n",
    "        m = df[\"MINUTO_DIA_SALIDA\"].astype(\"float32\")\n",
    "        df[\"SALIDA_SIN\"] = np.sin(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "        df[\"SALIDA_COS\"] = np.cos(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN/COS\n",
    "    if \"MONTH\" in df.columns:\n",
    "        df[\"MONTH_SIN\"] = np.sin(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "        df[\"MONTH_COS\"] = np.cos(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    # (Opcional) DISTANCIA_HAV si hay lat/lon\n",
    "    if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(df.columns) and \"DISTANCIA_HAV\" not in df.columns:\n",
    "        R = 6371.0\n",
    "        lat1 = np.radians(df[\"ORIGEN_LAT\"]); lon1 = np.radians(df[\"ORIGEN_LON\"])\n",
    "        lat2 = np.radians(df[\"DEST_LAT\"]);   lon2 = np.radians(df[\"DEST_LON\"])\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "        df[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "    # Target Encoding con mappings guardados\n",
    "    for c, mapping in mappings.items():\n",
    "        default = float(defaults.get(c, df_input.get(c, pd.Series([],dtype='object')).mean() if c in df_input.columns else 0.0))\n",
    "        df[f\"{c}_TE\"] = df[c].astype(\"string\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "    # Construir matriz final con las FEATURES del entrenamiento\n",
    "    X_inf = df.reindex(columns=FEATURES, fill_value=0).copy()\n",
    "\n",
    "    proba = model.predict_proba(X_inf)[:,1]\n",
    "    pred  = (proba >= THRESHOLD).astype(int)\n",
    "    out = df_input.copy()\n",
    "    out[\"proba_delay\"] = proba\n",
    "    out[\"pred_delay\"]  = pred\n",
    "    return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# infer_delay(pd.DataFrame([{\n",
    "#   \"AIRLINE\":\"AA\",\"ORIGIN_AIRPORT\":\"JFK\",\"DESTINATION_AIRPORT\":\"LAX\",\"MONTH\":7,\"DAY_OF_WEEK\":5,\"SCHEDULED_DEPARTURE\": 1730\n",
    "# }]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c585",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b79497",
   "metadata": {},
   "source": [
    "plit temporal (1â€“9 vs 10â€“12), Target Encoding KFold sin fuga, agregados histÃ³ricos sin fuga, entrena LightGBM con early stopping y guarda artefactos (modelo, mapeos TE y threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c42b74",
   "metadata": {},
   "source": [
    "Paso 0 Â· Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 0: imports y ruta =====\n",
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# Ajusta a tu ruta:\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1911550",
   "metadata": {},
   "source": [
    "Paso 1 Â· Carga del CSV (usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0776802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Columnas presentes: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA'] \n",
      "â†’ Faltantes (se derivarÃ¡n si aplica): []\n",
      "âœ“ Cargado: (5231130, 14) | en 24.1s\n",
      "âœ“ Rate retraso: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 1: carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"â†’ Columnas presentes:\", present, \"\\nâ†’ Faltantes (se derivarÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(f\"âœ“ Cargado: {v.shape} | en {time.time()-t0:.1f}s\")\n",
    "print(\"âœ“ Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3546d",
   "metadata": {},
   "source": [
    "Paso 2 Â· Derivar features que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO/HORA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa4560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 2: features derivados =====\n",
    "\n",
    "# Haversine (km)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto y hora de salida (si no existen)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23).astype(\"int16\")\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59).astype(\"int16\")\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"]//60).astype(\"int16\")\n",
    "\n",
    "# Ruta\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Cast finales (por si cargaron como object)\n",
    "for c in [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "\n",
    "print(\"âœ“ Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340eed",
   "metadata": {},
   "source": [
    "Paso 3 Â· Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaf7e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 3: selecciÃ³n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "            \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"]\n",
    "\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54d84f",
   "metadata": {},
   "source": [
    "Paso 4 Â· Split temporal (train: 1â€“9, valid: 10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2ae477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split â†’ X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 4: split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’ X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac9816",
   "metadata": {},
   "source": [
    "Paso 5 Â· Target Encoding KFold (sin fuga) para categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be913863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TE aplicado (fix categorÃ­as) | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 5 (FIX): Target Encoding KFold (sin fuga) robusto para columnas categÃ³ricas =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - Agrupa SIEMPRE usando la columna como object (evita comportamiento de 'category').\n",
    "      - Mapea con dict (no Series) para asegurarnos de obtener floats.\n",
    "    Devuelve:\n",
    "      enc (Series float32, aligned con train_df.index),\n",
    "      mapping (dict {categoria: valor_TE}),\n",
    "      global_mean (float)\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=\"float32\")\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        # claves como object para evitar dtype 'category'\n",
    "        keys_tr = train_df.iloc[tr_idx][col].astype(\"object\")\n",
    "        stats = y.iloc[tr_idx].groupby(keys_tr).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(keys_tr).size()\n",
    "        smoothed = ((stats*cnts + global_mean*smoothing) / (cnts + smoothing)).astype(\"float32\")\n",
    "        smoothed_dict = smoothed.to_dict()\n",
    "\n",
    "        keys_val = train_df.iloc[val_idx][col].astype(\"object\")\n",
    "        mapped = keys_val.map(smoothed_dict).astype(\"float32\")\n",
    "        enc.iloc[val_idx] = mapped.fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar en valid/test/producciÃ³n)\n",
    "    keys_all = train_df[col].astype(\"object\")\n",
    "    full_stats = y.groupby(keys_all).mean()\n",
    "    full_cnts  = y.groupby(keys_all).size()\n",
    "    final = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).astype(\"float32\")\n",
    "    mapping = final.to_dict()\n",
    "\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    # asegurar object antes de mapear, retornando float32\n",
    "    return series.astype(\"object\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "\n",
    "# --- Aplica TE en las columnas categÃ³ricas seleccionadas ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    # aÃ±ade columna _TE\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr.values.astype(\"float32\")\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = float(default)\n",
    "\n",
    "# matrices finales: dejamos numÃ©ricas + _TE (quitamos las categorÃ­as crudas)\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ“ TE aplicado (fix categorÃ­as) | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eca9a7",
   "metadata": {},
   "source": [
    "Paso 6 Â· Agregados histÃ³ricos (sin fuga) y uniÃ³n a matrices del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d93489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agregados aplicados en 9.3s\n",
      "âœ“ Columnas nuevas: ['AIR_n', 'AIR_rate', 'DES_n', 'DES_rate', 'ORI_n', 'ORI_rate', 'RUTA_HORA_n', 'RUTA_HORA_rate', 'RUTA_n', 'RUTA_rate'] \n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 6 Â· Agregados histÃ³ricos (sin fuga) â€” versiÃ³n robusta =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# 0) Asegurar llaves en 'v' (DataFrame completo)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "        v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "    else:\n",
    "        raise KeyError(\"No puedo crear RUTA: faltan ORIGIN_AIRPORT/DESTINATION_AIRPORT en 'v'.\")\n",
    "\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23).astype(\"int8\")\n",
    "    else:\n",
    "        raise KeyError(\"No puedo derivar HORA_SALIDA: falta MINUTO_DIA_SALIDA o SCHEDULED_DEPARTURE.\")\n",
    "\n",
    "# 1) MÃ¡scaras temporales (sin fuga)\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask]\n",
    "global_mean = float(v_train[target].mean())\n",
    "\n",
    "# 2) Inyectar las llaves requeridas en X_train / X_valid por Ã­ndice (asegura disponibilidad)\n",
    "needed_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "for k in needed_keys:\n",
    "    if k not in X_train.columns:\n",
    "        X_train[k] = v.loc[train_mask, k].values\n",
    "    if k not in X_valid.columns:\n",
    "        X_valid[k] = v.loc[valid_mask, k].values\n",
    "\n",
    "# 3) Definir specs de agregados y filtrar dinÃ¡micamente a las claves existentes\n",
    "candidate_aggs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "]\n",
    "available_in_vtrain = set(v_train.columns)\n",
    "aggs_specs = [(keys, pref) for keys, pref in candidate_aggs if set(keys).issubset(available_in_vtrain)]\n",
    "if not aggs_specs:\n",
    "    raise RuntimeError(\"No hay combinaciones de agregados vÃ¡lidas; revisa que existan las columnas clave.\")\n",
    "\n",
    "# 4) Funciones de agregaciÃ³n con suavizado y join\n",
    "def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"AGG\", smooth=20):\n",
    "    g = df.groupby(keys, observed=True)[target].agg([\"mean\",\"size\"]).reset_index()\n",
    "    g.rename(columns={\"mean\":f\"{pref}_rate\",\"size\":f\"{pref}_n\"}, inplace=True)\n",
    "    # suavizado para grupos pequeÃ±os\n",
    "    g[f\"{pref}_rate\"] = ((g[f\"{pref}_rate\"]*g[f\"{pref}_n\"] + global_mean*smooth) / (g[f\"{pref}_n\"] + smooth)).astype(\"float32\")\n",
    "    g[f\"{pref}_n\"] = g[f\"{pref}_n\"].astype(\"int32\")\n",
    "    return g\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "    merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "    merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "    return merged\n",
    "\n",
    "# 5) Construir matrices de trabajo partiendo de las del modelo (si existen) o de X_* crudas\n",
    "Xt = X_train_model.copy() if \"X_train_model\" in globals() else X_train.copy()\n",
    "Xv = X_valid_model.copy() if \"X_valid_model\" in globals() else X_valid.copy()\n",
    "\n",
    "# 6) Asegurar que las llaves estÃ©n presentes en Xt/Xv para poder hacer merge por columnas (y no por Ã­ndice)\n",
    "for keys, _ in aggs_specs:\n",
    "    for k in keys:\n",
    "        if k not in Xt.columns:\n",
    "            Xt[k] = X_train[k].values\n",
    "        if k not in Xv.columns:\n",
    "            Xv[k] = X_valid[k].values\n",
    "\n",
    "# 7) Aplicar agregados\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "print(f\"âœ“ Agregados aplicados en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# 8) (Opcional) eliminar las llaves aÃ±adidas si no estaban antes en la matriz del modelo\n",
    "if \"X_train_model\" in globals():\n",
    "    # deja solo columnas originales del modelo + nuevas *_rate/_n\n",
    "    keep_cols = set(X_train_model.columns) | {c for c in Xt.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")}\n",
    "    drop_cols = [c for c in Xt.columns if c not in keep_cols]\n",
    "    Xt = Xt.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    Xv = Xv.drop(columns=[c for c in Xv.columns if c not in (set(X_valid_model.columns) | keep_cols)], errors=\"ignore\")\n",
    "\n",
    "# 9) Actualizar matrices del modelo\n",
    "X_train_model = Xt.copy()\n",
    "X_valid_model = Xv.copy()\n",
    "\n",
    "new_cols = sorted([c for c in X_train_model.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")])\n",
    "print(\"âœ“ Columnas nuevas:\", new_cols[:10], \"...\" if len(new_cols) > 10 else \"\")\n",
    "print(\"Shapes ->\", X_train_model.shape, X_valid_model.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ea05c",
   "metadata": {},
   "source": [
    "Paso 7 Â· Entrenamiento LightGBM con early stopping (balanceo por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d12841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.304088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's auc: 0.599939\tvalid_0's binary_logloss: 0.569545\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.596086\tvalid_0's binary_logloss: 0.459714\n",
      "âœ“ Entrenado en 563.2s | best_iter=3\n",
      "ROC-AUC valid=0.5961\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 7: entrenamiento LightGBM =====\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,  # balanceo\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "print(f\"âœ“ Entrenado en {time.time()-t0:.1f}s | best_iter={model.best_iteration_}\")\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e322e",
   "metadata": {},
   "source": [
    "Paso 8 Â· MÃ©tricas a 0.5 y bÃºsqueda de mejor F1 (umbral operativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c225b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1=0.0000 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.192) ==\n",
      "Accuracy: 0.3391 | Precision: 0.1908 | Recall: 0.8728 | F1=0.3131 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[175601 595594]\n",
      " [ 20466 140423]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3390509868209303,\n",
       " 'pre': 0.190787712783808,\n",
       " 'rec': 0.8727942867442771,\n",
       " 'f1': 0.31312757412705455,\n",
       " 'thr': 0.19210526315789472}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Paso 8: mÃ©tricas =====\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc_val:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, thr=thr)\n",
    "\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best = {\"thr\":0.0, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 20):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b443d",
   "metadata": {},
   "source": [
    "Paso 9 Â· Guardar artefactos (modelo, mappings TE, defaults, threshold y metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49d2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Guardados: modelo + TE mappings/defaults + threshold + metadata.\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 9: guardar artefactos =====\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cols_te=cols_te,\n",
    "    auc_valid=float(auc_val),\n",
    "    split={\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    note=\"LightGBM + KFold Target Encoding + agregados histÃ³ricos (sin fuga)\"\n",
    ")\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815d8f",
   "metadata": {},
   "source": [
    "(Opcional) Paso 10 Â· FunciÃ³n de predicciÃ³n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a688b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 10: funciÃ³n de predicciÃ³n (producciÃ³n) =====\n",
    "import math\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"r\", encoding=\"utf-8\") as f: te_map = json.load(f)\n",
    "with open(\"models/te_defaults.json\", \"r\", encoding=\"utf-8\") as f: te_def = json.load(f)\n",
    "with open(\"models/threshold.json\", \"r\", encoding=\"utf-8\") as f: UMBRAL_OPERATIVO = json.load(f)[\"threshold\"]\n",
    "with open(\"models/metadata.json\", \"r\", encoding=\"utf-8\") as f: meta = json.load(f)\n",
    "feat_order = meta[\"features\"]\n",
    "\n",
    "def prep_features(sample):\n",
    "    \"\"\"\n",
    "    sample: dict con al menos\n",
    "      month, day_of_week, airline, origin, destination, scheduled_hour, scheduled_minute,\n",
    "      origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    Devuelve: DataFrame(1, features) listo para model.predict_proba\n",
    "    \"\"\"\n",
    "    month = int(sample[\"month\"])\n",
    "    dow   = int(sample[\"day_of_week\"])\n",
    "    airline = str(sample[\"airline\"])\n",
    "    origin  = str(sample[\"origin\"])\n",
    "    dest    = str(sample[\"destination\"])\n",
    "    sh      = int(sample[\"scheduled_hour\"])\n",
    "    sm      = int(sample[\"scheduled_minute\"])\n",
    "    olat, olon = float(sample[\"origen_lat\"]), float(sample[\"origen_lon\"])\n",
    "    dlat, dlon = float(sample[\"dest_lat\"]), float(sample[\"dest_lon\"])\n",
    "\n",
    "    # cÃ­clicos\n",
    "    salida_sin = math.sin(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    salida_cos = math.cos(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    month_sin  = math.sin(2*math.pi * (month/12))\n",
    "    month_cos  = math.cos(2*math.pi * (month/12))\n",
    "\n",
    "    # distancia\n",
    "    def hv(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1,lon1,lat2,lon2])\n",
    "        dlat = lat2-lat1; dlon = lon2-lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist = hv(olat,olon,dlat,dlon)\n",
    "\n",
    "    ruta = f\"{origin}_{dest}\"\n",
    "\n",
    "    row = dict(\n",
    "        MONTH=month, DAY_OF_WEEK=dow,\n",
    "        SALIDA_SIN=salida_sin, SALIDA_COS=salida_cos,\n",
    "        MONTH_SIN=month_sin, MONTH_COS=month_cos,\n",
    "        DISTANCIA_HAV=dist, MINUTO_DIA_SALIDA=sh*60+sm,\n",
    "        HORA_SALIDA=sh\n",
    "    )\n",
    "\n",
    "    # TE para categorÃ­as\n",
    "    for col, val in dict(AIRLINE=airline, ORIGIN_AIRPORT=origin, DESTINATION_AIRPORT=dest, RUTA=ruta).items():\n",
    "        if f\"{col}_TE\" in feat_order:\n",
    "            mapping = te_map.get(col, {})\n",
    "            default = te_def.get(col, float(np.mean(list(mapping.values()) or [0.18])))\n",
    "            row[f\"{col}_TE\"] = float(mapping.get(str(val), default))\n",
    "\n",
    "    X = pd.DataFrame([row], columns=feat_order).fillna(0)\n",
    "    return X\n",
    "\n",
    "def predecir(sample):\n",
    "    X1 = prep_features(sample)\n",
    "    proba = float(model.predict_proba(X1)[:,1])\n",
    "    pred  = int(proba >= UMBRAL_OPERATIVO)\n",
    "    return dict(prob=proba, delayed=pred, thr=UMBRAL_OPERATIVO)\n",
    "\n",
    "# Ejemplo:\n",
    "# predecir(dict(\n",
    "#   month=11, day_of_week=5, airline=\"AA\", origin=\"JFK\", destination=\"MIA\",\n",
    "#   scheduled_hour=14, scheduled_minute=30, origen_lat=40.6413, origen_lon=-73.7781,\n",
    "#   dest_lat=25.7959, dest_lon=-80.2870\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4adc5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral para PrecisiÃ³n â‰¥ 0.3: 0.229 | PrecisiÃ³n=0.308 | Recall=0.017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba)\n",
    "# encuentra el primer umbral con precisiÃ³n >= 0.30\n",
    "target_prec = 0.30\n",
    "idx = np.where(prec >= target_prec)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_op = thr[i-1] if i>0 else 0.99\n",
    "    print(f\"Umbral para PrecisiÃ³n â‰¥ {target_prec}: {thr_op:.3f} | PrecisiÃ³n={prec[i]:.3f} | Recall={rec[i]:.3f}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisiÃ³n en el set actual.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e89ef4",
   "metadata": {},
   "source": [
    "calcular el umbral a una precisiÃ³n objetivo (o recall objetivo),\n",
    "\n",
    "y un pequeÃ±o grid de LightGBM listo para pegar y correr con early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a837143",
   "metadata": {},
   "source": [
    "1) Elegir umbral operativo (por precisiÃ³n objetivo o por recall objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddda93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Umbral por PrecisiÃ³n â‰¥ 0.3 (thr=0.229) ==\n",
      "Accuracy: 0.8238 | Precision: 0.3080 | Recall: 0.0166 | F1: 0.0315 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[765203   5992]\n",
      " [158222   2667]]\n",
      "\n",
      "== Umbral por Recall â‰¥ 0.7 (thr=0.200) ==\n",
      "Accuracy: 0.4825 | Precision: 0.2062 | Recall: 0.7010 | F1: 0.3186 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[336982 434213]\n",
      " [ 48113 112776]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Usa las probabilidades del modelo ya entrenado\n",
    "y_proba_valid = model.predict_proba(X_valid_model)[:, 1]\n",
    "\n",
    "def report_at_threshold(y_true, y_proba, thr, title=\"\"):\n",
    "    y_hat = (y_proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# --- a) Umbral para alcanzar una precisiÃ³n objetivo (ej: 0.30) ---\n",
    "target_precision = 0.30\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba_valid)\n",
    "# thr tiene len = len(prec)-1 => alinear Ã­ndices\n",
    "idx = np.where(prec[:-1] >= target_precision)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_prec = float(thr[i])\n",
    "    res_prec = report_at_threshold(y_valid, y_proba_valid, thr_prec, f\"Umbral por PrecisiÃ³n â‰¥ {target_precision}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisiÃ³n en el set de validaciÃ³n.\")\n",
    "\n",
    "# --- b) Umbral para alcanzar un recall objetivo (ej: 0.70) ---\n",
    "target_recall = 0.70\n",
    "# Buscar umbral mÃ­nimo que consiga ese recall (prioriza mayor umbral para bajar falsos positivos)\n",
    "idx = np.where(rec[:-1] >= target_recall)[0]\n",
    "if len(idx):\n",
    "    i = idx[-1]  # el mÃ¡s alto que aÃºn cumple el recall objetivo\n",
    "    thr_rec = float(thr[i])\n",
    "    res_rec = report_at_threshold(y_valid, y_proba_valid, thr_rec, f\"Umbral por Recall â‰¥ {target_recall}\")\n",
    "else:\n",
    "    print(\"No se alcanza ese recall en el set de validaciÃ³n.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5fc01",
   "metadata": {},
   "source": [
    "2) Mini-grid de LightGBM con early stopping (rÃ¡pido y efectivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c766e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.670011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.535804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.4 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.536024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=3.2 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.9 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.446640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.369460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.7 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.485948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.467874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.1 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.452842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602254\tvalid_0's binary_logloss: 0.581103\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.424657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m     params[k] = v\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProbando:\u001b[39m\u001b[33m\"\u001b[39m, {k:params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys})\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m auc, secs, mdl = \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAUC valid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | tiempo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecs/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min | best_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmdl.best_iteration_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > best[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_eval\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     32\u001b[39m model = lgb.LGBMClassifier(**params)\n\u001b[32m     33\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m secs = time.time()-t0\n\u001b[32m     41\u001b[39m proba = model.predict_proba(X_valid_model)[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time, itertools\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Clase balanceada por ratio neg/pos (ya lo usaste antes)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "base = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,          # se explora mÃ¡s abajo tambiÃ©n 0.03\n",
    "    n_estimators=12000,          # early_stopping lo frenarÃ¡ antes si no mejora\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,               # bagging_fraction\n",
    "    colsample_bytree=0.85,       # feature_fraction\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"learning_rate\": [0.05, 0.03],\n",
    "    \"num_leaves\":    [63, 127],\n",
    "    \"min_child_samples\": [100, 300],\n",
    "    \"colsample_bytree\":  [0.8, 0.9],\n",
    "    \"subsample\":         [0.8, 0.9],\n",
    "}\n",
    "\n",
    "def train_eval(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    model.fit(\n",
    "        X_train_model, y_train,\n",
    "        eval_set=[(X_valid_model, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = model.predict_proba(X_valid_model)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return auc, secs, model\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"secs\": None, \"model\": None}\n",
    "\n",
    "keys = list(grid.keys())\n",
    "for values in itertools.product(*[grid[k] for k in keys]):\n",
    "    params = base.copy()\n",
    "    for k,v in zip(keys, values):\n",
    "        params[k] = v\n",
    "    print(\"\\nProbando:\", {k:params[k] for k in keys})\n",
    "    auc, secs, mdl = train_eval(params)\n",
    "    print(f\"AUC valid={auc:.4f} | tiempo={secs/60:.1f} min | best_iter={mdl.best_iteration_}\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best = {\"auc\": auc, \"params\": params, \"secs\": secs, \"model\": mdl}\n",
    "\n",
    "print(\"\\n=== MEJOR CONFIGURACIÃ“N ===\")\n",
    "print(best[\"params\"])\n",
    "print(f\"AUC valid={best['auc']:.4f} | tiempo={best['secs']/60:.1f} min | best_iter={best['model'].best_iteration_}\")\n",
    "\n",
    "# Opcional: guardar el mejor modelo\n",
    "# from joblib import dump\n",
    "# dump(best[\"model\"], \"models/lgbm_best.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21757cd8",
   "metadata": {},
   "source": [
    "### RevisiÃ³n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44b5e8",
   "metadata": {},
   "source": [
    "Carga eficiente + dtypes compactos\n",
    "\n",
    "DerivaciÃ³n de features\n",
    "\n",
    "Split temporal (1â€“9 vs 10â€“12)\n",
    "\n",
    "Target Encoding KFold sin fuga\n",
    "\n",
    "Agregados histÃ³ricos sin fuga\n",
    "\n",
    "Entrenamiento LightGBM + mini-bÃºsqueda\n",
    "\n",
    "SelecciÃ³n de umbral operativo\n",
    "\n",
    "Guardado de artefactos\n",
    "\n",
    "FunciÃ³n de scoring para producciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eac698",
   "metadata": {},
   "source": [
    "1) Carga eficiente del CSV (solo columnas Ãºtiles + dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fb8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Columnas presentes: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA']\n",
      "â†’ Faltantes (se derivarÃ¡n si aplica): []\n",
      "âœ“ Cargado: (5231130, 14)  |  en 36.3s\n",
      "âœ“ Rate retraso: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\",\n",
    "    # opcional de anÃ¡lisis (no para predecir): \"MOTIVO_RETRASO\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"â†’ Columnas presentes:\", present)\n",
    "print(\"â†’ Faltantes (se derivarÃ¡n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"MOTIVO_RETRASO\":\"category\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"âœ“ Cargado: {v.shape}  |  en {t1-t0:.1f}s\")\n",
    "print(\"âœ“ Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c109b3",
   "metadata": {},
   "source": [
    "2) Derivar features (distancia, estacionalidad, hora/minuto, ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f7b7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Derivar features =====\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    a = np.sin((lat2-lat1)/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin((lon2-lon1)/2)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns):\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"],v[\"ORIGEN_LON\"],v[\"DEST_LAT\"],v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mensual\n",
    "if \"MONTH\" in v:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "\n",
    "# Hora / minuto del dÃ­a programados\n",
    "if \"SCHEDULED_DEPARTURE\" in v:\n",
    "    mins = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    hrs  = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hrs*60 + mins).astype(\"int32\")\n",
    "    v[\"HORA_SALIDA\"]       = hrs.astype(\"int16\")\n",
    "\n",
    "# Ruta texto\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"âœ“ Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0ffbf",
   "metadata": {},
   "source": [
    "3) SelecciÃ³n de variables (features/target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e20c1776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) SelecciÃ³n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# usa solo lo que exista\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444bc1b",
   "metadata": {},
   "source": [
    "4) Split temporal (train=meses 1â€“9, valid=10â€“12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c906f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split â†’ X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) Split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split â†’ X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ba89e",
   "metadata": {},
   "source": [
    "5) Target Encoding (KFold sin fuga) sobre categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b95fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TE aplicado | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Target Encoding KFold (sin fuga) =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        stats = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = (\n",
    "            train_df.iloc[val_idx][col].astype(str).map(smoothed)\n",
    "                  .fillna(global_mean).astype(np.float32)\n",
    "        )\n",
    "\n",
    "    full_stats = y.groupby(train_df[col].astype(str)).mean()\n",
    "    full_cnts  = y.groupby(train_df[col].astype(str)).size()\n",
    "    mapping = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# quitar columnas categÃ³ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"âœ“ TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b05f9",
   "metadata": {},
   "source": [
    "6) Agregados histÃ³ricos (sin fuga) y join a las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 6) Agregados histÃ³ricos (1â€“9) =====\n",
    "# global_mean = float(y_train.mean())\n",
    "\n",
    "# v_train = v.loc[train_mask, [\"RETRASADO_LLEGADA\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]].copy()\n",
    "\n",
    "# # def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "# #     g = df.groupby(keys)[target]\n",
    "# #     stats = g.mean()\n",
    "# #     cnts  = g.size()\n",
    "# #     agg = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "# #     out = agg.reset_index().rename(columns={0:f\"{pref}_rate\"})\n",
    "# #     out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "# #     return out\n",
    "\n",
    "# # def left_join_agg(X_left, keys, pref):\n",
    "# #     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "# #     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "# #     merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "# #     merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "# #     return merged\n",
    "\n",
    "# # --- FIX funciones de agregados ---\n",
    "\n",
    "# def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "#     \"\"\"\n",
    "#     Devuelve un DF con columnas: keys + [f\"{pref}_rate\", f\"{pref}_n\"]\n",
    "#     usando suavizado para evitar overfitting en claves con pocos registros.\n",
    "#     \"\"\"\n",
    "#     g = df.groupby(keys)[target]\n",
    "#     stats = g.mean()                         # media por clave\n",
    "#     cnts  = g.size()                         # conteo por clave\n",
    "\n",
    "#     # smoothed rate (float32) y count (int32)\n",
    "#     smoothed = ((stats*cnts + global_mean*smooth) / (cnts + smooth)).astype(\"float32\")\n",
    "#     cnts     = cnts.astype(\"int32\")\n",
    "\n",
    "#     # Â¡OJO! Convertimos explÃ­citamente a DataFrame con nombres correctos\n",
    "#     out_rate = smoothed.to_frame(name=f\"{pref}_rate\").reset_index()\n",
    "#     out_cnt  = cnts.to_frame(name=f\"{pref}_n\").reset_index()\n",
    "\n",
    "#     # Unimos por las llaves\n",
    "#     out = out_rate.merge(out_cnt, on=keys, how=\"left\")\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def left_join_agg(X_left, keys, pref):\n",
    "#     \"\"\"\n",
    "#     Hace left-join contra el DF de agregados y rellena NaN con defaults.\n",
    "#     \"\"\"\n",
    "#     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "\n",
    "#     # merge\n",
    "#     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "\n",
    "#     # asegurar que las columnas existan (si no, crearlas con defaults)\n",
    "#     rate_col = f\"{pref}_rate\"\n",
    "#     n_col    = f\"{pref}_n\"\n",
    "\n",
    "#     if rate_col not in merged.columns:\n",
    "#         merged[rate_col] = np.nan\n",
    "#     if n_col not in merged.columns:\n",
    "#         merged[n_col] = np.nan\n",
    "\n",
    "#     merged[rate_col] = merged[rate_col].fillna(global_mean).astype(\"float32\")\n",
    "#     merged[n_col]    = merged[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "#     return merged\n",
    "\n",
    "\n",
    "# Xt = X_train_model.copy()\n",
    "# Xv = X_valid_model.copy()\n",
    "\n",
    "# # AÃ±adimos llaves temporales para poder unir\n",
    "# Xt[\"AIRLINE\"] = v.loc[train_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xt[\"ORIGIN_AIRPORT\"] = v.loc[train_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xt[\"DESTINATION_AIRPORT\"] = v.loc[train_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xt[\"RUTA\"] = v.loc[train_mask, \"RUTA\"].astype(str).values\n",
    "# Xt[\"HORA_SALIDA\"] = v.loc[train_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# Xv[\"AIRLINE\"] = v.loc[valid_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xv[\"ORIGIN_AIRPORT\"] = v.loc[valid_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xv[\"DESTINATION_AIRPORT\"] = v.loc[valid_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xv[\"RUTA\"] = v.loc[valid_mask, \"RUTA\"].astype(str).values\n",
    "# Xv[\"HORA_SALIDA\"] = v.loc[valid_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# aggs_specs = [\n",
    "#     ([\"AIRLINE\"], \"AIR\"),\n",
    "#     ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "#     ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "#     ([\"RUTA\"], \"RUTA\"),\n",
    "#     ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "# ]\n",
    "\n",
    "# t0 = time.time()\n",
    "# for keys, pref in aggs_specs:\n",
    "#     Xt = left_join_agg(Xt, keys, pref)\n",
    "#     Xv = left_join_agg(Xv, keys, pref)\n",
    "# # limpiar llaves temporales\n",
    "# drop_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "# Xt.drop(columns=[c for c in drop_keys if c in Xt], inplace=True)\n",
    "# Xv.drop(columns=[c for c in drop_keys if c in Xv], inplace=True)\n",
    "# t1 = time.time()\n",
    "\n",
    "# print(f\"âœ“ Agregados aplicados en {t1-t0:.1f}s\")\n",
    "# print(\"Shapes ->\", Xt.shape, Xv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8afbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agregados aplicados en 18.5s\n",
      "âœ“ Columnas nuevas: ['AIR_rate', 'AIR_n', 'DES_rate', 'DES_n', 'ORI_rate', 'ORI_n', 'RUTA_rate', 'RUTA_n', 'RUTA_HORA_rate', 'RUTA_HORA_n']\n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6) Agregados histÃ³ricos (sin fuga) y uniÃ³n a matrices del modelo\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- 6.1 Asegurar variables base ---\n",
    "assert 'RETRASADO_LLEGADA' in v.columns, \"No existe la columna target en 'v'.\"\n",
    "target = 'RETRASADO_LLEGADA'\n",
    "\n",
    "# Si no existen las mÃ¡scaras, las recreamos\n",
    "if 'train_mask' not in globals() or 'valid_mask' not in globals():\n",
    "    train_mask = v['MONTH'].between(1, 9)\n",
    "    valid_mask = v['MONTH'].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask].copy()                # SOLO meses 1â€“9 para evitar fuga\n",
    "global_mean = float(v_train[target].mean())       # media global para smoothing/NaN\n",
    "\n",
    "# Asegurar columnas de llaves en v (por si faltan)\n",
    "if 'RUTA' not in v.columns and {'ORIGIN_AIRPORT','DESTINATION_AIRPORT'}.issubset(v.columns):\n",
    "    v['RUTA'] = (v['ORIGIN_AIRPORT'].astype(str) + '_' +\n",
    "                 v['DESTINATION_AIRPORT'].astype(str))\n",
    "\n",
    "if 'HORA_SALIDA' not in v.columns and 'SCHEDULED_DEPARTURE' in v.columns:\n",
    "    # SCHEDULED_DEPARTURE en HHMM (int). Tomamos la hora.\n",
    "    v['HORA_SALIDA'] = (v['SCHEDULED_DEPARTURE'] // 100).clip(0, 23).astype('int16')\n",
    "\n",
    "# --- 6.2 Funciones robustas de agregados y merge ---\n",
    "def build_agg(df, keys, target='RETRASADO_LLEGADA', pref='', smooth=20):\n",
    "    \"\"\"\n",
    "    Devuelve DataFrame con columnas: keys + [f'{pref}_rate', f'{pref}_n'].\n",
    "    rate con suavizado; n = conteo.\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "\n",
    "    smoothed = ((stats * cnts + global_mean * smooth) / (cnts + smooth)).astype('float32')\n",
    "    cnts     = cnts.astype('int32')\n",
    "\n",
    "    out_rate = smoothed.to_frame(name=f'{pref}_rate').reset_index()\n",
    "    out_cnt  = cnts.to_frame(name=f'{pref}_n').reset_index()\n",
    "    out = out_rate.merge(out_cnt, on=keys, how='left')\n",
    "    return out\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    \"\"\"\n",
    "    Left join contra el DF de agregados (solo entrenado con v_train).\n",
    "    Rellena NaN con defaults: rate=global_mean, n=0.\n",
    "    \"\"\"\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how='left')\n",
    "\n",
    "    rate_col = f'{pref}_rate'\n",
    "    n_col    = f'{pref}_n'\n",
    "    if rate_col not in merged.columns:\n",
    "        merged[rate_col] = np.nan\n",
    "    if n_col not in merged.columns:\n",
    "        merged[n_col] = np.nan\n",
    "\n",
    "    merged[rate_col] = merged[rate_col].fillna(global_mean).astype('float32')\n",
    "    merged[n_col]    = merged[n_col].fillna(0).astype('int32')\n",
    "    return merged\n",
    "\n",
    "# --- 6.3 Preparar matrices Xt/Xv con llaves temporales (sin fuga) ---\n",
    "Xt = X_train_model.copy()\n",
    "Xv = X_valid_model.copy()\n",
    "\n",
    "# Agregamos llaves desde 'v' usando el Ã­ndice original (alineaciÃ³n 1:1)\n",
    "for col in ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']:\n",
    "    if col in v.columns:\n",
    "        if col not in Xt.columns:\n",
    "            Xt[col] = v.loc[X_train_model.index, col].values\n",
    "        if col not in Xv.columns:\n",
    "            Xv[col] = v.loc[X_valid_model.index, col].values\n",
    "\n",
    "# --- 6.4 EspecificaciÃ³n de agregados a calcular ---\n",
    "aggs_specs = []\n",
    "if 'AIRLINE' in Xt.columns:\n",
    "    aggs_specs.append((['AIRLINE'], 'AIR'))\n",
    "if 'DESTINATION_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['DESTINATION_AIRPORT'], 'DES'))\n",
    "if 'ORIGIN_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['ORIGIN_AIRPORT'], 'ORI'))\n",
    "if 'RUTA' in Xt.columns:\n",
    "    aggs_specs.append((['RUTA'], 'RUTA'))\n",
    "if {'RUTA', 'HORA_SALIDA'}.issubset(Xt.columns):\n",
    "    aggs_specs.append((['RUTA', 'HORA_SALIDA'], 'RUTA_HORA'))  # opcional, suele ayudar\n",
    "\n",
    "# --- 6.5 Aplicar agregados y limpiar llaves temporales ---\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "\n",
    "# Eliminamos llaves si no eran parte de X_*_model original\n",
    "temp_keys = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']\n",
    "Xt.drop(columns=[c for c in temp_keys if c in Xt.columns and c not in X_train_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "Xv.drop(columns=[c for c in temp_keys if c in Xv.columns and c not in X_valid_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "new_cols = [c for c in Xt.columns if c.endswith('_rate') or c.endswith('_n')]\n",
    "print(f\"âœ“ Agregados aplicados en {t1-t0:.1f}s\")\n",
    "print(\"âœ“ Columnas nuevas:\", new_cols)\n",
    "print(\"Shapes ->\", Xt.shape, Xv.shape)\n",
    "\n",
    "# Actualizamos las matrices del modelo para usar estas versiones enriquecidas\n",
    "X_train_model = Xt\n",
    "X_valid_model = Xv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5c90c",
   "metadata": {},
   "source": [
    "7) Entrenamiento LightGBM + mini-bÃºsqueda + AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07595942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 7) Entrenamiento LightGBM =====\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "# grid = [\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=300, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.03, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "# ]\n",
    "\n",
    "# best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "# for p in grid:\n",
    "#     params = dict(\n",
    "#         objective=\"binary\",\n",
    "#         learning_rate=p[\"learning_rate\"],\n",
    "#         n_estimators=12000,\n",
    "#         num_leaves=p[\"num_leaves\"],\n",
    "#         min_child_samples=p[\"min_child_samples\"],\n",
    "#         subsample=p[\"subsample\"],\n",
    "#         colsample_bytree=p[\"colsample_bytree\"],\n",
    "#         reg_alpha=0.0,\n",
    "#         reg_lambda=5.0,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#         scale_pos_weight=scale_pos_weight,\n",
    "#     )\n",
    "#     print(\"Probando:\", {k:params[k] for k in [\"learning_rate\",\"num_leaves\",\"min_child_samples\",\"colsample_bytree\",\"subsample\"]})\n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     t0 = time.time()\n",
    "#     model.fit(\n",
    "#         Xt, y_train,\n",
    "#         eval_set=[(Xv, y_valid)],\n",
    "#         eval_metric=\"auc\",\n",
    "#         callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "#     )\n",
    "#     t1 = time.time()\n",
    "#     val_proba = model.predict_proba(Xv)[:,1]\n",
    "#     auc_val = roc_auc_score(y_valid, val_proba)\n",
    "#     print(f\"AUC valid={auc_val:.4f} | tiempo={(t1-t0)/60:.1f} min | best_iter={model.best_iteration_}\\n\")\n",
    "#     if auc_val > best[\"auc\"]:\n",
    "#         best.update({\"auc\": auc_val, \"model\": model, \"params\": params})\n",
    "\n",
    "# print(\"=== MEJOR CONFIGURACIÃ“N ===\")\n",
    "# print(best[\"params\"])\n",
    "# print(f\"AUC valid={best['auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53af73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.602295\tvalid_0's binary_logloss: 0.583084\n",
      "[400]\tvalid_0's auc: 0.602473\tvalid_0's binary_logloss: 0.580826\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602496\tvalid_0's binary_logloss: 0.460413\n",
      "âœ… Entrenado en 261.8s | best_iter=1 | ROC-AUC valid=0.6025\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4275 | Precision: 0.2009 | Recall: 0.7782 | F1: 0.3194 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[273276 497919]\n",
      " [ 35688 125201]]\n",
      "â†’ Umbral F1 Ã³ptimo: {'thr': 0.2, 'f1': 0.3193866396941872}\n",
      "\n",
      "== Umbral por PrecisiÃ³n â‰³ 0.30 (aprox) (thr=0.209) ==\n",
      "Accuracy: 0.7000 | Precision: 0.2434 | Recall: 0.3499 | F1: 0.2871 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[596171 175024]\n",
      " [104588  56301]]\n",
      "\n",
      "== Umbral por Recall â‰¥ 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.17261212508743848,\n",
       " 'pre': 0.17261212508743848,\n",
       " 'rec': 1.0,\n",
       " 'f1': 0.2944061747179482,\n",
       " 'auc': 0.6024960911474394,\n",
       " 'thr': 0.05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 7) Entrenar LightGBM + mÃ©tricas/umbrales\n",
    "# ================================\n",
    "import time\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Peso para desbalance (cÃ¡lculo en TRAIN)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/ max(pos,1), 1.0)\n",
    "\n",
    "# Mejores hiperparÃ¡metros que te rindieron (puedes ajustarlos si deseas)\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=12000,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"âœ… Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n",
    "\n",
    "# ---- MÃ©tricas helper ----\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_05 = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# BÃºsqueda de mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo:\", best)\n",
    "\n",
    "# Umbral operativo por objetivo de negocio (opcional)\n",
    "# - si quieres â‰¥ 0.30 de precisiÃ³n\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best[\"thr\"]  # heurÃ­stica simple\n",
    "report_metrics(y_valid, valid_proba, thr_prec, \"Umbral por PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "\n",
    "# - si quieres â‰¥ 0.70 de recall (recuperaciÃ³n de retrasos)\n",
    "# barrido rÃ¡pido hasta lograr >=0.70 recall\n",
    "thr_rec = best[\"thr\"]\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    rec = recall_score(y_valid, (valid_proba >= thr).astype(int), zero_division=0)\n",
    "    if rec >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "report_metrics(y_valid, valid_proba, thr_rec, \"Umbral por Recall â‰¥ 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73177a53",
   "metadata": {},
   "source": [
    "8) Umbrales operativos (precisiÃ³n mÃ­nima vs recall mÃ­nimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da54e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\lgbm_retrasos.pkl | best_iteration=1\n",
      "âœ… Target Encoding guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\target_encoding.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agregados histÃ³ricos guardados: 5 tablas.\n",
      "âœ… Orden de features guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\feature_order.json\n",
      "âœ… Metadatos guardados en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata.json\n",
      "âœ… Schema de entrada guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\input_schema.json\n",
      "\n",
      "ğŸ” Smoke test de recargaâ€¦\n",
      "Smoke AUC (mini valid): 0.6169\n",
      "âœ… Artefactos listos.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8) Guardar artefactos del modelo y metadatos\n",
    "# ============================================\n",
    "import os, json, time, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "# 8.1 Guardar el modelo LightGBM (con best_iteration en metadatos)\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "print(f\"âœ… Modelo guardado en: {model_path} | best_iteration={best_iter}\")\n",
    "\n",
    "# 8.2 Preparar y guardar mapeos de Target Encoding (si existen)\n",
    "#   - mappings: dict por columna {categoria -> media_suavizada}\n",
    "#   - defaults: dict por columna {default_global_mean}\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "if \"mappings\" in globals() and \"defaults\" in globals():\n",
    "    joblib.dump({\"mappings\": mappings, \"defaults\": defaults}, te_path)\n",
    "    print(f\"âœ… Target Encoding guardado en: {te_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No se encontraron 'mappings'/'defaults' para Target Encoding; se omite guardado TE.\")\n",
    "\n",
    "# 8.3 Guardar tablas de agregados histÃ³ricos (si tienes funciones y specs)\n",
    "#     Reconstituimos los agregados con v_train para congelar el 'histÃ³rico' sin fuga.\n",
    "agg_tables = {}\n",
    "if \"aggs_specs\" in globals() and \"v_train\" in globals() and \"build_agg\" in globals():\n",
    "    for keys, pref in aggs_specs:\n",
    "        try:\n",
    "            agg_df = build_agg(v_train, keys, target=\"RETRASADO_LLEGADA\", pref=pref, smooth=20)\n",
    "            # Guardar cada agregado como CSV individual\n",
    "            csv_path = os.path.join(ARTIF_DIR, f\"agg_{pref}.csv\")\n",
    "            agg_df.to_csv(csv_path, index=False)\n",
    "            agg_tables[pref] = csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  No se pudo construir/guardar el agregado '{pref}': {e}\")\n",
    "    print(f\"âœ… Agregados histÃ³ricos guardados: {len(agg_tables)} tablas.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No se encontraron 'aggs_specs'/'v_train'/'build_agg'; se omite guardado de agregados.\")\n",
    "\n",
    "# 8.4 Guardar orden de features finales del modelo\n",
    "feature_order = list(X_train_model.columns)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_order\": feature_order}, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "# 8.5 Guardar metadatos (umbrales, medias, info para inferencia)\n",
    "#     - umbral base 0.5 y umbral F1 Ã³ptimo (si existe 'best')\n",
    "#     - best_iteration\n",
    "#     - columnas categÃ³ricas originales usadas en TE (si existen)\n",
    "#     - global_mean para TE/aggregates (si existe)\n",
    "meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": model_path,\n",
    "    \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "    \"thresholds\": {\n",
    "        \"base_05\": 0.5,\n",
    "        \"best_f1\": float(best[\"thr\"]) if \"best\" in globals() and isinstance(best, dict) and \"thr\" in best else None\n",
    "    },\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"agg_tables\": agg_tables,  # dict {pref -> csv_path}\n",
    "    \"global_mean\": float(global_mean) if \"global_mean\" in globals() else None,\n",
    "    \"te_cols\": list(mappings.keys()) if \"mappings\" in globals() else []\n",
    "}\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Metadatos guardados en: {meta_path}\")\n",
    "\n",
    "# 8.6 (Opcional) Guardar un \"schema\" ligero de dtypes esperados para entrada cruda\n",
    "schema = {\n",
    "    \"required_raw_fields\": [\n",
    "        \"MONTH\",\"DAY_OF_WEEK\",\n",
    "        \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "        \"SCHEDULED_DEPARTURE\",  # para reconstruir MINUTO_DIA_SALIDA/SALIDA_SIN/COS en producciÃ³n si hace falta\n",
    "        \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"  # para DISTANCIA_HAV si no viene en la entrada\n",
    "    ],\n",
    "    \"notes\": \"Si la entrada ya trae SALIDA_SIN/SALIDA_COS/DISTANCIA_HAV, se usan directamente.\"\n",
    "}\n",
    "schema_path = os.path.join(ARTIF_DIR, \"input_schema.json\")\n",
    "with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Schema de entrada guardado en: {schema_path}\")\n",
    "\n",
    "# 8.7 Smoke test de recarga rÃ¡pida\n",
    "print(\"\\nğŸ” Smoke test de recargaâ€¦\")\n",
    "_loaded_model = joblib.load(model_path)\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    _feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Usamos un mini-slice de validaciÃ³n para probar predict_proba\n",
    "idx = np.random.RandomState(42).choice(len(X_valid_model), size=min(5000, len(X_valid_model)), replace=False)\n",
    "mini_Xv = X_valid_model.iloc[idx][_feature_order]\n",
    "mini_yv = y_valid.iloc[idx]\n",
    "\n",
    "proba = _loaded_model.predict_proba(mini_Xv, num_iteration=getattr(_loaded_model, \"best_iteration_\", None))[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Smoke AUC (mini valid): {roc_auc_score(mini_yv, proba):.4f}\")\n",
    "print(\"âœ… Artefactos listos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb15bf6",
   "metadata": {},
   "source": [
    "Paso 9 Â· Cargador de artefactos + funciones de preprocesamiento para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4da7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos cargados. Features esperadas: 23\n",
      "TE cols: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA'] | Agregados: ['AIR', 'DES', 'ORI', 'RUTA', 'RUTA_HORA']\n",
      "Umbrales -> base: 0.5 | best_f1: 0.2 | best_iter: 1\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9) Cargar artefactos y helpers\n",
    "# ================================\n",
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# 9.1 Cargar artefactos\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "feat_path  = os.path.join(ARTIF_DIR,  \"feature_order.json\")\n",
    "meta_path  = os.path.join(ARTIF_DIR,  \"metadata.json\")\n",
    "te_path    = os.path.join(ARTIF_DIR,  \"target_encoding.pkl\")\n",
    "\n",
    "model        = joblib.load(model_path)\n",
    "feature_order = json.load(open(feat_path, \"r\", encoding=\"utf-8\"))[\"feature_order\"]\n",
    "META          = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Target Encoding (si existÃ­a)\n",
    "TE = None\n",
    "if os.path.exists(te_path):\n",
    "    TE = joblib.load(te_path)  # dict {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = TE[\"mappings\"]\n",
    "    defaults = TE[\"defaults\"]\n",
    "    te_cols  = list(mappings.keys())\n",
    "else:\n",
    "    mappings, defaults, te_cols = {}, {}, []\n",
    "\n",
    "# Agregados histÃ³ricos (si existen)\n",
    "agg_tables = META.get(\"agg_tables\", {})  # {pref: path_csv}\n",
    "aggs = {pref: pd.read_csv(path) for pref, path in agg_tables.items() if os.path.exists(path)}\n",
    "\n",
    "# Umbrales\n",
    "thr_base  = META[\"thresholds\"][\"base_05\"] or 0.5\n",
    "thr_bestf = META[\"thresholds\"][\"best_f1\"] if META[\"thresholds\"][\"best_f1\"] is not None else thr_base\n",
    "\n",
    "best_iter = META.get(\"best_iteration\", None)\n",
    "global_mean = META.get(\"global_mean\", None)\n",
    "print(\"Artefactos cargados. Features esperadas:\", len(feature_order))\n",
    "print(\"TE cols:\", te_cols, \"| Agregados:\", list(aggs.keys()))\n",
    "print(\"Umbrales -> base:\", thr_base, \"| best_f1:\", thr_bestf, \"| best_iter:\", best_iter)\n",
    "\n",
    "# 9.2 Helpers para reconstruir features si vienen crudas (producciÃ³n)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # MINUTO_DIA_SALIDA + SALIDA_SIN/COS (si no vienen)\n",
    "    if \"SALIDA_SIN\" not in out.columns or \"SALIDA_COS\" not in out.columns:\n",
    "        if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "            hs = (out[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (out[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            out[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "        if \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "            out[\"SALIDA_SIN\"] = np.sin(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "            out[\"SALIDA_COS\"] = np.cos(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "\n",
    "    # DISTANCIA_HAV (si no viene) â€” requiere coords\n",
    "    needs_dist = \"DISTANCIA_HAV\" not in out.columns\n",
    "    has_coords = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns)\n",
    "    if needs_dist and has_coords:\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # Tipos bÃ¡sicos\n",
    "    if \"MONTH\" in out:        out[\"MONTH\"]        = out[\"MONTH\"].astype(\"int16\", errors=\"ignore\")\n",
    "    if \"DAY_OF_WEEK\" in out:  out[\"DAY_OF_WEEK\"]  = out[\"DAY_OF_WEEK\"].astype(\"int16\", errors=\"ignore\")\n",
    "    for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]:\n",
    "        if c in out:\n",
    "            out[c] = out[c].astype(\"object\")  # TE espera object/str para mapear\n",
    "\n",
    "    return out\n",
    "\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "        # Quitamos la cruda si no estaba en feature_order\n",
    "        if c not in feature_order and c in out.columns:\n",
    "            out.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        # Detectar las llaves del agregado por los nombres de columnas del CSV\n",
    "        keys = [c for c in agg_df.columns if c.endswith((\"_AIRLINE\",\"_DEST\",\"_ORIG\",\"_RUTA\")) or c in [\"AIRLINE\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]]\n",
    "        # Si no detecta, intenta heurÃ­sticas por prefijo\n",
    "        if not keys:\n",
    "            if pref == \"AIR\":  keys = [\"AIRLINE\"]\n",
    "            if pref == \"DES\":  keys = [\"DESTINATION_AIRPORT\"]\n",
    "            if pref == \"ORI\":  keys = [\"ORIGIN_AIRPORT\"]\n",
    "            if pref == \"RUTA\": keys = [\"RUTA\"]\n",
    "            if pref == \"RUTA_HORA\": keys = [\"RUTA\",\"HORA_SALIDA\"]\n",
    "        # Merge left\n",
    "        out = out.merge(agg_df, on=keys, how=\"left\")\n",
    "        # Completar nulos con valores razonables\n",
    "        rate_col = f\"{pref}_rate\"; n_col = f\"{pref}_n\"\n",
    "        if rate_col in out:\n",
    "            fill_rate = global_mean if global_mean is not None else out[rate_col].mean()\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out:\n",
    "            out[n_col]    = out[n_col].fillna(0).astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def align_features(df: pd.DataFrame, order: list[str]) -> pd.DataFrame:\n",
    "    # Crea cualquier falta con 0.0, y descarta extras\n",
    "    X = df.copy()\n",
    "    for c in order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0.0\n",
    "    X = X[order]\n",
    "    # Dtypes suaves para LightGBM\n",
    "    for c in X.select_dtypes(include=\"float\").columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    for c in X.select_dtypes(include=\"int\").columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d05a2",
   "metadata": {},
   "source": [
    "Paso 10 Â· FunciÃ³n de scoring (proba + clase) y ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ba6bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Artefactos cargados | Modelo: lgbm_retrasos.pkl | Features: 23 | Agregados: 5 | TE cols: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba_retraso</th>\n",
       "      <th>pred_clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proba_retraso  pred_clase\n",
       "0         0.2113           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10) Scoring / Inferencia: cargar artefactos y predecir\n",
    "# ============================================\n",
    "import os, json, joblib, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----- 10.1 Cargar artefactos guardados en el paso 8 -----\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# Modelo LightGBM\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Metadatos (umbrales, best_iteration, rutas de agregados, etc.)\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    META = json.load(f)\n",
    "\n",
    "best_iter     = META.get(\"best_iteration\", None)\n",
    "feature_order = None\n",
    "thr_base      = META.get(\"thresholds\", {}).get(\"base_05\", 0.5)\n",
    "thr_bestf     = META.get(\"thresholds\", {}).get(\"best_f1\", None)\n",
    "global_mean   = META.get(\"global_mean\", None)  # media global del target (para TE y agregados)\n",
    "\n",
    "# Orden de features (columna a columna, EXACTO como el modelo fue entrenado)\n",
    "feat_path = META.get(\"feature_order_path\", os.path.join(ARTIF_DIR, \"feature_order.json\"))\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Target Encoding (mappings y defaults)\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "mappings, defaults, te_cols = {}, {}, []\n",
    "if os.path.exists(te_path):\n",
    "    te_obj   = joblib.load(te_path)  # {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = te_obj.get(\"mappings\", {})\n",
    "    defaults = te_obj.get(\"defaults\", {})\n",
    "    te_cols  = list(mappings.keys())\n",
    "\n",
    "# Agregados histÃ³ricos (tablas CSV por prefijo)\n",
    "# META[\"agg_tables\"] = dict { \"AIR\": \".../agg_AIR.csv\", \"ORI\": \"...\", ... }\n",
    "aggs = {}\n",
    "for pref, csv_path in META.get(\"agg_tables\", {}).items():\n",
    "    try:\n",
    "        aggs[pref] = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ No se pudo cargar agregado {pref} desde {csv_path}: {e}\")\n",
    "\n",
    "print(f\"âœ… Artefactos cargados | Modelo: {os.path.basename(model_path)} | Features: {len(feature_order)} | Agregados: {len(aggs)} | TE cols: {len(te_cols)}\")\n",
    "\n",
    "# ----- 10.2 Utilidades de ingenierÃ­a para inferencia -----\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crea/asegura columnas derivadas necesarias en producciÃ³n (sin usar el target).\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Reconstruir HORA_SALIDA/MIN_SALIDA/MINUTO_DIA_SALIDA si es necesario\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "    if \"MIN_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"MIN_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59).astype(\"int16\")\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(out.columns):\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (out[\"HORA_SALIDA\"]*60 + out[\"MIN_SALIDA\"]).astype(\"int32\")\n",
    "\n",
    "    # SALIDA_SIN / SALIDA_COS (codificaciÃ³n cÃ­clica)\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_SIN\"] = np.sin(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_COS\"] = np.cos(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # DISTANCIA_HAV si faltara y existen coordenadas\n",
    "    need_geo = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}\n",
    "    if \"DISTANCIA_HAV\" not in out.columns and need_geo.issubset(out.columns):\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN / MONTH_COS (estacionalidad)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.3 Target Encoding (no elimina columnas crudas) -----\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica TE y deja las columnas crudas intactas (las filtramos al alinear).\"\"\"\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# ----- 10.4 Agregados histÃ³ricos (usa las llaves del CSV) -----\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une agregados guardados (prefijo -> CSV). Detecta llaves como todas las columnas\n",
    "    del CSV salvo {pref+'_rate', pref+'_n'}. Si falta 'HORA_SALIDA', la reconstruye\n",
    "    a partir de SCHEDULED_DEPARTURE.\n",
    "    \"\"\"\n",
    "    if not aggs:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        rate_col = f\"{pref}_rate\"\n",
    "        n_col    = f\"{pref}_n\"\n",
    "        metric_cols = {rate_col, n_col}\n",
    "        key_cols = [c for c in agg_df.columns if c not in metric_cols]\n",
    "\n",
    "        # Reconstruye HORA_SALIDA si el agregado lo requiere\n",
    "        if \"HORA_SALIDA\" in key_cols and \"HORA_SALIDA\" not in out.columns:\n",
    "            if \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "                out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Saltando agregado {pref}: falta HORA_SALIDA y no hay SCHEDULED_DEPARTURE.\")\n",
    "                continue\n",
    "\n",
    "        missing = [c for c in key_cols if c not in out.columns]\n",
    "        if missing:\n",
    "            print(f\"âš ï¸ Saltando agregado {pref}: faltan llaves {missing}.\")\n",
    "            continue\n",
    "\n",
    "        out = out.merge(agg_df, on=key_cols, how=\"left\")\n",
    "\n",
    "        if rate_col in out.columns:\n",
    "            fill_rate = (global_mean if global_mean is not None \n",
    "                         else out[rate_col].mean(skipna=True))\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out.columns:\n",
    "            out[n_col] = out[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.5 Alinear features al orden del modelo -----\n",
    "def align_features(df: pd.DataFrame, feature_order: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mantiene solo las columnas que el modelo espera (en su orden).\n",
    "    Rellena faltantes con 0.0 para flotantes y 0 para enteros.\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            # default general: 0.0\n",
    "            X[c] = 0.0\n",
    "    X = X[feature_order]\n",
    "\n",
    "    # NaNs -> 0.0 (float) / 0 (int)\n",
    "    for c in X.columns:\n",
    "        if pd.api.types.is_float_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "        elif pd.api.types.is_integer_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0).astype(X[c].dtype)\n",
    "        else:\n",
    "            # si quedÃ³ categÃ³rica/objeto por accidente, conviÃ©rtela a string vacÃ­a y luego codifica a 0.0\n",
    "            X[c] = X[c].astype(str).fillna(\"\")\n",
    "    return X\n",
    "\n",
    "# ----- 10.6 FunciÃ³n de scoring Ãºnica -----\n",
    "def score_retraso(df_raw: pd.DataFrame, threshold: float = None):\n",
    "    \"\"\"\n",
    "    df_raw: DataFrame crudo de entrada (puede traer SCHEDULED_DEPARTURE y coords).\n",
    "    threshold: None usa el umbral best_f1 (si existe) o 0.5.\n",
    "    Devuelve: (probabilidades, clases, X_final)\n",
    "    \"\"\"\n",
    "    thr = threshold if threshold is not None else (thr_bestf if thr_bestf is not None else thr_base)\n",
    "\n",
    "    # 1) IngenierÃ­a necesaria\n",
    "    df1 = add_engineered_columns(df_raw)\n",
    "\n",
    "    # 2) Agregados histÃ³ricos (usan llaves crudas, por eso VAN ANTES del TE)\n",
    "    df2 = apply_aggregates(df1)\n",
    "\n",
    "    # 3) Target Encoding (aÃ±ade *_TE y mantiene crudas)\n",
    "    df3 = apply_target_encoding(df2)\n",
    "\n",
    "    # 4) Alinear al orden de features del modelo\n",
    "    X = align_features(df3, feature_order)\n",
    "\n",
    "    # 5) Predecir\n",
    "    proba = model.predict_proba(X, num_iteration=best_iter)[:, 1]\n",
    "    yhat  = (proba >= thr).astype(int)\n",
    "    return proba, yhat, X\n",
    "\n",
    "# ----- 10.7 Ejemplo de inferencia -----\n",
    "# Ajusta este ejemplo con rutas reales ORIGEN/DESTINO/AIRLINE de tu dataset\n",
    "ejemplo = pd.DataFrame([dict(\n",
    "    MONTH=10, DAY_OF_WEEK=5,\n",
    "    AIRLINE=\"WN\",\n",
    "    ORIGIN_AIRPORT=\"LAX\", DESTINATION_AIRPORT=\"LAS\",\n",
    "    SCHEDULED_DEPARTURE=1425,                    # HHMM\n",
    "    ORIGEN_LAT=33.9425, ORIGEN_LON=-118.4081,    # LAX\n",
    "    DEST_LAT=36.0801,  DEST_LON=-115.1522        # LAS\n",
    ")])\n",
    "\n",
    "proba, clase, X_infer = score_retraso(ejemplo, threshold=None)  # usa best_f1 si existe\n",
    "pd.DataFrame({\n",
    "    \"proba_retraso\": proba.round(4),\n",
    "    \"pred_clase\": clase\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e9d8e",
   "metadata": {},
   "source": [
    "A) XGBoost (binario, desbalance con scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3abb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.59114\n",
      "[400]\tvalid-auc:0.60046\n",
      "[409]\tvalid-auc:0.60037\n",
      "AUC valid=0.6048 | best_iter=10 | tiempo=4.2 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58746\n",
      "[400]\tvalid-auc:0.60018\n",
      "[417]\tvalid-auc:0.60026\n",
      "AUC valid=0.6051 | best_iter=17 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58871\n",
      "[400]\tvalid-auc:0.59957\n",
      "[543]\tvalid-auc:0.59910\n",
      "AUC valid=0.6004 | best_iter=143 | tiempo=6.1 min\n",
      "\n",
      "=== XGB MEJOR ===\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42} | AUC: 0.6051 | best_iter: 17\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7214 | Precision: 0.2462 | Recall: 0.2977 | F1: 0.2695 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[624549 146646]\n",
      " [112995  47894]]\n",
      "\n",
      "== Mejor F1 (thr=0.420) ==\n",
      "Accuracy: 0.4981 | Precision: 0.2106 | Recall: 0.6939 | F1: 0.3231 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[352603 418592]\n",
      " [ 49248 111641]]\n",
      "â†’ Umbral F1 Ã³ptimo (XGB): {'thr': 0.42, 'f1': 0.32307175867647103}\n",
      "\n",
      "== PrecisiÃ³n â‰³ 0.30 (aprox) (thr=0.499) ==\n",
      "Accuracy: 0.7203 | Precision: 0.2458 | Recall: 0.3000 | F1: 0.2702 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[623093 148102]\n",
      " [112622  48267]]\n",
      "\n",
      "== Recall â‰¥ 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# XGBoost: entrenamiento\n",
    "# =========================\n",
    "import time, numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_model, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid_model, label=y_valid)\n",
    "\n",
    "grid = [\n",
    "    dict(eta=0.05, max_depth=8,   min_child_weight=5,  subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.05, max_depth=10,  min_child_weight=10, subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.03, max_depth=10,  min_child_weight=10, subsample=0.9, colsample_bytree=0.9),\n",
    "]\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"booster\": None, \"nrounds\": None}\n",
    "\n",
    "for p in grid:\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": p[\"eta\"],\n",
    "        \"max_depth\": p[\"max_depth\"],\n",
    "        \"min_child_weight\": p[\"min_child_weight\"],\n",
    "        \"subsample\": p[\"subsample\"],\n",
    "        \"colsample_bytree\": p[\"colsample_bytree\"],\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"nthread\": -1,\n",
    "        \"seed\": 42,\n",
    "        # \"tree_method\": \"hist\",      # CPU rÃ¡pido\n",
    "        # \"device\": \"cuda\"            # si tienes GPU\n",
    "    }\n",
    "    print(\"Probando XGB:\", params)\n",
    "    t0 = time.time()\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=400,\n",
    "        verbose_eval=400\n",
    "    )\n",
    "    t1 = time.time()\n",
    "    proba = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1))\n",
    "    auc  = roc_auc_score(y_valid, proba)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={booster.best_iteration} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best.update({\"auc\": auc, \"params\": params, \"booster\": booster, \"nrounds\": booster.best_iteration})\n",
    "\n",
    "print(\"=== XGB MEJOR ===\")\n",
    "print(best[\"params\"], \"| AUC:\", round(best[\"auc\"], 4), \"| best_iter:\", best[\"nrounds\"])\n",
    "\n",
    "# MÃ©tricas en umbrales (igual que LightGBM)\n",
    "valid_proba = best[\"booster\"].predict(dvalid, iteration_range=(0, best[\"nrounds\"]+1))\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo (XGB):\", best_f1)\n",
    "\n",
    "# Objetivos de negocio (opcionales)\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "_ = report_metrics(y_valid, valid_proba, thr_prec, \"PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "thr_rec = 0.05\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "_ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall â‰¥ 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258a578",
   "metadata": {},
   "source": [
    "B) CatBoost (usa mismas features numÃ©ricas)\n",
    "\n",
    "PodrÃ­amos explotar categorÃ­as nativas, pero como se hizo Target Encoding, lo mÃ¡s estable es alimentar a CatBoost con el mismo set numÃ©rico que LGBM/XGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36f0a93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# CatBoost: entrenamiento\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier, Pool\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreport_metrics\u001b[39m(y_true, y_prob, thr, title=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CatBoost: entrenamiento\n",
    "# =========================\n",
    "import time, numpy as np\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "train_pool = Pool(X_train_model, label=y_train)\n",
    "valid_pool = Pool(X_valid_model, label=y_valid)\n",
    "\n",
    "grid = [\n",
    "    dict(learning_rate=0.05, depth=8,  l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "    dict(learning_rate=0.05, depth=10, l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "    dict(learning_rate=0.03, depth=10, l2_leaf_reg=8.0, bagging_temperature=1.0),\n",
    "]\n",
    "\n",
    "best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "for p in grid:\n",
    "    params = dict(\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"AUC\",\n",
    "        iterations=12000,\n",
    "        learning_rate=p[\"learning_rate\"],\n",
    "        depth=p[\"depth\"],\n",
    "        l2_leaf_reg=p[\"l2_leaf_reg\"],\n",
    "        bagging_temperature=p[\"bagging_temperature\"],\n",
    "        random_seed=42,\n",
    "        od_type=\"Iter\",     # early stopping\n",
    "        od_wait=400,\n",
    "        verbose=400,\n",
    "        class_weights=[1.0, scale_pos_weight],  # desbalance\n",
    "        # task_type=\"GPU\", devices=\"0\",       # si tienes GPU\n",
    "    )\n",
    "    print(\"Probando CAT:\", params)\n",
    "    t0 = time.time()\n",
    "    model_cb = CatBoostClassifier(**params)\n",
    "    model_cb.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "    t1 = time.time()\n",
    "\n",
    "    proba = model_cb.predict_proba(valid_pool)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={model_cb.get_best_iteration()} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best.update({\"auc\": auc, \"model\": model_cb, \"params\": params})\n",
    "\n",
    "print(\"=== CAT MEJOR ===\")\n",
    "print(best[\"params\"][\"learning_rate\"], best[\"params\"][\"depth\"], \"| AUC:\", round(best[\"auc\"],4),\n",
    "      \"| best_iter:\", best[\"model\"].get_best_iteration())\n",
    "\n",
    "valid_proba = best[\"model\"].predict_proba(valid_pool)[:,1]\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1 (CatBoost)\")\n",
    "print(\"â†’ Umbral F1 Ã³ptimo (CAT):\", best_f1)\n",
    "\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "_ = report_metrics(y_valid, valid_proba, thr_prec, \"PrecisiÃ³n â‰³ 0.30 (aprox)\")\n",
    "thr_rec = 0.05\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "_ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall â‰¥ 0.70\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
