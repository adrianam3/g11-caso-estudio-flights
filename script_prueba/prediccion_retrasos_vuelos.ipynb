{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0a1e6",
   "metadata": {},
   "source": [
    "\n",
    "# Predicci√≥n de Retrasos de Vuelos en la Industria A√©rea ‚úàÔ∏è\n",
    "\n",
    "**Objetivo:** Construir un modelo que **prediga si un vuelo sufrir√° un retraso de llegada mayor a 15 minutos** (`RETRASADO_LLEGADA` = 1).  \n",
    "Este notebook sigue la estructura de `machine_learning.ipynb`, con secciones de carga de datos, EDA, preparaci√≥n, entrenamiento con **LightGBM**, evaluaci√≥n y conclusiones.\n",
    "\n",
    "**Dataset de entrada:** `data/processed/flights_clean.csv` (resultado del pipeline de limpieza e ingenier√≠a de caracter√≠sticas).  \n",
    "**Tama√±o esperado:** ~5.2M filas (podr√≠a requerir >8GB RAM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481317d",
   "metadata": {},
   "source": [
    "## 1. Importaciones y configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc87b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"LightGBM no est√° instalado. Instala con: pip install lightgbm\") from e\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb56eb",
   "metadata": {},
   "source": [
    "## 2. Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74133005",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eafda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.read_csv(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf1a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 41 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   MONTH                50000 non-null  int64  \n",
      " 1   DAY                  50000 non-null  int64  \n",
      " 2   DAY_OF_WEEK          50000 non-null  int64  \n",
      " 3   AIRLINE              50000 non-null  object \n",
      " 4   ORIGIN_AIRPORT       50000 non-null  object \n",
      " 5   DESTINATION_AIRPORT  50000 non-null  object \n",
      " 6   SCHEDULED_DEPARTURE  50000 non-null  int64  \n",
      " 7   DEPARTURE_TIME       50000 non-null  float64\n",
      " 8   DEPARTURE_DELAY      50000 non-null  float64\n",
      " 9   SCHEDULED_TIME       50000 non-null  float64\n",
      " 10  DISTANCE             50000 non-null  int64  \n",
      " 11  SCHEDULED_ARRIVAL    50000 non-null  int64  \n",
      " 12  ARRIVAL_TIME         50000 non-null  float64\n",
      " 13  ARRIVAL_DELAY        50000 non-null  float64\n",
      " 14  AIRLINE_NAME         50000 non-null  object \n",
      " 15  ORIGEN_AEROPUERTO    50000 non-null  object \n",
      " 16  ORIGEN_CIUDAD        50000 non-null  object \n",
      " 17  ORIGEN_ESTADO        50000 non-null  object \n",
      " 18  ORIGEN_LAT           50000 non-null  float64\n",
      " 19  ORIGEN_LON           50000 non-null  float64\n",
      " 20  DEST_AEROPUERTO      50000 non-null  object \n",
      " 21  DEST_CIUDAD          50000 non-null  object \n",
      " 22  DEST_ESTADO          50000 non-null  object \n",
      " 23  DEST_LAT             50000 non-null  float64\n",
      " 24  DEST_LON             50000 non-null  float64\n",
      " 25  MOTIVO_RETRASO       50000 non-null  object \n",
      " 26  CANTIDAD_CAUSAS      50000 non-null  int64  \n",
      " 27  RETRASADO_LLEGADA    50000 non-null  int64  \n",
      " 28  RETRASADO_SALIDA     50000 non-null  int64  \n",
      " 29  HORA_SALIDA          50000 non-null  int64  \n",
      " 30  HORA_LLEGADA         50000 non-null  int64  \n",
      " 31  MIN_SALIDA           50000 non-null  int64  \n",
      " 32  MIN_LLEGADA          50000 non-null  int64  \n",
      " 33  MINUTO_DIA_SALIDA    50000 non-null  int64  \n",
      " 34  MINUTO_DIA_LLEGADA   50000 non-null  int64  \n",
      " 35  SALIDA_SIN           50000 non-null  float64\n",
      " 36  SALIDA_COS           50000 non-null  float64\n",
      " 37  LLEGADA_SIN          50000 non-null  float64\n",
      " 38  LLEGADA_COS          50000 non-null  float64\n",
      " 39  PERIODO_SALIDA       50000 non-null  object \n",
      " 40  PERIODO_LLEGADA      50000 non-null  object \n",
      "dtypes: float64(13), int64(15), object(13)\n",
      "memory usage: 15.6+ MB\n"
     ]
    }
   ],
   "source": [
    "v.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59569525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'SCHEDULED_TIME', 'DISTANCE',\n",
       "       'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'AIRLINE_NAME', 'ORIGEN_AEROPUERTO', 'ORIGEN_CIUDAD', 'ORIGEN_ESTADO', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_AEROPUERTO', 'DEST_CIUDAD',\n",
       "       'DEST_ESTADO', 'DEST_LAT', 'DEST_LON', 'MOTIVO_RETRASO', 'CANTIDAD_CAUSAS', 'RETRASADO_LLEGADA', 'RETRASADO_SALIDA', 'HORA_SALIDA', 'HORA_LLEGADA', 'MIN_SALIDA', 'MIN_LLEGADA',\n",
       "       'MINUTO_DIA_SALIDA', 'MINUTO_DIA_LLEGADA', 'SALIDA_SIN', 'SALIDA_COS', 'LLEGADA_SIN', 'LLEGADA_COS', 'PERIODO_SALIDA', 'PERIODO_LLEGADA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feab64",
   "metadata": {},
   "source": [
    "## 3. Inspecci√≥n r√°pida de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463af07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 41 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   MONTH                50000 non-null  int64  \n",
      " 1   DAY                  50000 non-null  int64  \n",
      " 2   DAY_OF_WEEK          50000 non-null  int64  \n",
      " 3   AIRLINE              50000 non-null  object \n",
      " 4   ORIGIN_AIRPORT       50000 non-null  object \n",
      " 5   DESTINATION_AIRPORT  50000 non-null  object \n",
      " 6   SCHEDULED_DEPARTURE  50000 non-null  int64  \n",
      " 7   DEPARTURE_TIME       50000 non-null  float64\n",
      " 8   DEPARTURE_DELAY      50000 non-null  float64\n",
      " 9   SCHEDULED_TIME       50000 non-null  float64\n",
      " 10  DISTANCE             50000 non-null  int64  \n",
      " 11  SCHEDULED_ARRIVAL    50000 non-null  int64  \n",
      " 12  ARRIVAL_TIME         50000 non-null  float64\n",
      " 13  ARRIVAL_DELAY        50000 non-null  float64\n",
      " 14  AIRLINE_NAME         50000 non-null  object \n",
      " 15  ORIGEN_AEROPUERTO    50000 non-null  object \n",
      " 16  ORIGEN_CIUDAD        50000 non-null  object \n",
      " 17  ORIGEN_ESTADO        50000 non-null  object \n",
      " 18  ORIGEN_LAT           50000 non-null  float64\n",
      " 19  ORIGEN_LON           50000 non-null  float64\n",
      " 20  DEST_AEROPUERTO      50000 non-null  object \n",
      " 21  DEST_CIUDAD          50000 non-null  object \n",
      " 22  DEST_ESTADO          50000 non-null  object \n",
      " 23  DEST_LAT             50000 non-null  float64\n",
      " 24  DEST_LON             50000 non-null  float64\n",
      " 25  MOTIVO_RETRASO       50000 non-null  object \n",
      " 26  CANTIDAD_CAUSAS      50000 non-null  int64  \n",
      " 27  RETRASADO_LLEGADA    50000 non-null  int64  \n",
      " 28  RETRASADO_SALIDA     50000 non-null  int64  \n",
      " 29  HORA_SALIDA          50000 non-null  int64  \n",
      " 30  HORA_LLEGADA         50000 non-null  int64  \n",
      " 31  MIN_SALIDA           50000 non-null  int64  \n",
      " 32  MIN_LLEGADA          50000 non-null  int64  \n",
      " 33  MINUTO_DIA_SALIDA    50000 non-null  int64  \n",
      " 34  MINUTO_DIA_LLEGADA   50000 non-null  int64  \n",
      " 35  SALIDA_SIN           50000 non-null  float64\n",
      " 36  SALIDA_COS           50000 non-null  float64\n",
      " 37  LLEGADA_SIN          50000 non-null  float64\n",
      " 38  LLEGADA_COS          50000 non-null  float64\n",
      " 39  PERIODO_SALIDA       50000 non-null  object \n",
      " 40  PERIODO_LLEGADA      50000 non-null  object \n",
      "dtypes: float64(13), int64(15), object(13)\n",
      "memory usage: 49.1 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v.info(memory_usage='deep', show_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b78f3",
   "metadata": {},
   "source": [
    "## 4. Distribuci√≥n de retrasos (llegada > 15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747d955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Distribuci√≥n de vuelos seg√∫n retraso en llegada (>15 min):\n",
      "\n",
      "A tiempo (0): 40,962 vuelos (81.92%)\n",
      "Retrasados (1): 9,038 vuelos (18.08%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert \"RETRASADO_LLEGADA\" in v.columns, \"No existe la columna RETRASADO_LLEGADA en el dataset limpio.\"\n",
    "\n",
    "conteo = v[\"RETRASADO_LLEGADA\"].value_counts().sort_index()\n",
    "porc = (conteo / conteo.sum() * 100).round(2)\n",
    "\n",
    "print(\"üìä Distribuci√≥n de vuelos seg√∫n retraso en llegada (>15 min):\\n\")\n",
    "print(f\"A tiempo (0): {conteo.get(0,0):,} vuelos ({porc.get(0,0):.2f}%)\")\n",
    "print(f\"Retrasados (1): {conteo.get(1,0):,} vuelos ({porc.get(1,0):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509f74a",
   "metadata": {},
   "source": [
    "## 5. Selecci√≥n de variables (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf346a",
   "metadata": {},
   "source": [
    "\n",
    "Usaremos variables **categ√≥ricas y de tiempo** ya generadas en el pipeline:\n",
    "\n",
    "- `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT` (categ√≥ricas)\n",
    "- `MONTH`, `DAY_OF_WEEK` (tiempo)\n",
    "- Codificaci√≥n c√≠clica: `SALIDA_SIN`, `SALIDA_COS` (derivadas de la hora programada de salida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380aa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0d6569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>SALIDA_SIN</th>\n",
       "      <th>SALIDA_COS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA</td>\n",
       "      <td>BOS</td>\n",
       "      <td>IAD</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.997859</td>\n",
       "      <td>0.065403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>CLE</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>-0.422618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA</td>\n",
       "      <td>DEN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.746057</td>\n",
       "      <td>-0.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EV</td>\n",
       "      <td>ATL</td>\n",
       "      <td>FAY</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.267238</td>\n",
       "      <td>-0.963630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WN</td>\n",
       "      <td>ABQ</td>\n",
       "      <td>OAK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980785</td>\n",
       "      <td>-0.195090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AIRLINE ORIGIN_AIRPORT DESTINATION_AIRPORT  MONTH  DAY_OF_WEEK  SALIDA_SIN  SALIDA_COS\n",
       "0      UA            BOS                 IAD      5            7    0.997859    0.065403\n",
       "1      DL            ATL                 CLE      5            1   -0.906308   -0.422618\n",
       "2      UA            DEN                 SEA      4            4   -0.746057   -0.665882\n",
       "3      EV            ATL                 FAY      7            5   -0.267238   -0.963630\n",
       "4      WN            ABQ                 OAK      1            1    0.980785   -0.195090"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\"\n",
    "]\n",
    "\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "# Liberar memoria del dataframe original si es necesario\n",
    "del v\n",
    "gc.collect()\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c69ea",
   "metadata": {},
   "source": [
    "## 6. Codificaci√≥n de variables categ√≥ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430587be",
   "metadata": {},
   "source": [
    "\n",
    "Para eficiencia con >5M de filas, usamos **Label Encoding** para `AIRLINE`, `ORIGIN_AIRPORT`, `DESTINATION_AIRPORT`.  \n",
    "LightGBM maneja bien etiquetas enteras y permite splits por categor√≠a, especialmente cuando las variables no son ordinales reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef32330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categ√≥ricas codificadas: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categorical_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Guardar encoders en memoria (opcional: persistir a disco si se desea)\n",
    "print(\"‚úÖ Categ√≥ricas codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8235487",
   "metadata": {},
   "source": [
    "## 7. Divisi√≥n Train/Test (estratificada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2506a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (40000, 7), Test: (10000, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0e536",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento: LightGBM (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ebb745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7230, number of negative: 32770\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 928\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "‚úÖ Modelo entrenado en 0.95s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = dict(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",  # ‚úÖ Compensa desbalance (18/82 aprox.)\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"‚úÖ Modelo entrenado en {t1 - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9c69f",
   "metadata": {},
   "source": [
    "## 9. Evaluaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c176b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8604    0.6666    0.7512      8192\n",
      "           1     0.2524    0.5100    0.3377      1808\n",
      "\n",
      "    accuracy                         0.6383     10000\n",
      "   macro avg     0.5564    0.5883    0.5444     10000\n",
      "weighted avg     0.7505    0.6383    0.6765     10000\n",
      "\n",
      "Accuracy:   0.6383\n",
      "Precision:  0.2524\n",
      "Recall:     0.5100\n",
      "F1-score:   0.3377\n",
      "ROC-AUC:    0.6320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {prec:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1-score:   {f1:.4f}\")\n",
    "print(f\"ROC-AUC:    {roc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8211b",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31077444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC = {roc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC - Retrasos de Llegada\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575726a",
   "metadata": {},
   "source": [
    "### Matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8431adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5461, 2731],\n",
       "       [ 886,  922]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233e1f",
   "metadata": {},
   "source": [
    "## 10. Importancia de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b031c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = lgb.plot_importance(model, max_num_features=20, importance_type=\"gain\")\n",
    "plt.title(\"Importancia de variables (LightGBM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75011e51",
   "metadata": {},
   "source": [
    "## 11. Funci√≥n de predicci√≥n (para integraci√≥n futura con API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3227b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de retraso (ejemplo): 0.4943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preparar_entrada(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute):\n",
    "    \"\"\"\n",
    "    Prepara un diccionario con las features necesarias para predicci√≥n.\n",
    "    - scheduled_hour: 0-23\n",
    "    - scheduled_minute: 0-59\n",
    "    \"\"\"\n",
    "    minuto_dia = scheduled_hour * 60 + scheduled_minute\n",
    "    salida_sin = math.sin(2 * math.pi * minuto_dia / (24*60))\n",
    "    salida_cos = math.cos(2 * math.pi * minuto_dia / (24*60))\n",
    "    row = {\n",
    "        \"AIRLINE\": encoders[\"AIRLINE\"].transform([str(airline)])[0],\n",
    "        \"ORIGIN_AIRPORT\": encoders[\"ORIGIN_AIRPORT\"].transform([str(origin)])[0],\n",
    "        \"DESTINATION_AIRPORT\": encoders[\"DESTINATION_AIRPORT\"].transform([str(destination)])[0],\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def predecir_probabilidad_delay(sample_dict):\n",
    "    \"\"\"Recibe un diccionario de features y devuelve probabilidad de retraso en llegada (>15 min).\"\"\"\n",
    "    df = pd.DataFrame([sample_dict])[list(model.feature_name_)]\n",
    "    proba = model.predict_proba(df)[:, 1][0]\n",
    "    return float(proba)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "ejemplo = preparar_entrada(\"AA\", \"JFK\", \"LAX\", 5, 4, 14, 30)\n",
    "print(\"Probabilidad de retraso (ejemplo):\", round(predecir_probabilidad_delay(ejemplo), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53002b4",
   "metadata": {},
   "source": [
    "## 12. Conclusiones y siguientes pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91e733",
   "metadata": {},
   "source": [
    "\n",
    "- El modelo **LightGBM** con `class_weight=\"balanced\"` maneja correctamente el desbalance (~18% retrasos).\n",
    "- Las variables de ubicaci√≥n (aeropuerto y aerol√≠nea) y la codificaci√≥n c√≠clica de la hora suelen aportar poder predictivo.\n",
    "- Para producci√≥n:\n",
    "  - serializar `model` y `encoders` con `joblib`,\n",
    "  - crear un endpoint `/flights/predict-delay` con FastAPI,\n",
    "  - validar en datos recientes y monitorear m√©tricas.\n",
    "\n",
    "**Mejoras posibles:**\n",
    "- Agregar variable de **distancia Haversine**.\n",
    "- Agregar **mes/d√≠a** como seno/coseno (estacionalidad).\n",
    "- Regularizaci√≥n y **b√∫squeda de hiperpar√°metros** (Optuna).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cf1c",
   "metadata": {},
   "source": [
    "## 13. (Opcional) Guardado de modelo y encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f03a122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from joblib import dump\n",
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "# dump(model, \"models/lgbm_delay_model.joblib\")\n",
    "# dump(encoders, \"models/label_encoders.joblib\")\n",
    "# print(\"‚úÖ Modelo y encoders guardados en carpeta models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d3b12a",
   "metadata": {},
   "source": [
    "### 14. Resumen: \n",
    "Resumo lo que muestran tus capturas y qu√© har√≠a para mejorarlo r√°pido:\n",
    "\n",
    "Lectura r√°pida de resultados\n",
    "\n",
    "Accuracy ~0.783: por debajo del baseline ‚Äútodo a tiempo‚Äù (‚âà0.815). Normal cuando forzamos a detectar m√°s retrasos.\n",
    "\n",
    "Precision ‚âà 0.63 | Recall ‚âà 0.64 | F1 ‚âà 0.63 | ROC-AUC ‚âà 0.689: rendimiento moderado; el modelo detecta una cantidad razonable de retrasos.\n",
    "\n",
    "Importancia de variables: domina SALIDA_SIN (hora del d√≠a), luego AIRLINE, MONTH, aeropuertos, DAY_OF_WEEK, SALIDA_COS. Tiene sentido: hora, aerol√≠nea, estacionalidad y aeropuertos pesan mucho.\n",
    "\n",
    "Nota: si la matriz de confusi√≥n no cuadra con tus m√©tricas, recuerda que confusion_matrix(y_test, y_pred) retorna [[TN, FP],[FN, TP]]. Verifica que est√©s leyendo TP = [1,1] y FP = [0,1], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a80dcd",
   "metadata": {},
   "source": [
    "###  Revisi√≥n 2  - Revsiones dicionales post resultados.. \n",
    "\n",
    "a continuaci√≥n (en orden de impacto vs. esfuerzo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87803653",
   "metadata": {},
   "source": [
    "1) Nuevas features: Haversine + estacionalidad\n",
    "\n",
    "Pega esta celda despu√©s de cargar v o (si ya ten√≠as X, y) vuelve a crear X con estas columnas nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90ca2bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/flights_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mv\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'v' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     DATA_PATH = \u001b[33m\"\u001b[39m\u001b[33mdata/processed/flights_clean.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m     t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     v = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRe-cargado v:\u001b[39m\u001b[33m\"\u001b[39m, v.shape, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33men \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-t0\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Chequeo de columnas necesarias\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/processed/flights_clean.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "# Si a√∫n tienes el DF completo como 'v', √∫salo. Si ya lo liberaste, vuelve a cargar flights_clean y rehaz lo m√≠nimo.\n",
    "try:\n",
    "    v\n",
    "except NameError:\n",
    "    import os, time\n",
    "    DATA_PATH = \"data/processed/flights_clean.csv\"\n",
    "    t0 = time.time()\n",
    "    v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    print(\"Re-cargado v:\", v.shape, f\"en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Chequeo de columnas necesarias\n",
    "req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "       \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\"]\n",
    "falta = [c for c in req if c not in v.columns]\n",
    "if falta:\n",
    "    raise ValueError(\"Faltan columnas en v: \" + \", \".join(falta))\n",
    "\n",
    "# 1) Distancia Haversine\n",
    "v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# 2) Estacionalidad anual del mes (seno/coseno)\n",
    "v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# 3) (Opcional) Minuto crudo del d√≠a si lo tienes; si no, lo calculo r√°pido\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Requiere HORA_SALIDA y MIN_SALIDA; si no est√°n, se puede derivar de SCHEDULED_DEPARTURE HHMM\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(np.int16)\n",
    "    else:\n",
    "        if \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "            hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# === Selecci√≥n de variables actualizada ===\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "features = [\n",
    "    \"AIRLINE\",\n",
    "    \"ORIGIN_AIRPORT\",\n",
    "    \"DESTINATION_AIRPORT\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\n",
    "    \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\n",
    "    \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "]\n",
    "\n",
    "# (Opcional) a√±adir \"MINUTO_DIA_SALIDA\" si existe:\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(int).copy()\n",
    "\n",
    "X.dtypes, X.shape, y.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655b525",
   "metadata": {},
   "source": [
    "2) Codificaci√≥n categ√≥rica igual que antes (LabelEncoder)\n",
    "\n",
    "Si ya lo ten√≠as, puedes saltarla; si no, ej√©c√∫tala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"Codificadas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ead0fb",
   "metadata": {},
   "source": [
    "3) Split y entrenamiento con early stopping\n",
    "\n",
    "Entrenamos con validaci√≥n en el set de test y paramos cuando deje de mejorar AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f580ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usar scale_pos_weight, ver m√°s abajo\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"‚úÖ Entrenado en {t1-t0:.1f}s | best_iteration_={model.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77873373",
   "metadata": {},
   "source": [
    "ponderaci√≥n expl√≠cita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903de63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = y_train.sum(); neg = len(y_train)-pos\n",
    "spw = neg / pos\n",
    "# usar scale_pos_weight=spw y quitar class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65c187",
   "metadata": {},
   "source": [
    "4) M√©tricas a umbral 0.5 y b√∫squeda del mejor umbral\n",
    "\n",
    "Calcula m√©tricas en 0.5, luego busca mejor F1 y mejor recall con precisi√≥n m√≠nima (p. ej. ‚â•0.60). Imprime confusiones y compara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# M√©tricas al umbral 0.5\n",
    "y_pred05 = (y_proba>=0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"Umbral 0.5\")\n",
    "\n",
    "# B√∫squeda de mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "\n",
    "# B√∫squeda max recall con precisi√≥n m√≠nima (ajusta min_prec segun negocio)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"prec\":0}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_test, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_test, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":thr, \"rec\":rec, \"prec\":pre}\n",
    "\n",
    "y_pred_rec = (y_proba>=best_rec[\"thr\"]).astype(int)\n",
    "rec_res = metricas(y_test, y_pred_rec, y_proba, f\"Max Recall con Prec ‚â• {min_prec:.2f} (thr={best_rec['thr']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2e2a7",
   "metadata": {},
   "source": [
    "5) (Opcional) Curva Precision-Recall para elegir umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65519f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b425c",
   "metadata": {},
   "source": [
    "### Revisi√≥n 3 -  nueva revisi√≥n "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932e89d",
   "metadata": {},
   "source": [
    "Despues de los resultados anteriores, se sugiere los siguiente\n",
    "aplicar Target Encoding con K-Fold (sin fuga) a RUTA, AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT. Incluyen: creaci√≥n de RUTA, codificaci√≥n KFold con smoothing, split estratificado, entrenamiento LightGBM con early stopping, comparaci√≥n de m√©tricas y guardado de mapeos para usar en producci√≥n/API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4713d",
   "metadata": {},
   "source": [
    "Notas finales\n",
    "\n",
    "El Target Encoding K-Fold suele subir AUC/F1 sobre tus resultados actuales, especialmente con RUTA_TE.\n",
    "\n",
    "El smoothing controla el ‚Äúoverfit‚Äù en categor√≠as raras. Para rutas con pocos vuelos, empuja la media hacia la global. Puedes probar 20, 50, 100.\n",
    "\n",
    "Para producci√≥n, ideal: precalcular DISTANCIA_HAV a partir de lat/lon del origen/destino (que ya tienes en cat√°logos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f25528",
   "metadata": {},
   "source": [
    "0) (Opcional) Asegurar features base\n",
    "\n",
    "Si ya calculaste DISTANCIA_HAV, MONTH_SIN/COS, etc., puedes saltar esta celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# RUTA (origen_destino)\n",
    "v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# Distancia (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(np.float32)\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# (opcional) minuto del d√≠a\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "v.shape, v.columns[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876fc30",
   "metadata": {},
   "source": [
    "1) Funciones de Target Encoding K-Fold (sin fuga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76725cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_target_encode(train_df, col, target, n_splits=5, smoothing=50, random_state=42):\n",
    "    \"\"\"\n",
    "    KFold target encoding sin fuga: devuelve\n",
    "      enc_train: serie con el encoding para el train (via KFold)\n",
    "      mapping: dict {categoria: media_suavizada} usando TODO el train (para aplicar en test/producci√≥n)\n",
    "      default: media global del target (para categor√≠as nuevas)\n",
    "    smoothing: mayor valor => m√°s peso de la media global si hay pocos datos por categor√≠a\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = float(train_df[target].mean())\n",
    "\n",
    "    enc_train = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, train_df[target]):\n",
    "        tr, val = train_df.iloc[tr_idx], train_df.iloc[val_idx]\n",
    "        stats = tr.groupby(col)[target].mean()\n",
    "        cnts  = tr[col].value_counts()\n",
    "        smooth = (cnts*stats + smoothing*global_mean) / (cnts + smoothing)\n",
    "        enc_train.iloc[val_idx] = val[col].map(smooth).fillna(global_mean).astype(np.float32)\n",
    "\n",
    "    # mapping final con TODO el train (para test/producci√≥n)\n",
    "    full_stats = train_df.groupby(col)[target].mean()\n",
    "    full_cnts  = train_df[col].value_counts()\n",
    "    mapping = ((full_cnts*full_stats + smoothing*global_mean)/(full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc_train, mapping, global_mean\n",
    "\n",
    "def aplicar_target_encoding_test(test_df, col, mapping, default):\n",
    "    return test_df[col].map(mapping).fillna(default).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321751c",
   "metadata": {},
   "source": [
    "2) Preparaci√≥n de train/test y generaci√≥n de *_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cols_te = [\"RUTA\", \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# Variables num√©ricas base (s√∫malas a gusto)\n",
    "num_features = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\"\n",
    "]\n",
    "if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "    num_features.append(\"MINUTO_DIA_SALIDA\")\n",
    "\n",
    "# Split estratificado ANTES del encoding para evitar fuga\n",
    "train_df, test_df = train_test_split(v, test_size=0.2, stratify=v[target], random_state=42)\n",
    "\n",
    "# Generar encodings en train y mappings para test\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    tr_enc, mapping, default = kfold_target_encode(train_df, c, target, n_splits=5, smoothing=50, random_state=42)\n",
    "    train_df[f\"{c}_TE\"] = tr_enc\n",
    "    test_df[f\"{c}_TE\"]  = aplicar_target_encoding_test(test_df, c, mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Conjunto final de features\n",
    "features = num_features + [f\"{c}_TE\" for c in cols_te]\n",
    "\n",
    "X_train = train_df[features].copy()\n",
    "y_train = train_df[target].astype(int).copy()\n",
    "X_test  = test_df[features].copy()\n",
    "y_test  = test_df[target].astype(int).copy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a74ce",
   "metadata": {},
   "source": [
    "3) Entrenamiento LightGBM con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",   # o usa scale_pos_weight = (neg/pos)\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_te = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model_te.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"‚úÖ Entrenado TE en {t1-t0:.1f}s | best_iteration_={model_te.best_iteration_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcbcfc",
   "metadata": {},
   "source": [
    "4) M√©tricas a 0.5 + b√∫squeda de mejor umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_proba = model_te.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metricas(y_true, y_hat, y_prob=None, titulo=\"\"):\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n=== {titulo} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]] =\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, roc=roc, cm=cm)\n",
    "\n",
    "# Umbral 0.5\n",
    "y_pred05 = (y_proba >= 0.5).astype(int)\n",
    "base = metricas(y_test, y_pred05, y_proba, \"TE - Umbral 0.5\")\n",
    "\n",
    "# Mejor F1\n",
    "umbrales = np.linspace(0.1, 0.9, 33)\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in umbrales:\n",
    "    y_hat = (y_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_hat, zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\":thr, \"f1\":f1}\n",
    "\n",
    "y_pred_f1 = (y_proba>=best_f1[\"thr\"]).astype(int)\n",
    "f1_res = metricas(y_test, y_pred_f1, y_proba, f\"TE - Mejor F1 (thr={best_f1['thr']:.3f})\")\n",
    "best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3be42",
   "metadata": {},
   "source": [
    "5) (Opcional) Guardar modelo + mapeos + umbral\n",
    "\n",
    "√ötil para tu endpoint /flights/predict-delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "dump(model_te, \"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"mappings\": {k: {str(kk): float(vv) for kk, vv in mp.items()} for k, mp in mappings.items()},\n",
    "               \"defaults\": defaults}, f)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1[\"thr\"])  # o fija el que defina negocio\n",
    "with open(\"models/umbral.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "print(\"‚úÖ Guardados: modelo, mappings y umbral.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4d5dc",
   "metadata": {},
   "source": [
    "6) (Producci√≥n) Aplicar TE a un nuevo registro\n",
    "\n",
    "Ejemplo de funci√≥n para preparar features a partir de un vuelo nuevo (usa mapeos TE; categor√≠as no vistas ‚Üí media global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db72026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json\n",
    "import numpy as np\n",
    "\n",
    "# Cargar mappings en producci√≥n:\n",
    "# from joblib import load; model_te = load(\"models/lgbm_delay_te.joblib\")\n",
    "# mappings = json.load(open(\"models/te_mappings.json\"))[\"mappings\"]\n",
    "# defaults = json.load(open(\"models/te_mappings.json\"))[\"defaults\"]\n",
    "# thr = json.load(open(\"models/umbral.json\"))[\"threshold\"]\n",
    "\n",
    "def preparar_features_api(airline, origin, destination, month, day_of_week, scheduled_hour, scheduled_minute,\n",
    "                          mappings=mappings, defaults=defaults):\n",
    "    # c√≠clicos de hora\n",
    "    minuto_dia = scheduled_hour*60 + scheduled_minute\n",
    "    salida_sin = math.sin(2*math.pi*minuto_dia/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*minuto_dia/(24*60))\n",
    "    # c√≠clicos de mes\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    fila = {\n",
    "        \"MONTH\": month,\n",
    "        \"DAY_OF_WEEK\": day_of_week,\n",
    "        \"SALIDA_SIN\": salida_sin,\n",
    "        \"SALIDA_COS\": salida_cos,\n",
    "        \"MONTH_SIN\": month_sin,\n",
    "        \"MONTH_COS\": month_cos,\n",
    "        # DISTANCIA_HAV deber√≠a venir precalculada en back si tienes lat/lon; si no, dejar 0 o estimar\n",
    "        \"DISTANCIA_HAV\": 0.0,  \n",
    "        \"RUTA_TE\": float(mappings[\"RUTA\"].get(ruta, defaults[\"RUTA\"])),\n",
    "        \"AIRLINE_TE\": float(mappings[\"AIRLINE\"].get(airline, defaults[\"AIRLINE\"])),\n",
    "        \"ORIGIN_AIRPORT_TE\": float(mappings[\"ORIGIN_AIRPORT\"].get(origin, defaults[\"ORIGIN_AIRPORT\"])),\n",
    "        \"DESTINATION_AIRPORT_TE\": float(mappings[\"DESTINATION_AIRPORT\"].get(destination, defaults[\"DESTINATION_AIRPORT\"]))\n",
    "    }\n",
    "    return pd.DataFrame([fila])[list(model_te.feature_name_)]\n",
    "\n",
    "def predecir_prob_retraso_api(df_features, modelo=model_te, thr=UMBRAL_OPERATIVO):\n",
    "    proba = float(modelo.predict_proba(df_features)[:,1][0])\n",
    "    return {\"prob_delay\": proba, \"delayed\": int(proba >= thr)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62839ad",
   "metadata": {},
   "source": [
    "## revisi√≥n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fce5d",
   "metadata": {},
   "source": [
    "modelo champion (LightGBM + LabelEncoder) con validaci√≥n temporal, tuning ligero, elecci√≥n de umbral y guardado de artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de3547",
   "metadata": {},
   "source": [
    "1) Carga optimizada desde CSV (solo columnas + dtypes chicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da44dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Funci√≥n Haversine (km) ---\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# --- Chequeos m√≠nimos (ya existen en tu CSV seg√∫n lo que enviaste) ---\n",
    "cols_req = [\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\"MONTH\",\"DAY_OF_WEEK\",\n",
    "            \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RETRASADO_LLEGADA\",\n",
    "            \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\"]\n",
    "faltan = [c for c in cols_req if c not in v.columns]\n",
    "if faltan:\n",
    "    raise ValueError(\"Faltan columnas base en v: \" + \", \".join(faltan))\n",
    "\n",
    "# 1) Distancia Haversine (si no existe)\n",
    "if \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"],\n",
    "                                      v[\"DEST_LAT\"],   v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# 2) Estacionalidad del mes (seno/coseno) (si no existen)\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# 3) Minuto del d√≠a de salida (si no existe)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    # Ya tienes HORA_SALIDA y MIN_SALIDA en tu CSV\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "\n",
    "# (Opcional) ruta texto (√∫til para an√°lisis; para modelar, mejor encoding posterior)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Dtypes compactos para ahorro de RAM\n",
    "v[\"RETRASADO_LLEGADA\"] = v[\"RETRASADO_LLEGADA\"].astype(\"int8\")\n",
    "v[\"MONTH\"] = v[\"MONTH\"].astype(\"int8\")\n",
    "v[\"DAY_OF_WEEK\"] = v[\"DAY_OF_WEEK\"].astype(\"int8\")\n",
    "v[\"DISTANCIA_HAV\"] = v[\"DISTANCIA_HAV\"].astype(\"float32\")\n",
    "v[\"SALIDA_SIN\"] = v[\"SALIDA_SIN\"].astype(\"float32\")\n",
    "v[\"SALIDA_COS\"] = v[\"SALIDA_COS\"].astype(\"float32\")\n",
    "v[\"MONTH_SIN\"] = v[\"MONTH_SIN\"].astype(\"float32\")\n",
    "v[\"MONTH_COS\"] = v[\"MONTH_COS\"].astype(\"float32\")\n",
    "\n",
    "print(\"‚úÖ Columnas derivadas listas. v.shape:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d3c7",
   "metadata": {},
   "source": [
    "Selecci√≥n de variables (features) y salida X, y\n",
    "\n",
    "Incluye las nuevas columnas; deja listas para el siguiente paso (split temporal + encoding + fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de entrada para el modelo\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# Categ√≥ricas crudas (luego se codifican con LabelEncoder o category.codes)\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# Num√©ricas/derivadas\n",
    "num_cols = [\n",
    "    \"MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\", \"SALIDA_COS\",\n",
    "    \"MONTH_SIN\", \"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\"\n",
    "]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "\n",
    "# Validar presencia\n",
    "missing = [c for c in features + [target] if c not in v.columns]\n",
    "if missing:\n",
    "    raise ValueError(\"Faltan columnas para features/target: \" + \", \".join(missing))\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(int)\n",
    "\n",
    "print(\"‚úÖ Features y target listos.\")\n",
    "print(\"X.shape:\", X.shape, \"| y.mean (rate retraso):\", y.mean().round(4))\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811818a7",
   "metadata": {},
   "source": [
    "3) Split temporal (train: meses 1‚Äì9, valid: 10‚Äì12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ba8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos v[\"MONTH\"] para crear los masks y luego cortamos X,y sin hacer copias grandes\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask]\n",
    "y_train = y.loc[train_mask].astype(\"int8\")\n",
    "\n",
    "X_valid = X.loc[valid_mask]\n",
    "y_valid = y.loc[valid_mask].astype(\"int8\")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Valid:\", X_valid.shape,\n",
    "      \"| Rate train:\", float(y_train.mean()),\n",
    "      \"| Rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b634f",
   "metadata": {},
   "source": [
    "4) Codificaci√≥n de categ√≥ricas (LabelEncoder) solo con train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc885a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "encoders = {}\n",
    "\n",
    "# Ajuste en train y transformaci√≥n consistente en valid\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[c] = le.fit_transform(X_train[c].astype(str))\n",
    "    X_valid[c] = le.transform(X_valid[c].astype(str))\n",
    "    encoders[c] = le\n",
    "\n",
    "# Tipos compactos\n",
    "X_train = X_train.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "X_valid = X_valid.astype({\n",
    "    \"AIRLINE\":\"int16\",\"ORIGIN_AIRPORT\":\"int16\",\"DESTINATION_AIRPORT\":\"int16\",\n",
    "    \"MONTH\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"MINUTO_DIA_SALIDA\":\"int16\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\"MONTH_SIN\":\"float32\",\"MONTH_COS\":\"float32\",\"DISTANCIA_HAV\":\"float32\"\n",
    "})\n",
    "\n",
    "X_train.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595cf824",
   "metadata": {},
   "source": [
    "5) Entrenamiento LightGBM con early stopping (validaci√≥n temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb, time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    # class_weight=\"balanced\",   # o comenta y usa scale_pos_weight=scale_pos_weight\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"‚úÖ Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28654e3",
   "metadata": {},
   "source": [
    "6) M√©tricas a 0.5 y b√∫squeda de umbral (mejor F1 y ‚Äúprecisi√≥n m√≠nima‚Äù)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# 6.1 Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# 6.2 Mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e93e34",
   "metadata": {},
   "source": [
    "(Opcional si Operaciones pide precisi√≥n m√≠nima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 M√°ximo Recall con Precisi√≥n m√≠nima (ajusta min_prec)\n",
    "min_prec = 0.60\n",
    "best_rec = {\"thr\":0.5, \"rec\":-1, \"pre\":0}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    pre = precision_score(y_valid, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_valid, y_hat, zero_division=0)\n",
    "    if pre >= min_prec and rec > best_rec[\"rec\"]:\n",
    "        best_rec = {\"thr\":float(thr), \"rec\":float(rec), \"pre\":float(pre)}\n",
    "if best_rec[\"rec\"] > 0:\n",
    "    best_rec_res = report_metrics(y_valid, valid_proba, best_rec[\"thr\"], f\"Max Recall con Prec ‚â• {min_prec:.2f}\")\n",
    "    best_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391b2dc",
   "metadata": {},
   "source": [
    "7) Guardar artefactos (modelo + encoders + umbral + metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])  # o fija el que defina negocio\n",
    "\n",
    "dump(model, \"models/lgbm_delay_champion.joblib\")\n",
    "dump(encoders, \"models/label_encoders.joblib\")\n",
    "with open(\"models/threshold.json\",\"w\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train.columns),\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"note\": \"LightGBM + LabelEncoder + temporal split (features derivadas activadas)\"\n",
    "}\n",
    "with open(\"models/metadata.json\",\"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Guardados: modelo, encoders, threshold y metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac669c",
   "metadata": {},
   "source": [
    "8) (Opcional) Curvas ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc as sk_auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_valid, valid_proba)\n",
    "plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_valid, valid_proba):.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',c='grey'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC - Validaci√≥n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_valid, valid_proba)\n",
    "prauc = sk_auc(rec, prec)\n",
    "plt.figure(); plt.plot(rec, prec, label=f\"PR AUC={prauc:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall - Validaci√≥n temporal\"); plt.grid(alpha=.3); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7517c08",
   "metadata": {},
   "source": [
    "### **Revisi√≥n 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2daa7",
   "metadata": {},
   "source": [
    "guion completo para pasar de flights_clean.csv a un modelo LightGBM con Target Encoding KFold (TE) en AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT y RUTA, con split temporal (1‚Äì9 vs 10‚Äì12), b√∫squeda de umbral operativo, y guardado de artefactos (modelo, mappings TE, threshold, metadata) + una funci√≥n de predicci√≥n para producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9a80e",
   "metadata": {},
   "source": [
    "Paso 0 ¬∑ Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")\n",
    "# print(\"Leyendo:\", os.path.abspath(DATA_PATH))\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "print(\"üìÑ Usando archivo:\", DATA_PATH)\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"‚úÖ Cargado {v.shape} en {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbff90a",
   "metadata": {},
   "source": [
    "Paso 1 ¬∑ Carga del CSV (solo columnas √∫tiles + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que vamos a usar en el pipeline\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"HORA_SALIDA\",\"MIN_SALIDA\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# Ver qu√© hay realmente en el archivo y cargar s√≥lo lo que exista\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "if missing: print(\"‚ö†Ô∏è Faltan en CSV (no pasa nada, se calcular√°n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"HORA_SALIDA\":\"int8\",\"MIN_SALIDA\":\"int8\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"‚úÖ Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ff53d",
   "metadata": {},
   "source": [
    "Paso 2 ¬∑ Derivar columnas que faltan (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# Distancia Haversine\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "if \"MONTH\" in v.columns and \"MONTH_COS\" not in v.columns:\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto del d√≠a de salida\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
    "    if {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(v.columns):\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (v[\"HORA_SALIDA\"]*60 + v[\"MIN_SALIDA\"]).astype(\"int16\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "        ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "        v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "\n",
    "# Ruta (texto)\n",
    "if \"RUTA\" not in v.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "# Normalizaci√≥n de tipos\n",
    "to_float32 = [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]\n",
    "for c in to_float32:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "for c in [\"MONTH\",\"DAY_OF_WEEK\",\"RETRASADO_LLEGADA\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"int8\")\n",
    "\n",
    "print(\"‚úÖ Derivadas listas | columnas:\", len(v.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09913e8",
   "metadata": {},
   "source": [
    "Paso 3 ¬∑ Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8303a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "missing_feats = [c for c in cat_cols + num_cols if c not in v.columns]\n",
    "if missing_feats: print(\"‚ö†Ô∏è Faltaron features (se omiten):\", missing_feats)\n",
    "\n",
    "X = v[features]\n",
    "y = v[target].astype(\"int8\")\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303c6b9",
   "metadata": {},
   "source": [
    "Paso 4 ¬∑ Split temporal (train 1‚Äì9, valid 10‚Äì12) + copias seguras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38360397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598da970",
   "metadata": {},
   "source": [
    "Paso 5 ¬∑ Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 5 (FIX) ¬∑ Target Encoding KFold (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA) ===\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kfold_target_encode(x_col: pd.Series, y: pd.Series, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - x_col: Series (columna categ√≥rica) indexada igual que y\n",
    "      - y:     Series binaria (0/1) indexada igual que x_col\n",
    "    Devuelve:\n",
    "      - enc_train: Series con el encoding para cada fila del train (out-of-fold)\n",
    "      - mapping:   dict categor√≠a -> valor TE (para aplicar en valid/test/producci√≥n)\n",
    "      - global_mean: float con la media global del target (fallback)\n",
    "    \"\"\"\n",
    "    assert x_col.index.equals(y.index), \"x_col y y deben estar alineados por √≠ndice\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=y.index, dtype=\"float32\")\n",
    "    global_mean = float(y.mean())\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(x_col, y):\n",
    "        # construimos df temporal con col + target para este fold\n",
    "        df_tr = pd.DataFrame({\"col\": x_col.iloc[tr_idx].astype(str), \"y\": y.iloc[tr_idx].astype(int)})\n",
    "        df_val = pd.DataFrame({\"col\": x_col.iloc[val_idx].astype(str)})\n",
    "\n",
    "        stats = df_tr.groupby(\"col\")[\"y\"].mean()\n",
    "        cnts  = df_tr.groupby(\"col\")[\"y\"].size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = df_val[\"col\"].map(smoothed).fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar fuera del train)\n",
    "    df_full = pd.DataFrame({\"col\": x_col.astype(str), \"y\": y.astype(int)})\n",
    "    stats_f = df_full.groupby(\"col\")[\"y\"].mean()\n",
    "    cnts_f  = df_full.groupby(\"col\")[\"y\"].size()\n",
    "    mapping = ((stats_f*cnts_f + global_mean*smoothing) / (cnts_f + smoothing)).astype(\"float32\").to_dict()\n",
    "\n",
    "    return enc.astype(\"float32\"), mapping, global_mean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "# --- columnas a codificar (s√≥lo las que existan en X_train) ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "\n",
    "# Trabajamos sobre copias para evitar warnings\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "mappings, defaults = {}, {}\n",
    "for c in cols_te:\n",
    "    enc_train, mapping, default = kfold_target_encode(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_train\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Modelo us\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c27e0",
   "metadata": {},
   "source": [
    "Ese NameError indica que no se cre√≥ X_train_model/X_valid_model (normalmente porque la celda del Paso 5 ‚Äì Target Encoding no se ejecut√≥ o fall√≥ a mitad). Deja una celda puente antes del entrenamiento que:\n",
    "\n",
    "Verifica que corriste los pasos 3‚Äì5.\n",
    "\n",
    "Si hay TE, arma X_train_model/X_valid_model.\n",
    "\n",
    "Da mensajes claros si falta algo.\n",
    "\n",
    "Pega y ejecuta esto justo antes del Paso 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95170f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Puente de seguridad antes de entrenar (Paso 6) ---\n",
    "# Requiere X_train, X_valid, y_train, y_valid de los pasos 3‚Äì4\n",
    "assert 'X_train' in globals() and 'X_valid' in globals(), \"Falta correr el Paso 3‚Äì4 (definir X_train/X_valid).\"\n",
    "assert 'y_train' in globals() and 'y_valid' in globals(), \"Falta correr el Paso 3‚Äì4 (definir y_train/y_valid).\"\n",
    "\n",
    "# Columnas categ√≥ricas originales que se codifican con TE\n",
    "cols_te_base = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "cols_te = [c for c in cols_te_base if c in X_train.columns or f\"{c}_TE\" in X_train.columns]\n",
    "\n",
    "# ¬øexisten columnas TE? (AIRLINE_TE, etc.)\n",
    "te_cols_present = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" in X_train.columns]\n",
    "if len(te_cols_present) != len(cols_te):\n",
    "    faltan = [f\"{c}_TE\" for c in cols_te if f\"{c}_TE\" not in X_train.columns]\n",
    "    raise ValueError(\n",
    "        \"Faltan columnas de Target Encoding. \"\n",
    "        f\"No se ejecut√≥ (o fall√≥) el Paso 5.\\nFaltan: {faltan}\"\n",
    "    )\n",
    "\n",
    "# Construir matrices finales: todas las num√©ricas + columnas _TE; quitamos las categ√≥ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te_base if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te_base if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"‚úÖ Listo para entrenar\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas ejemplo:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0983568",
   "metadata": {},
   "source": [
    "Paso 6 ¬∑ Entrenamiento LightGBM con early stopping (balanceado por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=5.0,\n",
    "    scale_pos_weight=scale_pos_weight,  # en vez de class_weight\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"‚úÖ Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59d392",
   "metadata": {},
   "source": [
    "Paso 7 ¬∑ M√©tricas (0.5, mejor F1) y umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy={acc:.4f} | Precision={pre:.4f} | Recall={rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_res = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (b√∫squeda gruesa)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9520168",
   "metadata": {},
   "source": [
    "Paso 8 ¬∑ Guardar artefactos (modelo, mappings TE, defaults y threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_f1_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "# Guardamos mappings y defaults de TE\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = {\n",
    "    \"features\": list(X_train_model.columns),\n",
    "    \"cols_te\": cols_te,\n",
    "    \"auc_valid\": float(auc_val),\n",
    "    \"split\": {\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    \"scale_pos_weight\": float(scale_pos_weight),\n",
    "    \"note\": \"LightGBM + KFold Target Encoding (AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA)\"\n",
    "}\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0cbfe",
   "metadata": {},
   "source": [
    "Paso 9 ¬∑ (Producci√≥n) Funci√≥n de predicci√≥n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar artefactos (en tu API/servicio)\n",
    "from joblib import load\n",
    "import json, numpy as np, math\n",
    "\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f: te_mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f: te_defaults = json.load(f)\n",
    "with open(\"models/threshold.json\",\"r\",encoding=\"utf-8\") as f: UMBRAL = json.load(f)[\"threshold\"]\n",
    "\n",
    "def prep_features_for_inference(\n",
    "    month:int, day_of_week:int,\n",
    "    airline:str, origin:str, destination:str,\n",
    "    scheduled_hour:int, scheduled_minute:int,\n",
    "    origen_lat:float, origen_lon:float, dest_lat:float, dest_lon:float\n",
    "):\n",
    "    # c√≠clicos\n",
    "    salida_min = int(scheduled_hour)*60 + int(scheduled_minute)\n",
    "    salida_sin = math.sin(2*math.pi*salida_min/(24*60))\n",
    "    salida_cos = math.cos(2*math.pi*salida_min/(24*60))\n",
    "    month_sin = math.sin(2*math.pi*month/12)\n",
    "    month_cos = math.cos(2*math.pi*month/12)\n",
    "\n",
    "    # haversine\n",
    "    def haversine_km(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1 = math.radians(lat1); lon1 = math.radians(lon1)\n",
    "        lat2 = math.radians(lat2); lon2 = math.radians(lon2)\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist_hav = haversine_km(origen_lat, origen_lon, dest_lat, dest_lon)\n",
    "    ruta = f\"{origin}_{destination}\"\n",
    "\n",
    "    # aplicar TE (map -> valor; si no existe categor√≠a, usar default global)\n",
    "    def te(col, val):\n",
    "        mp = te_mappings[col]; default = te_defaults[col]\n",
    "        return float(mp.get(str(val), default))\n",
    "\n",
    "    row = {\n",
    "        \"MONTH\": np.int8(month),\n",
    "        \"DAY_OF_WEEK\": np.int8(day_of_week),\n",
    "        \"SALIDA_SIN\": np.float32(salida_sin),\n",
    "        \"SALIDA_COS\": np.float32(salida_cos),\n",
    "        \"MONTH_SIN\": np.float32(month_sin),\n",
    "        \"MONTH_COS\": np.float32(month_cos),\n",
    "        \"DISTANCIA_HAV\": np.float32(dist_hav),\n",
    "        \"MINUTO_DIA_SALIDA\": np.int16(salida_min),\n",
    "        # TE\n",
    "        \"AIRLINE_TE\": np.float32(te(\"AIRLINE\", airline)),\n",
    "        \"ORIGIN_AIRPORT_TE\": np.float32(te(\"ORIGIN_AIRPORT\", origin)),\n",
    "        \"DESTINATION_AIRPORT_TE\": np.float32(te(\"DESTINATION_AIRPORT\", destination)),\n",
    "        \"RUTA_TE\": np.float32(te(\"RUTA\", ruta)),\n",
    "    }\n",
    "\n",
    "    # El orden de columnas debe coincidir con X_train_model.columns\n",
    "    cols = list(c for c in model.feature_name_)\n",
    "    Xrow = np.array([[row[c] for c in cols]], dtype=np.float32)\n",
    "    return Xrow\n",
    "\n",
    "def predict_delay_prob(\n",
    "    month, day_of_week, airline, origin, destination,\n",
    "    scheduled_hour, scheduled_minute,\n",
    "    origen_lat, origen_lon, dest_lat, dest_lon\n",
    "):\n",
    "    Xrow = prep_features_for_inference(\n",
    "        month, day_of_week, airline, origin, destination,\n",
    "        scheduled_hour, scheduled_minute, origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    )\n",
    "    proba = float(model.predict_proba(Xrow)[0,1])\n",
    "    delayed = proba >= UMBRAL\n",
    "    return {\"prob_delay\": round(proba,4), \"delayed\": bool(delayed), \"threshold\": UMBRAL}\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# predict_delay_prob(7, 4, \"DL\", \"ATL\", \"JFK\", 14, 30, 33.64, -84.43, 40.64, -73.78)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b547da0",
   "metadata": {},
   "source": [
    "### Revisi√≥n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bb55d",
   "metadata": {},
   "source": [
    "0. Encabezado y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75909967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0 ¬∑ Imports, ruta y helpers\n",
    "import os, time, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb26ea",
   "metadata": {},
   "source": [
    "1. Carga del CSV (eficiente: usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1 ¬∑ Carga del CSV (solo columnas √∫tiles + dtypes compactos)\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")  # ajusta si lo tienes en otra ruta\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# leer solo el header para ver qu√© hay\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"Columnas cargadas:\", present, \"\\nFaltantes (se derivan si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"‚úÖ Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae384338",
   "metadata": {},
   "source": [
    "2. Derivar columnas que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2 ¬∑ Derivaci√≥n de features faltantes\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# Minuto del d√≠a (si no vino ya)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# Ruta (texto)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"‚úÖ Derivadas OK | columnas:\", len(v.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b124f86",
   "metadata": {},
   "source": [
    "3. Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb86f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3 ¬∑ Selecci√≥n de variables\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c3b6a",
   "metadata": {},
   "source": [
    "4. Split temporal (train: meses 1‚Äì9, valid: 10‚Äì12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4 ¬∑ Split temporal (evita fuga)\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dd98a",
   "metadata": {},
   "source": [
    "5. Target Encoding KFold (sin fuga) para categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Target Encoding KFold ROBUSTO (sin fuga) + armado de matrices\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) PRERREQUISITOS / ALINEACIONES ---\n",
    "# Deben existir de pasos previos:\n",
    "#   v, X, y, X_train, X_valid, y_train, y_valid\n",
    "for obj_name in [\"X_train\", \"X_valid\", \"y_train\", \"y_valid\"]:\n",
    "    assert obj_name in globals(), f\"Falta {obj_name}. Ejecuta los pasos previos.\"\n",
    "\n",
    "# Asegurar que y_train/y_valid est√°n alineados con X_train/X_valid\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_valid = y_valid.loc[X_valid.index]\n",
    "\n",
    "# Crear RUTA si no existe\n",
    "if \"RUTA\" not in X_train.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(X_train.columns):\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    X_train[\"RUTA\"] = (X_train[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_train[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    X_valid[\"RUTA\"] = (X_valid[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_valid[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Columnas a codificar (solo si existen)\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "print(\"TE sobre:\", cols_te)\n",
    "\n",
    "# --- 1) Funciones TE robustas (operan con Series) ---\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42) -> tuple[pd.Series, dict, float]:\n",
    "    \"\"\"\n",
    "    s: Serie categ√≥rica (mismo √≠ndice que y)\n",
    "    y: Serie binaria 0/1 (mismo √≠ndice que s)\n",
    "    Devuelve:\n",
    "      enc     -> Serie con el encoding KFold para s (alineada a s.index)\n",
    "      mapping -> dict valor_categoria -> encoding_final (con TODO el train)\n",
    "      gmean   -> media global (fallback)\n",
    "    \"\"\"\n",
    "    # Alineaci√≥n defensiva por √≠ndice\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx]\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    # Normalizaci√≥n de tipos\n",
    "    s = s.astype(\"string\")  # evita NaNs tipo objeto raros\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    # Mapping final con TODO el train (para producci√≥n/valid)\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float) -> pd.Series:\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "# --- 2) Ejecutar TE ---\n",
    "mappings, defaults = {}, {}\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid[f\"{c}_TE\"] = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "print(\"‚úÖ TE aplicado sin fuga.\")\n",
    "print(\"Ejemplo TE:\", {k: list(v)[:2] if hasattr(v, \"__iter__\") else v for k,v in list(mappings.items())[:1]})\n",
    "\n",
    "# --- 3) Construir matrices finales: quitamos las categor√≠as crudas ---\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"Listo para entrenar:\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas (primeras 12):\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3435a",
   "metadata": {},
   "source": [
    "6. Entrenamiento LightGBM (early stopping, balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Chequeos r√°pidos\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "print(\"Shapes:\", X_train_model.shape, X_valid_model.shape)\n",
    "\n",
    "# Balanceo por proporci√≥n de clases (pos/neg)\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    # usa uno u otro balanceo (recomiendo este):\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"‚úÖ Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5051f6",
   "metadata": {},
   "source": [
    "7) M√©tricas base (0.5) + b√∫squeda de mejor umbral (por F1) y matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (b√∫squeda simple)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627403",
   "metadata": {},
   "source": [
    "### revisi√≥n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf6512",
   "metadata": {},
   "source": [
    "agregados hist√≥ricos + reentrenamiento con LGBM y comparaci√≥n con el modelo anterior\n",
    "\n",
    "agregar se√±ales hist√≥ricas sin fuga (congesti√≥n por ruta y hora), re-entrenar LightGBM y comparar contra tu baseline. \n",
    "\n",
    "Est√° pensado para ejecutarlo despu√©s de tu pipeline anterior (ya se debe tener v, X_train, X_valid, y_train, y_valid, X_train_model, X_valid_model listos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee88653",
   "metadata": {},
   "source": [
    "1) Agregados hist√≥ricos (solo con TRAIN 1‚Äì9) ‚Üí sin fuga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 1: Carga eficiente con tus nombres + fallbacks seguros ===\n",
    "import time, numpy as np, pandas as pd\n",
    "\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "\n",
    "# Fallbacks de carga (para poder derivar luego):\n",
    "# 1) Si no viene RETRASADO_LLEGADA, intenta traer ARRIVAL_DELAY para crearlo.\n",
    "if \"RETRASADO_LLEGADA\" not in present and \"ARRIVAL_DELAY\" in header:\n",
    "    present.append(\"ARRIVAL_DELAY\")\n",
    "\n",
    "# 2) Si no vienen SALIDA_SIN/COS, con SCHEDULED_DEPARTURE podemos derivarlas (ya est√° en present).\n",
    "# 3) Si faltan lat/lon y existen equivalentes en tu archivo, a√±ade aqu√≠ sus alias si aplica.\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"ARRIVAL_DELAY\":\"float32\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(\"‚Üí Cargado:\", v.shape, \"| en\", f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "# === Derivaciones m√≠nimas para completar faltantes ===\n",
    "\n",
    "# Target desde ARRIVAL_DELAY si no vino RETRASADO_LLEGADA\n",
    "if \"RETRASADO_LLEGADA\" not in v.columns:\n",
    "    if \"ARRIVAL_DELAY\" not in v.columns:\n",
    "        raise ValueError(\"No hay RETRASADO_LLEGADA ni ARRIVAL_DELAY para derivarlo.\")\n",
    "    v[\"RETRASADO_LLEGADA\"] = (v[\"ARRIVAL_DELAY\"] > 15).astype(\"int8\")\n",
    "\n",
    "# SALIDA_SIN/COS desde SCHEDULED_DEPARTURE si faltan\n",
    "if (\"SALIDA_SIN\" not in v.columns or \"SALIDA_COS\" not in v.columns) and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    minuto = (hs*60 + ms).astype(\"int16\")\n",
    "    v[\"SALIDA_SIN\"] = np.sin(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "    v[\"SALIDA_COS\"] = np.cos(2*np.pi*minuto/(24*60)).astype(\"float32\")\n",
    "\n",
    "# Distancia Haversine (opcional) si m√°s adelante la quieres usar y tienes lat/lon\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(v[\"ORIGEN_LAT\"]); lon1 = np.radians(v[\"ORIGEN_LON\"])\n",
    "    lat2 = np.radians(v[\"DEST_LAT\"]);   lon2 = np.radians(v[\"DEST_LON\"])\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    v[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Estacionalidad del mes (opcional)\n",
    "if \"MONTH\" in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# RUTA (√∫til para Target Encoding)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(f\"‚úÖ Listo: rate={v['RETRASADO_LLEGADA'].mean():.4f} | cols={len(v.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e5160",
   "metadata": {},
   "source": [
    "Paso 2 ¬∑ Split temporal (train=meses 1‚Äì9, valid=10‚Äì12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el DataFrame v cargado en el Paso 1\n",
    "assert \"MONTH\" in v.columns and \"RETRASADO_LLEGADA\" in v.columns, \"Faltan MONTH o RETRASADO_LLEGADA.\"\n",
    "\n",
    "# Variables base (ajusta si quieres usar m√°s adelante otras)\n",
    "# Si ya creaste RUTA en el Paso 1, se usar√°; si no existe, la generamos aqu√≠.\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "cat_cols = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in v.columns]\n",
    "num_cols = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "                        \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"] if c in v.columns]\n",
    "\n",
    "features = cat_cols + num_cols\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split ‚Üí\",\n",
    "      \"X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b3ff8",
   "metadata": {},
   "source": [
    "Paso 3 ¬∑ Target Encoding KFold (sin fuga) para categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1dfa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42):\n",
    "    # Alineaci√≥n defensiva\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx].astype(\"string\")\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = cat_cols[:]  # todas las categ√≥ricas disponibles\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "# Copias para no tocar los originales\n",
    "X_train_te = X_train.copy()\n",
    "X_valid_te = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train_te[c], y_train,\n",
    "                                                          n_splits=5, smoothing=50, seed=42)\n",
    "    X_train_te[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid_te[f\"{c}_TE\"]  = apply_te(X_valid_te[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# Matrices finales de entrenamiento (quitamos las columnas categ√≥ricas crudas)\n",
    "X_train_model = X_train_te.drop(columns=cols_te).copy()\n",
    "X_valid_model = X_valid_te.drop(columns=cols_te).copy()\n",
    "\n",
    "print(\"‚úÖ TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7828e",
   "metadata": {},
   "source": [
    "(Opcional fuerte) Paso 4 ¬∑ Agregados hist√≥ricos RUTA√óHORA (sin fuga)\n",
    "\n",
    "Si quieres mejorar precisi√≥n/recall, agrega estad√≠sticos hist√≥ricos usando solo train (meses 1‚Äì9) y mapea a train/valid. Si no lo necesitas ahora, salta al paso 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "assert \"SCHEDULED_DEPARTURE\" in v.columns, \"Requiere SCHEDULED_DEPARTURE para obtener HORA_SALIDA.\"\n",
    "HORA_all = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "RUTA_all = v[\"RUTA\"].astype(str) if \"RUTA\" in v.columns else (\n",
    "    v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    ")\n",
    "\n",
    "train_mask_all = v[\"MONTH\"].between(1, 9)\n",
    "\n",
    "aux = pd.DataFrame({\n",
    "    \"RUTA\": RUTA_all[train_mask_all].values,\n",
    "    \"HORA_SALIDA\": HORA_all[train_mask_all].values,\n",
    "    \"RETRASADO_LLEGADA\": v.loc[train_mask_all, \"RETRASADO_LLEGADA\"].values\n",
    "})\n",
    "# Si existe ARRIVAL_DELAY, incluimos media de retraso\n",
    "if \"ARRIVAL_DELAY\" in v.columns:\n",
    "    aux[\"ARRIVAL_DELAY\"] = v.loc[train_mask_all, \"ARRIVAL_DELAY\"].values\n",
    "\n",
    "agg_dict = {\"rate_delay\": (\"RETRASADO_LLEGADA\",\"mean\"),\n",
    "            \"n\": (\"RETRASADO_LLEGADA\",\"size\")}\n",
    "if \"ARRIVAL_DELAY\" in aux.columns:\n",
    "    agg_dict[\"mean_arr_delay\"] = (\"ARRIVAL_DELAY\",\"mean\")\n",
    "\n",
    "agg_ruta_hora = (aux\n",
    "    .groupby([\"RUTA\",\"HORA_SALIDA\"], observed=True)\n",
    "    .agg(**agg_dict)\n",
    "    .reset_index())\n",
    "\n",
    "g_rate = float(agg_ruta_hora[\"rate_delay\"].mean())\n",
    "g_n    = 0.0\n",
    "g_mean_arr = float(agg_ruta_hora[\"mean_arr_delay\"].mean()) if \"mean_arr_delay\" in agg_ruta_hora.columns else 0.0\n",
    "\n",
    "def add_route_hour_stats(X_in, idx):\n",
    "    tmp = pd.DataFrame({\n",
    "        \"RUTA\": RUTA_all.loc[idx].astype(str).values,\n",
    "        \"HORA_SALIDA\": HORA_all.loc[idx].astype(\"int16\").values\n",
    "    }, index=idx)\n",
    "    merged = tmp.merge(agg_ruta_hora, on=[\"RUTA\",\"HORA_SALIDA\"], how=\"left\")\n",
    "    X_in[\"RUTA_HORA_rate\"] = merged[\"rate_delay\"].fillna(g_rate).astype(\"float32\").values\n",
    "    X_in[\"RUTA_HORA_n\"]    = merged[\"n\"].fillna(g_n).astype(\"float32\").values\n",
    "    if \"mean_arr_delay\" in merged.columns:\n",
    "        X_in[\"RUTA_HORA_mean_arr\"] = merged[\"mean_arr_delay\"].fillna(g_mean_arr).astype(\"float32\").values\n",
    "    return X_in\n",
    "\n",
    "X_train_model = add_route_hour_stats(X_train_model.copy(), X_train_model.index)\n",
    "X_valid_model = add_route_hour_stats(X_valid_model.copy(), X_valid_model.index)\n",
    "\n",
    "print(\"‚úî Agregados a√±adidos:\",\n",
    "      [c for c in X_train_model.columns if c.startswith(\"RUTA_HORA_\")],\n",
    "      \"| tiempo:\", f\"{time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2668c",
   "metadata": {},
   "source": [
    "Paso 5 ¬∑ Entrenamiento con LightGBM (early stopping + balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"‚úÖ Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd192744",
   "metadata": {},
   "source": [
    "Paso 6 ¬∑ M√©tricas y selecci√≥n de umbral operativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (puedes restringir por precisi√≥n m√≠nima si tu negocio lo requiere)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], f\"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c1e68",
   "metadata": {},
   "source": [
    "Paso 7 ¬∑ Guardado de artefactos (modelo + TE + columnas + umbral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from joblib import dump\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Umbral operativo (elige el que te convenga: best[\"thr\"] o un umbral de negocio)\n",
    "UMBRAL_OPERATIVO = float(best[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:{str(kk):float(vv) for kk,vv in v.items()} for k,v in mappings.items()}, f)\n",
    "with open(\"models/te_defaults.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({k:float(v) for k,v in defaults.items()}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cat_encoded=cols_te,\n",
    "    split=\"temporal: train 1-9, valid 10-12\",\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    auc_valid=float(auc_val),\n",
    "    threshold=float(UMBRAL_OPERATIVO),\n",
    "    notes=\"LightGBM con TE KFold (sin fuga)\" + (\" + agregados RUTA√óHORA\" if any(c.startswith(\"RUTA_HORA_\") for c in X_train_model.columns) else \"\")\n",
    ")\n",
    "with open(\"models/metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"üíæ Guardado: models/lgbm_delay.joblib, te_mappings.json, te_defaults.json, metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91db1cb",
   "metadata": {},
   "source": [
    "Paso 8 ¬∑ Funci√≥n de inferencia (para API / dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from joblib import load\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay.joblib\")\n",
    "with open(\"models/te_mappings.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    mappings = json.load(f)\n",
    "with open(\"models/te_defaults.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    defaults = json.load(f)\n",
    "with open(\"models/metadata.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "FEATURES = meta[\"features\"]\n",
    "THRESHOLD = float(meta[\"threshold\"])\n",
    "\n",
    "def infer_delay(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"df_input: columnas m√≠nimas: AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, MONTH, DAY_OF_WEEK, SCHEDULED_DEPARTURE (+ lat/lon si quieres DISTANCIA_HAV).\n",
    "       Devuelve: proba_delay y pred (0/1) usando THRESHOLD guardado.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in df.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(df.columns):\n",
    "        df[\"RUTA\"] = df[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + df[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # Derivar MINUTO_DIA_SALIDA / SALIDA_SIN/COS si hace falta\n",
    "    if \"MINUTO_DIA_SALIDA\" not in df.columns and \"SCHEDULED_DEPARTURE\" in df.columns:\n",
    "        hs = (df[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "        ms = (df[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "        df[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "    if \"SALIDA_SIN\" not in df.columns and \"MINUTO_DIA_SALIDA\" in df.columns:\n",
    "        m = df[\"MINUTO_DIA_SALIDA\"].astype(\"float32\")\n",
    "        df[\"SALIDA_SIN\"] = np.sin(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "        df[\"SALIDA_COS\"] = np.cos(2*np.pi*m/(24*60)).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN/COS\n",
    "    if \"MONTH\" in df.columns:\n",
    "        df[\"MONTH_SIN\"] = np.sin(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "        df[\"MONTH_COS\"] = np.cos(2*np.pi*df[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    # (Opcional) DISTANCIA_HAV si hay lat/lon\n",
    "    if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(df.columns) and \"DISTANCIA_HAV\" not in df.columns:\n",
    "        R = 6371.0\n",
    "        lat1 = np.radians(df[\"ORIGEN_LAT\"]); lon1 = np.radians(df[\"ORIGEN_LON\"])\n",
    "        lat2 = np.radians(df[\"DEST_LAT\"]);   lon2 = np.radians(df[\"DEST_LON\"])\n",
    "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "        df[\"DISTANCIA_HAV\"] = (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "    # Target Encoding con mappings guardados\n",
    "    for c, mapping in mappings.items():\n",
    "        default = float(defaults.get(c, df_input.get(c, pd.Series([],dtype='object')).mean() if c in df_input.columns else 0.0))\n",
    "        df[f\"{c}_TE\"] = df[c].astype(\"string\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "    # Construir matriz final con las FEATURES del entrenamiento\n",
    "    X_inf = df.reindex(columns=FEATURES, fill_value=0).copy()\n",
    "\n",
    "    proba = model.predict_proba(X_inf)[:,1]\n",
    "    pred  = (proba >= THRESHOLD).astype(int)\n",
    "    out = df_input.copy()\n",
    "    out[\"proba_delay\"] = proba\n",
    "    out[\"pred_delay\"]  = pred\n",
    "    return out\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# infer_delay(pd.DataFrame([{\n",
    "#   \"AIRLINE\":\"AA\",\"ORIGIN_AIRPORT\":\"JFK\",\"DESTINATION_AIRPORT\":\"LAX\",\"MONTH\":7,\"DAY_OF_WEEK\":5,\"SCHEDULED_DEPARTURE\": 1730\n",
    "# }]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c585",
   "metadata": {},
   "source": [
    "### Revisi√≥n 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b79497",
   "metadata": {},
   "source": [
    "plit temporal (1‚Äì9 vs 10‚Äì12), Target Encoding KFold sin fuga, agregados hist√≥ricos sin fuga, entrena LightGBM con early stopping y guarda artefactos (modelo, mapeos TE y threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c42b74",
   "metadata": {},
   "source": [
    "Paso 0 ¬∑ Importaciones y ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 0: imports y ruta =====\n",
    "import os, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "# Ajusta a tu ruta:\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1911550",
   "metadata": {},
   "source": [
    "Paso 1 ¬∑ Carga del CSV (usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0776802d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/flights_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===== Paso 1: carga eficiente =====\u001b[39;00m\n\u001b[32m      2\u001b[39m need_cols = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMONTH\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mDAY\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mDAY_OF_WEEK\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAIRLINE\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mORIGIN_AIRPORT\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mDESTINATION_AIRPORT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRETRASADO_LLEGADA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m header = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.columns.tolist()\n\u001b[32m     12\u001b[39m present = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m need_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m header]\n\u001b[32m     13\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m need_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m header]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/processed/flights_clean.csv'"
     ]
    }
   ],
   "source": [
    "# ===== Paso 1: carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"‚Üí Columnas presentes:\", present, \"\\n‚Üí Faltantes (se derivar√°n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "print(f\"‚úì Cargado: {v.shape} | en {time.time()-t0:.1f}s\")\n",
    "print(\"‚úì Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3546d",
   "metadata": {},
   "source": [
    "Paso 2 ¬∑ Derivar features que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO/HORA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa4560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 2: features derivados =====\n",
    "\n",
    "# Haversine (km)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad del mes\n",
    "if \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "# Minuto y hora de salida (si no existen)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23).astype(\"int16\")\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59).astype(\"int16\")\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"]//60).astype(\"int16\")\n",
    "\n",
    "# Ruta\n",
    "if \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = (v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Cast finales (por si cargaron como object)\n",
    "for c in [\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\"]:\n",
    "    if c in v.columns: v[c] = v[c].astype(\"float32\")\n",
    "\n",
    "print(\"‚úì Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c340eed",
   "metadata": {},
   "source": [
    "Paso 3 ¬∑ Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaf7e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 3: selecci√≥n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "            \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"]\n",
    "\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54d84f",
   "metadata": {},
   "source": [
    "Paso 4 ¬∑ Split temporal (train: 1‚Äì9, valid: 10‚Äì12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2ae477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ‚Üí X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 4: split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split ‚Üí X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac9816",
   "metadata": {},
   "source": [
    "Paso 5 ¬∑ Target Encoding KFold (sin fuga) para categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be913863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TE aplicado (fix categor√≠as) | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 5 (FIX): Target Encoding KFold (sin fuga) robusto para columnas categ√≥ricas =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    \"\"\"\n",
    "    TE sin fuga:\n",
    "      - Agrupa SIEMPRE usando la columna como object (evita comportamiento de 'category').\n",
    "      - Mapea con dict (no Series) para asegurarnos de obtener floats.\n",
    "    Devuelve:\n",
    "      enc (Series float32, aligned con train_df.index),\n",
    "      mapping (dict {categoria: valor_TE}),\n",
    "      global_mean (float)\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=\"float32\")\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        # claves como object para evitar dtype 'category'\n",
    "        keys_tr = train_df.iloc[tr_idx][col].astype(\"object\")\n",
    "        stats = y.iloc[tr_idx].groupby(keys_tr).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(keys_tr).size()\n",
    "        smoothed = ((stats*cnts + global_mean*smoothing) / (cnts + smoothing)).astype(\"float32\")\n",
    "        smoothed_dict = smoothed.to_dict()\n",
    "\n",
    "        keys_val = train_df.iloc[val_idx][col].astype(\"object\")\n",
    "        mapped = keys_val.map(smoothed_dict).astype(\"float32\")\n",
    "        enc.iloc[val_idx] = mapped.fillna(global_mean).astype(\"float32\")\n",
    "\n",
    "    # mapping final con TODO el train (para aplicar en valid/test/producci√≥n)\n",
    "    keys_all = train_df[col].astype(\"object\")\n",
    "    full_stats = y.groupby(keys_all).mean()\n",
    "    full_cnts  = y.groupby(keys_all).size()\n",
    "    final = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).astype(\"float32\")\n",
    "    mapping = final.to_dict()\n",
    "\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    # asegurar object antes de mapear, retornando float32\n",
    "    return series.astype(\"object\").map(mapping).fillna(default).astype(\"float32\")\n",
    "\n",
    "\n",
    "# --- Aplica TE en las columnas categ√≥ricas seleccionadas ---\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    # a√±ade columna _TE\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr.values.astype(\"float32\")\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = float(default)\n",
    "\n",
    "# matrices finales: dejamos num√©ricas + _TE (quitamos las categor√≠as crudas)\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"‚úì TE aplicado (fix categor√≠as) | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eca9a7",
   "metadata": {},
   "source": [
    "Paso 6 ¬∑ Agregados hist√≥ricos (sin fuga) y uni√≥n a matrices del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d93489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agregados aplicados en 9.3s\n",
      "‚úì Columnas nuevas: ['AIR_n', 'AIR_rate', 'DES_n', 'DES_rate', 'ORI_n', 'ORI_rate', 'RUTA_HORA_n', 'RUTA_HORA_rate', 'RUTA_n', 'RUTA_rate'] \n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 6 ¬∑ Agregados hist√≥ricos (sin fuga) ‚Äî versi√≥n robusta =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# 0) Asegurar llaves en 'v' (DataFrame completo)\n",
    "if \"RUTA\" not in v.columns:\n",
    "    if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "        v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "    else:\n",
    "        raise KeyError(\"No puedo crear RUTA: faltan ORIGIN_AIRPORT/DESTINATION_AIRPORT en 'v'.\")\n",
    "\n",
    "if \"HORA_SALIDA\" not in v.columns:\n",
    "    if \"MINUTO_DIA_SALIDA\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "    elif \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "        v[\"HORA_SALIDA\"] = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23).astype(\"int8\")\n",
    "    else:\n",
    "        raise KeyError(\"No puedo derivar HORA_SALIDA: falta MINUTO_DIA_SALIDA o SCHEDULED_DEPARTURE.\")\n",
    "\n",
    "# 1) M√°scaras temporales (sin fuga)\n",
    "train_mask = v[\"MONTH\"].between(1, 9)\n",
    "valid_mask = v[\"MONTH\"].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask]\n",
    "global_mean = float(v_train[target].mean())\n",
    "\n",
    "# 2) Inyectar las llaves requeridas en X_train / X_valid por √≠ndice (asegura disponibilidad)\n",
    "needed_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "for k in needed_keys:\n",
    "    if k not in X_train.columns:\n",
    "        X_train[k] = v.loc[train_mask, k].values\n",
    "    if k not in X_valid.columns:\n",
    "        X_valid[k] = v.loc[valid_mask, k].values\n",
    "\n",
    "# 3) Definir specs de agregados y filtrar din√°micamente a las claves existentes\n",
    "candidate_aggs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "]\n",
    "available_in_vtrain = set(v_train.columns)\n",
    "aggs_specs = [(keys, pref) for keys, pref in candidate_aggs if set(keys).issubset(available_in_vtrain)]\n",
    "if not aggs_specs:\n",
    "    raise RuntimeError(\"No hay combinaciones de agregados v√°lidas; revisa que existan las columnas clave.\")\n",
    "\n",
    "# 4) Funciones de agregaci√≥n con suavizado y join\n",
    "def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"AGG\", smooth=20):\n",
    "    g = df.groupby(keys, observed=True)[target].agg([\"mean\",\"size\"]).reset_index()\n",
    "    g.rename(columns={\"mean\":f\"{pref}_rate\",\"size\":f\"{pref}_n\"}, inplace=True)\n",
    "    # suavizado para grupos peque√±os\n",
    "    g[f\"{pref}_rate\"] = ((g[f\"{pref}_rate\"]*g[f\"{pref}_n\"] + global_mean*smooth) / (g[f\"{pref}_n\"] + smooth)).astype(\"float32\")\n",
    "    g[f\"{pref}_n\"] = g[f\"{pref}_n\"].astype(\"int32\")\n",
    "    return g\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "    merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "    merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "    return merged\n",
    "\n",
    "# 5) Construir matrices de trabajo partiendo de las del modelo (si existen) o de X_* crudas\n",
    "Xt = X_train_model.copy() if \"X_train_model\" in globals() else X_train.copy()\n",
    "Xv = X_valid_model.copy() if \"X_valid_model\" in globals() else X_valid.copy()\n",
    "\n",
    "# 6) Asegurar que las llaves est√©n presentes en Xt/Xv para poder hacer merge por columnas (y no por √≠ndice)\n",
    "for keys, _ in aggs_specs:\n",
    "    for k in keys:\n",
    "        if k not in Xt.columns:\n",
    "            Xt[k] = X_train[k].values\n",
    "        if k not in Xv.columns:\n",
    "            Xv[k] = X_valid[k].values\n",
    "\n",
    "# 7) Aplicar agregados\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "print(f\"‚úì Agregados aplicados en {time.time()-t0:.1f}s\")\n",
    "\n",
    "# 8) (Opcional) eliminar las llaves a√±adidas si no estaban antes en la matriz del modelo\n",
    "if \"X_train_model\" in globals():\n",
    "    # deja solo columnas originales del modelo + nuevas *_rate/_n\n",
    "    keep_cols = set(X_train_model.columns) | {c for c in Xt.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")}\n",
    "    drop_cols = [c for c in Xt.columns if c not in keep_cols]\n",
    "    Xt = Xt.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    Xv = Xv.drop(columns=[c for c in Xv.columns if c not in (set(X_valid_model.columns) | keep_cols)], errors=\"ignore\")\n",
    "\n",
    "# 9) Actualizar matrices del modelo\n",
    "X_train_model = Xt.copy()\n",
    "X_valid_model = Xv.copy()\n",
    "\n",
    "new_cols = sorted([c for c in X_train_model.columns if c.endswith(\"_rate\") or c.endswith(\"_n\")])\n",
    "print(\"‚úì Columnas nuevas:\", new_cols[:10], \"...\" if len(new_cols) > 10 else \"\")\n",
    "print(\"Shapes ->\", X_train_model.shape, X_valid_model.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ea05c",
   "metadata": {},
   "source": [
    "Paso 7 ¬∑ Entrenamiento LightGBM con early stopping (balanceo por scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d12841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.304088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's auc: 0.599939\tvalid_0's binary_logloss: 0.569545\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.596086\tvalid_0's binary_logloss: 0.459714\n",
      "‚úì Entrenado en 563.2s | best_iter=3\n",
      "ROC-AUC valid=0.5961\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 7: entrenamiento LightGBM =====\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=12000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    scale_pos_weight=scale_pos_weight,  # balanceo\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "print(f\"‚úì Entrenado en {time.time()-t0:.1f}s | best_iter={model.best_iteration_}\")\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e322e",
   "metadata": {},
   "source": [
    "Paso 8 ¬∑ M√©tricas a 0.5 y b√∫squeda de mejor F1 (umbral operativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c225b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1=0.0000 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.192) ==\n",
      "Accuracy: 0.3391 | Precision: 0.1908 | Recall: 0.8728 | F1=0.3131 | ROC-AUC=0.5961\n",
      "CM [TN, FP; FN, TP]=\n",
      " [[175601 595594]\n",
      " [ 20466 140423]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3390509868209303,\n",
       " 'pre': 0.190787712783808,\n",
       " 'rec': 0.8727942867442771,\n",
       " 'f1': 0.31312757412705455,\n",
       " 'thr': 0.19210526315789472}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Paso 8: m√©tricas =====\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc_val:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, thr=thr)\n",
    "\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best = {\"thr\":0.0, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 20):\n",
    "    y_hat = (valid_proba>=thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "\n",
    "best_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b443d",
   "metadata": {},
   "source": [
    "Paso 9 ¬∑ Guardar artefactos (modelo, mappings TE, defaults, threshold y metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49d2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Guardados: modelo + TE mappings/defaults + threshold + metadata.\n"
     ]
    }
   ],
   "source": [
    "# ===== Paso 9: guardar artefactos =====\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "UMBRAL_OPERATIVO = float(best_res[\"thr\"])\n",
    "\n",
    "dump(model, \"models/lgbm_delay_te.joblib\")\n",
    "\n",
    "with open(\"models/te_mappings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({k: {str(cat): float(val) for cat, val in mp.items()} for k, mp in mappings.items()}, f)\n",
    "\n",
    "with open(\"models/te_defaults.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(defaults, f)\n",
    "\n",
    "with open(\"models/threshold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": UMBRAL_OPERATIVO}, f)\n",
    "\n",
    "meta = dict(\n",
    "    features=list(X_train_model.columns),\n",
    "    cols_te=cols_te,\n",
    "    auc_valid=float(auc_val),\n",
    "    split={\"train_months\":\"1-9\", \"valid_months\":\"10-12\"},\n",
    "    scale_pos_weight=float(scale_pos_weight),\n",
    "    note=\"LightGBM + KFold Target Encoding + agregados hist√≥ricos (sin fuga)\"\n",
    ")\n",
    "with open(\"models/metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"‚úì Guardados: modelo + TE mappings/defaults + threshold + metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4815d8f",
   "metadata": {},
   "source": [
    "(Opcional) Paso 10 ¬∑ Funci√≥n de predicci√≥n para un vuelo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a688b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Paso 10: funci√≥n de predicci√≥n (producci√≥n) =====\n",
    "import math\n",
    "\n",
    "# Cargar artefactos\n",
    "model = load(\"models/lgbm_delay_te.joblib\")\n",
    "with open(\"models/te_mappings.json\", \"r\", encoding=\"utf-8\") as f: te_map = json.load(f)\n",
    "with open(\"models/te_defaults.json\", \"r\", encoding=\"utf-8\") as f: te_def = json.load(f)\n",
    "with open(\"models/threshold.json\", \"r\", encoding=\"utf-8\") as f: UMBRAL_OPERATIVO = json.load(f)[\"threshold\"]\n",
    "with open(\"models/metadata.json\", \"r\", encoding=\"utf-8\") as f: meta = json.load(f)\n",
    "feat_order = meta[\"features\"]\n",
    "\n",
    "def prep_features(sample):\n",
    "    \"\"\"\n",
    "    sample: dict con al menos\n",
    "      month, day_of_week, airline, origin, destination, scheduled_hour, scheduled_minute,\n",
    "      origen_lat, origen_lon, dest_lat, dest_lon\n",
    "    Devuelve: DataFrame(1, features) listo para model.predict_proba\n",
    "    \"\"\"\n",
    "    month = int(sample[\"month\"])\n",
    "    dow   = int(sample[\"day_of_week\"])\n",
    "    airline = str(sample[\"airline\"])\n",
    "    origin  = str(sample[\"origin\"])\n",
    "    dest    = str(sample[\"destination\"])\n",
    "    sh      = int(sample[\"scheduled_hour\"])\n",
    "    sm      = int(sample[\"scheduled_minute\"])\n",
    "    olat, olon = float(sample[\"origen_lat\"]), float(sample[\"origen_lon\"])\n",
    "    dlat, dlon = float(sample[\"dest_lat\"]), float(sample[\"dest_lon\"])\n",
    "\n",
    "    # c√≠clicos\n",
    "    salida_sin = math.sin(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    salida_cos = math.cos(2*math.pi * ((sh*60+sm)/(24*60)))\n",
    "    month_sin  = math.sin(2*math.pi * (month/12))\n",
    "    month_cos  = math.cos(2*math.pi * (month/12))\n",
    "\n",
    "    # distancia\n",
    "    def hv(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1,lon1,lat2,lon2])\n",
    "        dlat = lat2-lat1; dlon = lon2-lon1\n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    dist = hv(olat,olon,dlat,dlon)\n",
    "\n",
    "    ruta = f\"{origin}_{dest}\"\n",
    "\n",
    "    row = dict(\n",
    "        MONTH=month, DAY_OF_WEEK=dow,\n",
    "        SALIDA_SIN=salida_sin, SALIDA_COS=salida_cos,\n",
    "        MONTH_SIN=month_sin, MONTH_COS=month_cos,\n",
    "        DISTANCIA_HAV=dist, MINUTO_DIA_SALIDA=sh*60+sm,\n",
    "        HORA_SALIDA=sh\n",
    "    )\n",
    "\n",
    "    # TE para categor√≠as\n",
    "    for col, val in dict(AIRLINE=airline, ORIGIN_AIRPORT=origin, DESTINATION_AIRPORT=dest, RUTA=ruta).items():\n",
    "        if f\"{col}_TE\" in feat_order:\n",
    "            mapping = te_map.get(col, {})\n",
    "            default = te_def.get(col, float(np.mean(list(mapping.values()) or [0.18])))\n",
    "            row[f\"{col}_TE\"] = float(mapping.get(str(val), default))\n",
    "\n",
    "    X = pd.DataFrame([row], columns=feat_order).fillna(0)\n",
    "    return X\n",
    "\n",
    "def predecir(sample):\n",
    "    X1 = prep_features(sample)\n",
    "    proba = float(model.predict_proba(X1)[:,1])\n",
    "    pred  = int(proba >= UMBRAL_OPERATIVO)\n",
    "    return dict(prob=proba, delayed=pred, thr=UMBRAL_OPERATIVO)\n",
    "\n",
    "# Ejemplo:\n",
    "# predecir(dict(\n",
    "#   month=11, day_of_week=5, airline=\"AA\", origin=\"JFK\", destination=\"MIA\",\n",
    "#   scheduled_hour=14, scheduled_minute=30, origen_lat=40.6413, origen_lon=-73.7781,\n",
    "#   dest_lat=25.7959, dest_lon=-80.2870\n",
    "# ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4adc5c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral para Precisi√≥n ‚â• 0.3: 0.229 | Precisi√≥n=0.308 | Recall=0.017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba)\n",
    "# encuentra el primer umbral con precisi√≥n >= 0.30\n",
    "target_prec = 0.30\n",
    "idx = np.where(prec >= target_prec)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_op = thr[i-1] if i>0 else 0.99\n",
    "    print(f\"Umbral para Precisi√≥n ‚â• {target_prec}: {thr_op:.3f} | Precisi√≥n={prec[i]:.3f} | Recall={rec[i]:.3f}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisi√≥n en el set actual.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e89ef4",
   "metadata": {},
   "source": [
    "calcular el umbral a una precisi√≥n objetivo (o recall objetivo),\n",
    "\n",
    "y un peque√±o grid de LightGBM listo para pegar y correr con early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a837143",
   "metadata": {},
   "source": [
    "1) Elegir umbral operativo (por precisi√≥n objetivo o por recall objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddda93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Umbral por Precisi√≥n ‚â• 0.3 (thr=0.229) ==\n",
      "Accuracy: 0.8238 | Precision: 0.3080 | Recall: 0.0166 | F1: 0.0315 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[765203   5992]\n",
      " [158222   2667]]\n",
      "\n",
      "== Umbral por Recall ‚â• 0.7 (thr=0.200) ==\n",
      "Accuracy: 0.4825 | Precision: 0.2062 | Recall: 0.7010 | F1: 0.3186 | ROC-AUC: 0.5961\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[336982 434213]\n",
      " [ 48113 112776]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Usa las probabilidades del modelo ya entrenado\n",
    "y_proba_valid = model.predict_proba(X_valid_model)[:, 1]\n",
    "\n",
    "def report_at_threshold(y_true, y_proba, thr, title=\"\"):\n",
    "    y_hat = (y_proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# --- a) Umbral para alcanzar una precisi√≥n objetivo (ej: 0.30) ---\n",
    "target_precision = 0.30\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_valid, y_proba_valid)\n",
    "# thr tiene len = len(prec)-1 => alinear √≠ndices\n",
    "idx = np.where(prec[:-1] >= target_precision)[0]\n",
    "if len(idx):\n",
    "    i = idx[0]\n",
    "    thr_prec = float(thr[i])\n",
    "    res_prec = report_at_threshold(y_valid, y_proba_valid, thr_prec, f\"Umbral por Precisi√≥n ‚â• {target_precision}\")\n",
    "else:\n",
    "    print(\"No se alcanza esa precisi√≥n en el set de validaci√≥n.\")\n",
    "\n",
    "# --- b) Umbral para alcanzar un recall objetivo (ej: 0.70) ---\n",
    "target_recall = 0.70\n",
    "# Buscar umbral m√≠nimo que consiga ese recall (prioriza mayor umbral para bajar falsos positivos)\n",
    "idx = np.where(rec[:-1] >= target_recall)[0]\n",
    "if len(idx):\n",
    "    i = idx[-1]  # el m√°s alto que a√∫n cumple el recall objetivo\n",
    "    thr_rec = float(thr[i])\n",
    "    res_rec = report_at_threshold(y_valid, y_proba_valid, thr_rec, f\"Umbral por Recall ‚â• {target_recall}\")\n",
    "else:\n",
    "    print(\"No se alcanza ese recall en el set de validaci√≥n.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5fc01",
   "metadata": {},
   "source": [
    "2) Mini-grid de LightGBM con early stopping (r√°pido y efectivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c766e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.670011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.535804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.601788\tvalid_0's binary_logloss: 0.580937\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=3.4 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.536024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=3.2 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.597746\tvalid_0's binary_logloss: 0.569464\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.9 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.446640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602257\tvalid_0's binary_logloss: 0.581656\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602285\tvalid_0's binary_logloss: 0.460513\n",
      "AUC valid=0.6023 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.369460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.7 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 63, 'min_child_samples': 300, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.485948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598509\tvalid_0's binary_logloss: 0.568933\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.601646\tvalid_0's binary_logloss: 0.459812\n",
      "AUC valid=0.6016 | tiempo=2.5 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.8 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602504\tvalid_0's binary_logloss: 0.582059\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.0 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 100, 'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.467874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.598631\tvalid_0's binary_logloss: 0.566213\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.600624\tvalid_0's binary_logloss: 0.459715\n",
      "AUC valid=0.6006 | tiempo=3.1 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.452842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's auc: 0.602254\tvalid_0's binary_logloss: 0.581103\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602381\tvalid_0's binary_logloss: 0.460405\n",
      "AUC valid=0.6024 | tiempo=2.6 min | best_iter=1\n",
      "\n",
      "Probando: {'learning_rate': 0.05, 'num_leaves': 127, 'min_child_samples': 300, 'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.424657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m     params[k] = v\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProbando:\u001b[39m\u001b[33m\"\u001b[39m, {k:params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys})\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m auc, secs, mdl = \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAUC valid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | tiempo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecs/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min | best_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmdl.best_iteration_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > best[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_eval\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     32\u001b[39m model = lgb.LGBMClassifier(**params)\n\u001b[32m     33\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m secs = time.time()-t0\n\u001b[32m     41\u001b[39m proba = model.predict_proba(X_valid_model)[:,\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time, itertools\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Clase balanceada por ratio neg/pos (ya lo usaste antes)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "base = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,          # se explora m√°s abajo tambi√©n 0.03\n",
    "    n_estimators=12000,          # early_stopping lo frenar√° antes si no mejora\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,               # bagging_fraction\n",
    "    colsample_bytree=0.85,       # feature_fraction\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "grid = {\n",
    "    \"learning_rate\": [0.05, 0.03],\n",
    "    \"num_leaves\":    [63, 127],\n",
    "    \"min_child_samples\": [100, 300],\n",
    "    \"colsample_bytree\":  [0.8, 0.9],\n",
    "    \"subsample\":         [0.8, 0.9],\n",
    "}\n",
    "\n",
    "def train_eval(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    model.fit(\n",
    "        X_train_model, y_train,\n",
    "        eval_set=[(X_valid_model, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = model.predict_proba(X_valid_model)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return auc, secs, model\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"secs\": None, \"model\": None}\n",
    "\n",
    "keys = list(grid.keys())\n",
    "for values in itertools.product(*[grid[k] for k in keys]):\n",
    "    params = base.copy()\n",
    "    for k,v in zip(keys, values):\n",
    "        params[k] = v\n",
    "    print(\"\\nProbando:\", {k:params[k] for k in keys})\n",
    "    auc, secs, mdl = train_eval(params)\n",
    "    print(f\"AUC valid={auc:.4f} | tiempo={secs/60:.1f} min | best_iter={mdl.best_iteration_}\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best = {\"auc\": auc, \"params\": params, \"secs\": secs, \"model\": mdl}\n",
    "\n",
    "print(\"\\n=== MEJOR CONFIGURACI√ìN ===\")\n",
    "print(best[\"params\"])\n",
    "print(f\"AUC valid={best['auc']:.4f} | tiempo={best['secs']/60:.1f} min | best_iter={best['model'].best_iteration_}\")\n",
    "\n",
    "# Opcional: guardar el mejor modelo\n",
    "# from joblib import dump\n",
    "# dump(best[\"model\"], \"models/lgbm_best.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21757cd8",
   "metadata": {},
   "source": [
    "### Revisi√≥n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44b5e8",
   "metadata": {},
   "source": [
    "Carga eficiente + dtypes compactos\n",
    "\n",
    "Derivaci√≥n de features\n",
    "\n",
    "Split temporal (1‚Äì9 vs 10‚Äì12)\n",
    "\n",
    "Target Encoding KFold sin fuga\n",
    "\n",
    "Agregados hist√≥ricos sin fuga\n",
    "\n",
    "Entrenamiento LightGBM + mini-b√∫squeda\n",
    "\n",
    "Selecci√≥n de umbral operativo\n",
    "\n",
    "Guardado de artefactos\n",
    "\n",
    "Funci√≥n de scoring para producci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eac698",
   "metadata": {},
   "source": [
    "1) Carga eficiente del CSV (solo columnas √∫tiles + dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67fb8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Columnas presentes: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA']\n",
      "‚Üí Faltantes (se derivar√°n si aplica): []\n",
      "‚úì Cargado: (5231130, 14)  |  en 36.3s\n",
      "‚úì Rate retraso: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Carga eficiente =====\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\",\n",
    "    # opcional de an√°lisis (no para predecir): \"MOTIVO_RETRASO\"\n",
    "]\n",
    "\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"‚Üí Columnas presentes:\", present)\n",
    "print(\"‚Üí Faltantes (se derivar√°n si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\",\n",
    "    \"MOTIVO_RETRASO\":\"category\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"‚úì Cargado: {v.shape}  |  en {t1-t0:.1f}s\")\n",
    "print(\"‚úì Rate retraso:\", float(v[\"RETRASADO_LLEGADA\"].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c109b3",
   "metadata": {},
   "source": [
    "2) Derivar features (distancia, estacionalidad, hora/minuto, ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f7b7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Listo: rate= 0.18471362783949166 | cols= 20\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Derivar features =====\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    a = np.sin((lat2-lat1)/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin((lon2-lon1)/2)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(\"float32\")\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns):\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"],v[\"ORIGEN_LON\"],v[\"DEST_LAT\"],v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mensual\n",
    "if \"MONTH\" in v:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi*(v[\"MONTH\"]/12)).astype(\"float32\")\n",
    "\n",
    "# Hora / minuto del d√≠a programados\n",
    "if \"SCHEDULED_DEPARTURE\" in v:\n",
    "    mins = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0,59)\n",
    "    hrs  = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0,23)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hrs*60 + mins).astype(\"int32\")\n",
    "    v[\"HORA_SALIDA\"]       = hrs.astype(\"int16\")\n",
    "\n",
    "# Ruta texto\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns):\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"‚úì Listo: rate=\", float(v[\"RETRASADO_LLEGADA\"].mean()), \"| cols=\", v.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0ffbf",
   "metadata": {},
   "source": [
    "3) Selecci√≥n de variables (features/target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e20c1776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 13) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Selecci√≥n de variables =====\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# usa solo lo que exista\n",
    "features = [c for c in (cat_cols + num_cols) if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444bc1b",
   "metadata": {},
   "source": [
    "4) Split temporal (train=meses 1‚Äì9, valid=10‚Äì12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c906f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ‚Üí X_train (4299046, 13) | X_valid (932084, 13) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) Split temporal =====\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Split ‚Üí X_train\", X_train.shape, \"| X_valid\", X_valid.shape,\n",
    "      \"| rate train\", float(y_train.mean()), \"| rate valid\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ba89e",
   "metadata": {},
   "source": [
    "5) Target Encoding (KFold sin fuga) sobre categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b95fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TE aplicado | X_train_model: (4299046, 13) | X_valid_model: (932084, 13)\n",
      "Ejemplo columnas: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE']\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Target Encoding KFold (sin fuga) =====\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if y_train.name is None:\n",
    "    y_train = y_train.rename(\"RETRASADO_LLEGADA\")\n",
    "\n",
    "def kfold_target_encode(train_df, col, y, n_splits=5, smoothing=50, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_df.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(train_df, y):\n",
    "        stats = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).mean()\n",
    "        cnts  = y.iloc[tr_idx].groupby(train_df.iloc[tr_idx][col].astype(str)).size()\n",
    "        smoothed = (stats*cnts + global_mean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = (\n",
    "            train_df.iloc[val_idx][col].astype(str).map(smoothed)\n",
    "                  .fillna(global_mean).astype(np.float32)\n",
    "        )\n",
    "\n",
    "    full_stats = y.groupby(train_df[col].astype(str)).mean()\n",
    "    full_cnts  = y.groupby(train_df[col].astype(str)).size()\n",
    "    mapping = ((full_stats*full_cnts + global_mean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "    return enc, mapping, global_mean\n",
    "\n",
    "def apply_te(series, mapping, default):\n",
    "    return series.astype(str).map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "mappings, defaults = {}, {}\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode(X_train[[c]], c, y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train.loc[:, f\"{c}_TE\"] = enc_tr\n",
    "    X_valid.loc[:, f\"{c}_TE\"]  = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "# quitar columnas categ√≥ricas crudas\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"‚úì TE aplicado | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Ejemplo columnas:\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b05f9",
   "metadata": {},
   "source": [
    "6) Agregados hist√≥ricos (sin fuga) y join a las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 6) Agregados hist√≥ricos (1‚Äì9) =====\n",
    "# global_mean = float(y_train.mean())\n",
    "\n",
    "# v_train = v.loc[train_mask, [\"RETRASADO_LLEGADA\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]].copy()\n",
    "\n",
    "# # def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "# #     g = df.groupby(keys)[target]\n",
    "# #     stats = g.mean()\n",
    "# #     cnts  = g.size()\n",
    "# #     agg = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "# #     out = agg.reset_index().rename(columns={0:f\"{pref}_rate\"})\n",
    "# #     out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "# #     return out\n",
    "\n",
    "# # def left_join_agg(X_left, keys, pref):\n",
    "# #     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "# #     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "# #     merged[f\"{pref}_rate\"] = merged[f\"{pref}_rate\"].fillna(global_mean).astype(\"float32\")\n",
    "# #     merged[f\"{pref}_n\"]    = merged[f\"{pref}_n\"].fillna(0).astype(\"int32\")\n",
    "# #     return merged\n",
    "\n",
    "# # --- FIX funciones de agregados ---\n",
    "\n",
    "# def build_agg(df, keys, target=\"RETRASADO_LLEGADA\", pref=\"\", smooth=20):\n",
    "#     \"\"\"\n",
    "#     Devuelve un DF con columnas: keys + [f\"{pref}_rate\", f\"{pref}_n\"]\n",
    "#     usando suavizado para evitar overfitting en claves con pocos registros.\n",
    "#     \"\"\"\n",
    "#     g = df.groupby(keys)[target]\n",
    "#     stats = g.mean()                         # media por clave\n",
    "#     cnts  = g.size()                         # conteo por clave\n",
    "\n",
    "#     # smoothed rate (float32) y count (int32)\n",
    "#     smoothed = ((stats*cnts + global_mean*smooth) / (cnts + smooth)).astype(\"float32\")\n",
    "#     cnts     = cnts.astype(\"int32\")\n",
    "\n",
    "#     # ¬°OJO! Convertimos expl√≠citamente a DataFrame con nombres correctos\n",
    "#     out_rate = smoothed.to_frame(name=f\"{pref}_rate\").reset_index()\n",
    "#     out_cnt  = cnts.to_frame(name=f\"{pref}_n\").reset_index()\n",
    "\n",
    "#     # Unimos por las llaves\n",
    "#     out = out_rate.merge(out_cnt, on=keys, how=\"left\")\n",
    "#     return out\n",
    "\n",
    "\n",
    "# def left_join_agg(X_left, keys, pref):\n",
    "#     \"\"\"\n",
    "#     Hace left-join contra el DF de agregados y rellena NaN con defaults.\n",
    "#     \"\"\"\n",
    "#     agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "\n",
    "#     # merge\n",
    "#     merged = X_left.merge(agg_df, on=keys, how=\"left\")\n",
    "\n",
    "#     # asegurar que las columnas existan (si no, crearlas con defaults)\n",
    "#     rate_col = f\"{pref}_rate\"\n",
    "#     n_col    = f\"{pref}_n\"\n",
    "\n",
    "#     if rate_col not in merged.columns:\n",
    "#         merged[rate_col] = np.nan\n",
    "#     if n_col not in merged.columns:\n",
    "#         merged[n_col] = np.nan\n",
    "\n",
    "#     merged[rate_col] = merged[rate_col].fillna(global_mean).astype(\"float32\")\n",
    "#     merged[n_col]    = merged[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "#     return merged\n",
    "\n",
    "\n",
    "# Xt = X_train_model.copy()\n",
    "# Xv = X_valid_model.copy()\n",
    "\n",
    "# # A√±adimos llaves temporales para poder unir\n",
    "# Xt[\"AIRLINE\"] = v.loc[train_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xt[\"ORIGIN_AIRPORT\"] = v.loc[train_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xt[\"DESTINATION_AIRPORT\"] = v.loc[train_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xt[\"RUTA\"] = v.loc[train_mask, \"RUTA\"].astype(str).values\n",
    "# Xt[\"HORA_SALIDA\"] = v.loc[train_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# Xv[\"AIRLINE\"] = v.loc[valid_mask, \"AIRLINE\"].astype(str).values\n",
    "# Xv[\"ORIGIN_AIRPORT\"] = v.loc[valid_mask, \"ORIGIN_AIRPORT\"].astype(str).values\n",
    "# Xv[\"DESTINATION_AIRPORT\"] = v.loc[valid_mask, \"DESTINATION_AIRPORT\"].astype(str).values\n",
    "# Xv[\"RUTA\"] = v.loc[valid_mask, \"RUTA\"].astype(str).values\n",
    "# Xv[\"HORA_SALIDA\"] = v.loc[valid_mask, \"HORA_SALIDA\"].astype(\"int16\").values\n",
    "\n",
    "# aggs_specs = [\n",
    "#     ([\"AIRLINE\"], \"AIR\"),\n",
    "#     ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "#     ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "#     ([\"RUTA\"], \"RUTA\"),\n",
    "#     ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
    "# ]\n",
    "\n",
    "# t0 = time.time()\n",
    "# for keys, pref in aggs_specs:\n",
    "#     Xt = left_join_agg(Xt, keys, pref)\n",
    "#     Xv = left_join_agg(Xv, keys, pref)\n",
    "# # limpiar llaves temporales\n",
    "# drop_keys = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]\n",
    "# Xt.drop(columns=[c for c in drop_keys if c in Xt], inplace=True)\n",
    "# Xv.drop(columns=[c for c in drop_keys if c in Xv], inplace=True)\n",
    "# t1 = time.time()\n",
    "\n",
    "# print(f\"‚úì Agregados aplicados en {t1-t0:.1f}s\")\n",
    "# print(\"Shapes ->\", Xt.shape, Xv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd8afbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agregados aplicados en 18.5s\n",
      "‚úì Columnas nuevas: ['AIR_rate', 'AIR_n', 'DES_rate', 'DES_n', 'ORI_rate', 'ORI_n', 'RUTA_rate', 'RUTA_n', 'RUTA_HORA_rate', 'RUTA_HORA_n']\n",
      "Shapes -> (4299046, 23) (932084, 23)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6) Agregados hist√≥ricos (sin fuga) y uni√≥n a matrices del modelo\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- 6.1 Asegurar variables base ---\n",
    "assert 'RETRASADO_LLEGADA' in v.columns, \"No existe la columna target en 'v'.\"\n",
    "target = 'RETRASADO_LLEGADA'\n",
    "\n",
    "# Si no existen las m√°scaras, las recreamos\n",
    "if 'train_mask' not in globals() or 'valid_mask' not in globals():\n",
    "    train_mask = v['MONTH'].between(1, 9)\n",
    "    valid_mask = v['MONTH'].between(10, 12)\n",
    "\n",
    "v_train = v.loc[train_mask].copy()                # SOLO meses 1‚Äì9 para evitar fuga\n",
    "global_mean = float(v_train[target].mean())       # media global para smoothing/NaN\n",
    "\n",
    "# Asegurar columnas de llaves en v (por si faltan)\n",
    "if 'RUTA' not in v.columns and {'ORIGIN_AIRPORT','DESTINATION_AIRPORT'}.issubset(v.columns):\n",
    "    v['RUTA'] = (v['ORIGIN_AIRPORT'].astype(str) + '_' +\n",
    "                 v['DESTINATION_AIRPORT'].astype(str))\n",
    "\n",
    "if 'HORA_SALIDA' not in v.columns and 'SCHEDULED_DEPARTURE' in v.columns:\n",
    "    # SCHEDULED_DEPARTURE en HHMM (int). Tomamos la hora.\n",
    "    v['HORA_SALIDA'] = (v['SCHEDULED_DEPARTURE'] // 100).clip(0, 23).astype('int16')\n",
    "\n",
    "# --- 6.2 Funciones robustas de agregados y merge ---\n",
    "def build_agg(df, keys, target='RETRASADO_LLEGADA', pref='', smooth=20):\n",
    "    \"\"\"\n",
    "    Devuelve DataFrame con columnas: keys + [f'{pref}_rate', f'{pref}_n'].\n",
    "    rate con suavizado; n = conteo.\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "\n",
    "    smoothed = ((stats * cnts + global_mean * smooth) / (cnts + smooth)).astype('float32')\n",
    "    cnts     = cnts.astype('int32')\n",
    "\n",
    "    out_rate = smoothed.to_frame(name=f'{pref}_rate').reset_index()\n",
    "    out_cnt  = cnts.to_frame(name=f'{pref}_n').reset_index()\n",
    "    out = out_rate.merge(out_cnt, on=keys, how='left')\n",
    "    return out\n",
    "\n",
    "def left_join_agg(X_left, keys, pref):\n",
    "    \"\"\"\n",
    "    Left join contra el DF de agregados (solo entrenado con v_train).\n",
    "    Rellena NaN con defaults: rate=global_mean, n=0.\n",
    "    \"\"\"\n",
    "    agg_df = build_agg(v_train, keys, target=target, pref=pref, smooth=20)\n",
    "    merged = X_left.merge(agg_df, on=keys, how='left')\n",
    "\n",
    "    rate_col = f'{pref}_rate'\n",
    "    n_col    = f'{pref}_n'\n",
    "    if rate_col not in merged.columns:\n",
    "        merged[rate_col] = np.nan\n",
    "    if n_col not in merged.columns:\n",
    "        merged[n_col] = np.nan\n",
    "\n",
    "    merged[rate_col] = merged[rate_col].fillna(global_mean).astype('float32')\n",
    "    merged[n_col]    = merged[n_col].fillna(0).astype('int32')\n",
    "    return merged\n",
    "\n",
    "# --- 6.3 Preparar matrices Xt/Xv con llaves temporales (sin fuga) ---\n",
    "Xt = X_train_model.copy()\n",
    "Xv = X_valid_model.copy()\n",
    "\n",
    "# Agregamos llaves desde 'v' usando el √≠ndice original (alineaci√≥n 1:1)\n",
    "for col in ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']:\n",
    "    if col in v.columns:\n",
    "        if col not in Xt.columns:\n",
    "            Xt[col] = v.loc[X_train_model.index, col].values\n",
    "        if col not in Xv.columns:\n",
    "            Xv[col] = v.loc[X_valid_model.index, col].values\n",
    "\n",
    "# --- 6.4 Especificaci√≥n de agregados a calcular ---\n",
    "aggs_specs = []\n",
    "if 'AIRLINE' in Xt.columns:\n",
    "    aggs_specs.append((['AIRLINE'], 'AIR'))\n",
    "if 'DESTINATION_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['DESTINATION_AIRPORT'], 'DES'))\n",
    "if 'ORIGIN_AIRPORT' in Xt.columns:\n",
    "    aggs_specs.append((['ORIGIN_AIRPORT'], 'ORI'))\n",
    "if 'RUTA' in Xt.columns:\n",
    "    aggs_specs.append((['RUTA'], 'RUTA'))\n",
    "if {'RUTA', 'HORA_SALIDA'}.issubset(Xt.columns):\n",
    "    aggs_specs.append((['RUTA', 'HORA_SALIDA'], 'RUTA_HORA'))  # opcional, suele ayudar\n",
    "\n",
    "# --- 6.5 Aplicar agregados y limpiar llaves temporales ---\n",
    "t0 = time.time()\n",
    "for keys, pref in aggs_specs:\n",
    "    Xt = left_join_agg(Xt, keys, pref)\n",
    "    Xv = left_join_agg(Xv, keys, pref)\n",
    "\n",
    "# Eliminamos llaves si no eran parte de X_*_model original\n",
    "temp_keys = ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA', 'HORA_SALIDA']\n",
    "Xt.drop(columns=[c for c in temp_keys if c in Xt.columns and c not in X_train_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "Xv.drop(columns=[c for c in temp_keys if c in Xv.columns and c not in X_valid_model.columns],\n",
    "        inplace=True, errors='ignore')\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "new_cols = [c for c in Xt.columns if c.endswith('_rate') or c.endswith('_n')]\n",
    "print(f\"‚úì Agregados aplicados en {t1-t0:.1f}s\")\n",
    "print(\"‚úì Columnas nuevas:\", new_cols)\n",
    "print(\"Shapes ->\", Xt.shape, Xv.shape)\n",
    "\n",
    "# Actualizamos las matrices del modelo para usar estas versiones enriquecidas\n",
    "X_train_model = Xt\n",
    "X_valid_model = Xv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5c90c",
   "metadata": {},
   "source": [
    "7) Entrenamiento LightGBM + mini-b√∫squeda + AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07595942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 7) Entrenamiento LightGBM =====\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# scale_pos_weight = neg / max(pos,1)\n",
    "\n",
    "# grid = [\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.05, num_leaves=127, min_child_samples=300, colsample_bytree=0.8, subsample=0.8),\n",
    "#     dict(learning_rate=0.03, num_leaves=127, min_child_samples=100, colsample_bytree=0.8, subsample=0.8),\n",
    "# ]\n",
    "\n",
    "# best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "# for p in grid:\n",
    "#     params = dict(\n",
    "#         objective=\"binary\",\n",
    "#         learning_rate=p[\"learning_rate\"],\n",
    "#         n_estimators=12000,\n",
    "#         num_leaves=p[\"num_leaves\"],\n",
    "#         min_child_samples=p[\"min_child_samples\"],\n",
    "#         subsample=p[\"subsample\"],\n",
    "#         colsample_bytree=p[\"colsample_bytree\"],\n",
    "#         reg_alpha=0.0,\n",
    "#         reg_lambda=5.0,\n",
    "#         n_jobs=-1,\n",
    "#         random_state=42,\n",
    "#         scale_pos_weight=scale_pos_weight,\n",
    "#     )\n",
    "#     print(\"Probando:\", {k:params[k] for k in [\"learning_rate\",\"num_leaves\",\"min_child_samples\",\"colsample_bytree\",\"subsample\"]})\n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     t0 = time.time()\n",
    "#     model.fit(\n",
    "#         Xt, y_train,\n",
    "#         eval_set=[(Xv, y_valid)],\n",
    "#         eval_metric=\"auc\",\n",
    "#         callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(400)]\n",
    "#     )\n",
    "#     t1 = time.time()\n",
    "#     val_proba = model.predict_proba(Xv)[:,1]\n",
    "#     auc_val = roc_auc_score(y_valid, val_proba)\n",
    "#     print(f\"AUC valid={auc_val:.4f} | tiempo={(t1-t0)/60:.1f} min | best_iter={model.best_iteration_}\\n\")\n",
    "#     if auc_val > best[\"auc\"]:\n",
    "#         best.update({\"auc\": auc_val, \"model\": model, \"params\": params})\n",
    "\n",
    "# print(\"=== MEJOR CONFIGURACI√ìN ===\")\n",
    "# print(best[\"params\"])\n",
    "# print(f\"AUC valid={best['auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53af73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.602295\tvalid_0's binary_logloss: 0.583084\n",
      "[400]\tvalid_0's auc: 0.602473\tvalid_0's binary_logloss: 0.580826\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602496\tvalid_0's binary_logloss: 0.460413\n",
      "‚úÖ Entrenado en 261.8s | best_iter=1 | ROC-AUC valid=0.6025\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4275 | Precision: 0.2009 | Recall: 0.7782 | F1: 0.3194 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[273276 497919]\n",
      " [ 35688 125201]]\n",
      "‚Üí Umbral F1 √≥ptimo: {'thr': 0.2, 'f1': 0.3193866396941872}\n",
      "\n",
      "== Umbral por Precisi√≥n ‚â≥ 0.30 (aprox) (thr=0.209) ==\n",
      "Accuracy: 0.7000 | Precision: 0.2434 | Recall: 0.3499 | F1: 0.2871 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[596171 175024]\n",
      " [104588  56301]]\n",
      "\n",
      "== Umbral por Recall ‚â• 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.17261212508743848,\n",
       " 'pre': 0.17261212508743848,\n",
       " 'rec': 1.0,\n",
       " 'f1': 0.2944061747179482,\n",
       " 'auc': 0.6024960911474394,\n",
       " 'thr': 0.05}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 7) Entrenar LightGBM + m√©tricas/umbrales\n",
    "# ================================\n",
    "import time\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Peso para desbalance (c√°lculo en TRAIN)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/ max(pos,1), 1.0)\n",
    "\n",
    "# Mejores hiperpar√°metros que te rindieron (puedes ajustarlos si deseas)\n",
    "params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=12000,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:,1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"‚úÖ Entrenado en {t1-t0:.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n",
    "\n",
    "# ---- M√©tricas helper ----\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base_05 = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# B√∫squeda de mejor F1\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "print(\"‚Üí Umbral F1 √≥ptimo:\", best)\n",
    "\n",
    "# Umbral operativo por objetivo de negocio (opcional)\n",
    "# - si quieres ‚â• 0.30 de precisi√≥n\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best[\"thr\"]  # heur√≠stica simple\n",
    "report_metrics(y_valid, valid_proba, thr_prec, \"Umbral por Precisi√≥n ‚â≥ 0.30 (aprox)\")\n",
    "\n",
    "# - si quieres ‚â• 0.70 de recall (recuperaci√≥n de retrasos)\n",
    "# barrido r√°pido hasta lograr >=0.70 recall\n",
    "thr_rec = best[\"thr\"]\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    rec = recall_score(y_valid, (valid_proba >= thr).astype(int), zero_division=0)\n",
    "    if rec >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "report_metrics(y_valid, valid_proba, thr_rec, \"Umbral por Recall ‚â• 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73177a53",
   "metadata": {},
   "source": [
    "8) Umbrales operativos (precisi√≥n m√≠nima vs recall m√≠nimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da54e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\lgbm_retrasos.pkl | best_iteration=1\n",
      "‚úÖ Target Encoding guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\target_encoding.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n",
      "C:\\Users\\Adrian Merlo\\AppData\\Local\\Temp\\ipykernel_26968\\353620751.py:35: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = df.groupby(keys)[target]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agregados hist√≥ricos guardados: 5 tablas.\n",
      "‚úÖ Orden de features guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\feature_order.json\n",
      "‚úÖ Metadatos guardados en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata.json\n",
      "‚úÖ Schema de entrada guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\input_schema.json\n",
      "\n",
      "üîé Smoke test de recarga‚Ä¶\n",
      "Smoke AUC (mini valid): 0.6169\n",
      "‚úÖ Artefactos listos.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8) Guardar artefactos del modelo y metadatos\n",
    "# ============================================\n",
    "import os, json, time, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "# 8.1 Guardar el modelo LightGBM (con best_iteration en metadatos)\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "print(f\"‚úÖ Modelo guardado en: {model_path} | best_iteration={best_iter}\")\n",
    "\n",
    "# 8.2 Preparar y guardar mapeos de Target Encoding (si existen)\n",
    "#   - mappings: dict por columna {categoria -> media_suavizada}\n",
    "#   - defaults: dict por columna {default_global_mean}\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "if \"mappings\" in globals() and \"defaults\" in globals():\n",
    "    joblib.dump({\"mappings\": mappings, \"defaults\": defaults}, te_path)\n",
    "    print(f\"‚úÖ Target Encoding guardado en: {te_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron 'mappings'/'defaults' para Target Encoding; se omite guardado TE.\")\n",
    "\n",
    "# 8.3 Guardar tablas de agregados hist√≥ricos (si tienes funciones y specs)\n",
    "#     Reconstituimos los agregados con v_train para congelar el 'hist√≥rico' sin fuga.\n",
    "agg_tables = {}\n",
    "if \"aggs_specs\" in globals() and \"v_train\" in globals() and \"build_agg\" in globals():\n",
    "    for keys, pref in aggs_specs:\n",
    "        try:\n",
    "            agg_df = build_agg(v_train, keys, target=\"RETRASADO_LLEGADA\", pref=pref, smooth=20)\n",
    "            # Guardar cada agregado como CSV individual\n",
    "            csv_path = os.path.join(ARTIF_DIR, f\"agg_{pref}.csv\")\n",
    "            agg_df.to_csv(csv_path, index=False)\n",
    "            agg_tables[pref] = csv_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  No se pudo construir/guardar el agregado '{pref}': {e}\")\n",
    "    print(f\"‚úÖ Agregados hist√≥ricos guardados: {len(agg_tables)} tablas.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron 'aggs_specs'/'v_train'/'build_agg'; se omite guardado de agregados.\")\n",
    "\n",
    "# 8.4 Guardar orden de features finales del modelo\n",
    "feature_order = list(X_train_model.columns)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_order\": feature_order}, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "# 8.5 Guardar metadatos (umbrales, medias, info para inferencia)\n",
    "#     - umbral base 0.5 y umbral F1 √≥ptimo (si existe 'best')\n",
    "#     - best_iteration\n",
    "#     - columnas categ√≥ricas originales usadas en TE (si existen)\n",
    "#     - global_mean para TE/aggregates (si existe)\n",
    "meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": model_path,\n",
    "    \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "    \"thresholds\": {\n",
    "        \"base_05\": 0.5,\n",
    "        \"best_f1\": float(best[\"thr\"]) if \"best\" in globals() and isinstance(best, dict) and \"thr\" in best else None\n",
    "    },\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"agg_tables\": agg_tables,  # dict {pref -> csv_path}\n",
    "    \"global_mean\": float(global_mean) if \"global_mean\" in globals() else None,\n",
    "    \"te_cols\": list(mappings.keys()) if \"mappings\" in globals() else []\n",
    "}\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Metadatos guardados en: {meta_path}\")\n",
    "\n",
    "# 8.6 (Opcional) Guardar un \"schema\" ligero de dtypes esperados para entrada cruda\n",
    "schema = {\n",
    "    \"required_raw_fields\": [\n",
    "        \"MONTH\",\"DAY_OF_WEEK\",\n",
    "        \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "        \"SCHEDULED_DEPARTURE\",  # para reconstruir MINUTO_DIA_SALIDA/SALIDA_SIN/COS en producci√≥n si hace falta\n",
    "        \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"  # para DISTANCIA_HAV si no viene en la entrada\n",
    "    ],\n",
    "    \"notes\": \"Si la entrada ya trae SALIDA_SIN/SALIDA_COS/DISTANCIA_HAV, se usan directamente.\"\n",
    "}\n",
    "schema_path = os.path.join(ARTIF_DIR, \"input_schema.json\")\n",
    "with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Schema de entrada guardado en: {schema_path}\")\n",
    "\n",
    "# 8.7 Smoke test de recarga r√°pida\n",
    "print(\"\\nüîé Smoke test de recarga‚Ä¶\")\n",
    "_loaded_model = joblib.load(model_path)\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    _feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Usamos un mini-slice de validaci√≥n para probar predict_proba\n",
    "idx = np.random.RandomState(42).choice(len(X_valid_model), size=min(5000, len(X_valid_model)), replace=False)\n",
    "mini_Xv = X_valid_model.iloc[idx][_feature_order]\n",
    "mini_yv = y_valid.iloc[idx]\n",
    "\n",
    "proba = _loaded_model.predict_proba(mini_Xv, num_iteration=getattr(_loaded_model, \"best_iteration_\", None))[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f\"Smoke AUC (mini valid): {roc_auc_score(mini_yv, proba):.4f}\")\n",
    "print(\"‚úÖ Artefactos listos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb15bf6",
   "metadata": {},
   "source": [
    "Paso 9 ¬∑ Cargador de artefactos + funciones de preprocesamiento para inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4da7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos cargados. Features esperadas: 23\n",
      "TE cols: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA'] | Agregados: ['AIR', 'DES', 'ORI', 'RUTA', 'RUTA_HORA']\n",
      "Umbrales -> base: 0.5 | best_f1: 0.2 | best_iter: 1\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9) Cargar artefactos y helpers\n",
    "# ================================\n",
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# 9.1 Cargar artefactos\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "feat_path  = os.path.join(ARTIF_DIR,  \"feature_order.json\")\n",
    "meta_path  = os.path.join(ARTIF_DIR,  \"metadata.json\")\n",
    "te_path    = os.path.join(ARTIF_DIR,  \"target_encoding.pkl\")\n",
    "\n",
    "model        = joblib.load(model_path)\n",
    "feature_order = json.load(open(feat_path, \"r\", encoding=\"utf-8\"))[\"feature_order\"]\n",
    "META          = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Target Encoding (si exist√≠a)\n",
    "TE = None\n",
    "if os.path.exists(te_path):\n",
    "    TE = joblib.load(te_path)  # dict {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = TE[\"mappings\"]\n",
    "    defaults = TE[\"defaults\"]\n",
    "    te_cols  = list(mappings.keys())\n",
    "else:\n",
    "    mappings, defaults, te_cols = {}, {}, []\n",
    "\n",
    "# Agregados hist√≥ricos (si existen)\n",
    "agg_tables = META.get(\"agg_tables\", {})  # {pref: path_csv}\n",
    "aggs = {pref: pd.read_csv(path) for pref, path in agg_tables.items() if os.path.exists(path)}\n",
    "\n",
    "# Umbrales\n",
    "thr_base  = META[\"thresholds\"][\"base_05\"] or 0.5\n",
    "thr_bestf = META[\"thresholds\"][\"best_f1\"] if META[\"thresholds\"][\"best_f1\"] is not None else thr_base\n",
    "\n",
    "best_iter = META.get(\"best_iteration\", None)\n",
    "global_mean = META.get(\"global_mean\", None)\n",
    "print(\"Artefactos cargados. Features esperadas:\", len(feature_order))\n",
    "print(\"TE cols:\", te_cols, \"| Agregados:\", list(aggs.keys()))\n",
    "print(\"Umbrales -> base:\", thr_base, \"| best_f1:\", thr_bestf, \"| best_iter:\", best_iter)\n",
    "\n",
    "# 9.2 Helpers para reconstruir features si vienen crudas (producci√≥n)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # MINUTO_DIA_SALIDA + SALIDA_SIN/COS (si no vienen)\n",
    "    if \"SALIDA_SIN\" not in out.columns or \"SALIDA_COS\" not in out.columns:\n",
    "        if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "            hs = (out[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "            ms = (out[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "            out[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(\"int16\")\n",
    "        if \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "            out[\"SALIDA_SIN\"] = np.sin(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "            out[\"SALIDA_COS\"] = np.cos(2*np.pi*out[\"MINUTO_DIA_SALIDA\"]/1440).astype(\"float32\")\n",
    "\n",
    "    # DISTANCIA_HAV (si no viene) ‚Äî requiere coords\n",
    "    needs_dist = \"DISTANCIA_HAV\" not in out.columns\n",
    "    has_coords = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns)\n",
    "    if needs_dist and has_coords:\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # Tipos b√°sicos\n",
    "    if \"MONTH\" in out:        out[\"MONTH\"]        = out[\"MONTH\"].astype(\"int16\", errors=\"ignore\")\n",
    "    if \"DAY_OF_WEEK\" in out:  out[\"DAY_OF_WEEK\"]  = out[\"DAY_OF_WEEK\"].astype(\"int16\", errors=\"ignore\")\n",
    "    for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]:\n",
    "        if c in out:\n",
    "            out[c] = out[c].astype(\"object\")  # TE espera object/str para mapear\n",
    "\n",
    "    return out\n",
    "\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "        # Quitamos la cruda si no estaba en feature_order\n",
    "        if c not in feature_order and c in out.columns:\n",
    "            out.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        # Detectar las llaves del agregado por los nombres de columnas del CSV\n",
    "        keys = [c for c in agg_df.columns if c.endswith((\"_AIRLINE\",\"_DEST\",\"_ORIG\",\"_RUTA\")) or c in [\"AIRLINE\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\",\"RUTA\",\"HORA_SALIDA\"]]\n",
    "        # Si no detecta, intenta heur√≠sticas por prefijo\n",
    "        if not keys:\n",
    "            if pref == \"AIR\":  keys = [\"AIRLINE\"]\n",
    "            if pref == \"DES\":  keys = [\"DESTINATION_AIRPORT\"]\n",
    "            if pref == \"ORI\":  keys = [\"ORIGIN_AIRPORT\"]\n",
    "            if pref == \"RUTA\": keys = [\"RUTA\"]\n",
    "            if pref == \"RUTA_HORA\": keys = [\"RUTA\",\"HORA_SALIDA\"]\n",
    "        # Merge left\n",
    "        out = out.merge(agg_df, on=keys, how=\"left\")\n",
    "        # Completar nulos con valores razonables\n",
    "        rate_col = f\"{pref}_rate\"; n_col = f\"{pref}_n\"\n",
    "        if rate_col in out:\n",
    "            fill_rate = global_mean if global_mean is not None else out[rate_col].mean()\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out:\n",
    "            out[n_col]    = out[n_col].fillna(0).astype(\"int32\")\n",
    "    return out\n",
    "\n",
    "def align_features(df: pd.DataFrame, order: list[str]) -> pd.DataFrame:\n",
    "    # Crea cualquier falta con 0.0, y descarta extras\n",
    "    X = df.copy()\n",
    "    for c in order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0.0\n",
    "    X = X[order]\n",
    "    # Dtypes suaves para LightGBM\n",
    "    for c in X.select_dtypes(include=\"float\").columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    for c in X.select_dtypes(include=\"int\").columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d05a2",
   "metadata": {},
   "source": [
    "Paso 10 ¬∑ Funci√≥n de scoring (proba + clase) y ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ba6bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Artefactos cargados | Modelo: lgbm_retrasos.pkl | Features: 23 | Agregados: 5 | TE cols: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba_retraso</th>\n",
       "      <th>pred_clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   proba_retraso  pred_clase\n",
       "0         0.2113           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 10) Scoring / Inferencia: cargar artefactos y predecir\n",
    "# ============================================\n",
    "import os, json, joblib, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----- 10.1 Cargar artefactos guardados en el paso 8 -----\n",
    "BASE_DIR   = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "# Modelo LightGBM\n",
    "model_path = os.path.join(MODELS_DIR, \"lgbm_retrasos.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Metadatos (umbrales, best_iteration, rutas de agregados, etc.)\n",
    "meta_path = os.path.join(ARTIF_DIR, \"metadata.json\")\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    META = json.load(f)\n",
    "\n",
    "best_iter     = META.get(\"best_iteration\", None)\n",
    "feature_order = None\n",
    "thr_base      = META.get(\"thresholds\", {}).get(\"base_05\", 0.5)\n",
    "thr_bestf     = META.get(\"thresholds\", {}).get(\"best_f1\", None)\n",
    "global_mean   = META.get(\"global_mean\", None)  # media global del target (para TE y agregados)\n",
    "\n",
    "# Orden de features (columna a columna, EXACTO como el modelo fue entrenado)\n",
    "feat_path = META.get(\"feature_order_path\", os.path.join(ARTIF_DIR, \"feature_order.json\"))\n",
    "with open(feat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    feature_order = json.load(f)[\"feature_order\"]\n",
    "\n",
    "# Target Encoding (mappings y defaults)\n",
    "te_path = os.path.join(ARTIF_DIR, \"target_encoding.pkl\")\n",
    "mappings, defaults, te_cols = {}, {}, []\n",
    "if os.path.exists(te_path):\n",
    "    te_obj   = joblib.load(te_path)  # {\"mappings\": {...}, \"defaults\": {...}}\n",
    "    mappings = te_obj.get(\"mappings\", {})\n",
    "    defaults = te_obj.get(\"defaults\", {})\n",
    "    te_cols  = list(mappings.keys())\n",
    "\n",
    "# Agregados hist√≥ricos (tablas CSV por prefijo)\n",
    "# META[\"agg_tables\"] = dict { \"AIR\": \".../agg_AIR.csv\", \"ORI\": \"...\", ... }\n",
    "aggs = {}\n",
    "for pref, csv_path in META.get(\"agg_tables\", {}).items():\n",
    "    try:\n",
    "        aggs[pref] = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se pudo cargar agregado {pref} desde {csv_path}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Artefactos cargados | Modelo: {os.path.basename(model_path)} | Features: {len(feature_order)} | Agregados: {len(aggs)} | TE cols: {len(te_cols)}\")\n",
    "\n",
    "# ----- 10.2 Utilidades de ingenier√≠a para inferencia -----\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "def add_engineered_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crea/asegura columnas derivadas necesarias en producci√≥n (sin usar el target).\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Reconstruir HORA_SALIDA/MIN_SALIDA/MINUTO_DIA_SALIDA si es necesario\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "    if \"MIN_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        out[\"MIN_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59).astype(\"int16\")\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and {\"HORA_SALIDA\",\"MIN_SALIDA\"}.issubset(out.columns):\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (out[\"HORA_SALIDA\"]*60 + out[\"MIN_SALIDA\"]).astype(\"int32\")\n",
    "\n",
    "    # SALIDA_SIN / SALIDA_COS (codificaci√≥n c√≠clica)\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_SIN\"] = np.sin(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"SALIDA_COS\"] = np.cos(2*np.pi * out[\"MINUTO_DIA_SALIDA\"] / (24*60)).astype(\"float32\")\n",
    "\n",
    "    # RUTA\n",
    "    if \"RUTA\" not in out.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(out.columns):\n",
    "        out[\"RUTA\"] = out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "    # DISTANCIA_HAV si faltara y existen coordenadas\n",
    "    need_geo = {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}\n",
    "    if \"DISTANCIA_HAV\" not in out.columns and need_geo.issubset(out.columns):\n",
    "        out[\"DISTANCIA_HAV\"] = haversine_km(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "\n",
    "    # MONTH_SIN / MONTH_COS (estacionalidad)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi * out[\"MONTH\"]/12).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.3 Target Encoding (no elimina columnas crudas) -----\n",
    "def apply_target_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica TE y deja las columnas crudas intactas (las filtramos al alinear).\"\"\"\n",
    "    if not mappings:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for c in te_cols:\n",
    "        default = defaults.get(c, global_mean if global_mean is not None else 0.0)\n",
    "        out[f\"{c}_TE\"] = out[c].map(mappings[c]).fillna(default).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# ----- 10.4 Agregados hist√≥ricos (usa las llaves del CSV) -----\n",
    "def apply_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Une agregados guardados (prefijo -> CSV). Detecta llaves como todas las columnas\n",
    "    del CSV salvo {pref+'_rate', pref+'_n'}. Si falta 'HORA_SALIDA', la reconstruye\n",
    "    a partir de SCHEDULED_DEPARTURE.\n",
    "    \"\"\"\n",
    "    if not aggs:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    for pref, agg_df in aggs.items():\n",
    "        rate_col = f\"{pref}_rate\"\n",
    "        n_col    = f\"{pref}_n\"\n",
    "        metric_cols = {rate_col, n_col}\n",
    "        key_cols = [c for c in agg_df.columns if c not in metric_cols]\n",
    "\n",
    "        # Reconstruye HORA_SALIDA si el agregado lo requiere\n",
    "        if \"HORA_SALIDA\" in key_cols and \"HORA_SALIDA\" not in out.columns:\n",
    "            if \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "                out[\"HORA_SALIDA\"] = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Saltando agregado {pref}: falta HORA_SALIDA y no hay SCHEDULED_DEPARTURE.\")\n",
    "                continue\n",
    "\n",
    "        missing = [c for c in key_cols if c not in out.columns]\n",
    "        if missing:\n",
    "            print(f\"‚ö†Ô∏è Saltando agregado {pref}: faltan llaves {missing}.\")\n",
    "            continue\n",
    "\n",
    "        out = out.merge(agg_df, on=key_cols, how=\"left\")\n",
    "\n",
    "        if rate_col in out.columns:\n",
    "            fill_rate = (global_mean if global_mean is not None \n",
    "                         else out[rate_col].mean(skipna=True))\n",
    "            out[rate_col] = out[rate_col].fillna(fill_rate).astype(\"float32\")\n",
    "        if n_col in out.columns:\n",
    "            out[n_col] = out[n_col].fillna(0).astype(\"int32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ----- 10.5 Alinear features al orden del modelo -----\n",
    "def align_features(df: pd.DataFrame, feature_order: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mantiene solo las columnas que el modelo espera (en su orden).\n",
    "    Rellena faltantes con 0.0 para flotantes y 0 para enteros.\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            # default general: 0.0\n",
    "            X[c] = 0.0\n",
    "    X = X[feature_order]\n",
    "\n",
    "    # NaNs -> 0.0 (float) / 0 (int)\n",
    "    for c in X.columns:\n",
    "        if pd.api.types.is_float_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "        elif pd.api.types.is_integer_dtype(X[c]):\n",
    "            X[c] = X[c].fillna(0).astype(X[c].dtype)\n",
    "        else:\n",
    "            # si qued√≥ categ√≥rica/objeto por accidente, convi√©rtela a string vac√≠a y luego codifica a 0.0\n",
    "            X[c] = X[c].astype(str).fillna(\"\")\n",
    "    return X\n",
    "\n",
    "# ----- 10.6 Funci√≥n de scoring √∫nica -----\n",
    "def score_retraso(df_raw: pd.DataFrame, threshold: float = None):\n",
    "    \"\"\"\n",
    "    df_raw: DataFrame crudo de entrada (puede traer SCHEDULED_DEPARTURE y coords).\n",
    "    threshold: None usa el umbral best_f1 (si existe) o 0.5.\n",
    "    Devuelve: (probabilidades, clases, X_final)\n",
    "    \"\"\"\n",
    "    thr = threshold if threshold is not None else (thr_bestf if thr_bestf is not None else thr_base)\n",
    "\n",
    "    # 1) Ingenier√≠a necesaria\n",
    "    df1 = add_engineered_columns(df_raw)\n",
    "\n",
    "    # 2) Agregados hist√≥ricos (usan llaves crudas, por eso VAN ANTES del TE)\n",
    "    df2 = apply_aggregates(df1)\n",
    "\n",
    "    # 3) Target Encoding (a√±ade *_TE y mantiene crudas)\n",
    "    df3 = apply_target_encoding(df2)\n",
    "\n",
    "    # 4) Alinear al orden de features del modelo\n",
    "    X = align_features(df3, feature_order)\n",
    "\n",
    "    # 5) Predecir\n",
    "    proba = model.predict_proba(X, num_iteration=best_iter)[:, 1]\n",
    "    yhat  = (proba >= thr).astype(int)\n",
    "    return proba, yhat, X\n",
    "\n",
    "# ----- 10.7 Ejemplo de inferencia -----\n",
    "# Ajusta este ejemplo con rutas reales ORIGEN/DESTINO/AIRLINE de tu dataset\n",
    "ejemplo = pd.DataFrame([dict(\n",
    "    MONTH=10, DAY_OF_WEEK=5,\n",
    "    AIRLINE=\"WN\",\n",
    "    ORIGIN_AIRPORT=\"LAX\", DESTINATION_AIRPORT=\"LAS\",\n",
    "    SCHEDULED_DEPARTURE=1425,                    # HHMM\n",
    "    ORIGEN_LAT=33.9425, ORIGEN_LON=-118.4081,    # LAX\n",
    "    DEST_LAT=36.0801,  DEST_LON=-115.1522        # LAS\n",
    ")])\n",
    "\n",
    "proba, clase, X_infer = score_retraso(ejemplo, threshold=None)  # usa best_f1 si existe\n",
    "pd.DataFrame({\n",
    "    \"proba_retraso\": proba.round(4),\n",
    "    \"pred_clase\": clase\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e9d8e",
   "metadata": {},
   "source": [
    "A) XGBoost (binario, desbalance con scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3abb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.59114\n",
      "[400]\tvalid-auc:0.60046\n",
      "[409]\tvalid-auc:0.60037\n",
      "AUC valid=0.6048 | best_iter=10 | tiempo=4.2 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58746\n",
      "[400]\tvalid-auc:0.60018\n",
      "[417]\tvalid-auc:0.60026\n",
      "AUC valid=0.6051 | best_iter=17 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42}\n",
      "[0]\tvalid-auc:0.58871\n",
      "[400]\tvalid-auc:0.59957\n",
      "[543]\tvalid-auc:0.59910\n",
      "AUC valid=0.6004 | best_iter=143 | tiempo=6.1 min\n",
      "\n",
      "=== XGB MEJOR ===\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'auc', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'lambda': 5.0, 'alpha': 0.0, 'scale_pos_weight': 4.337963077931689, 'nthread': -1, 'seed': 42} | AUC: 0.6051 | best_iter: 17\n",
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7214 | Precision: 0.2462 | Recall: 0.2977 | F1: 0.2695 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[624549 146646]\n",
      " [112995  47894]]\n",
      "\n",
      "== Mejor F1 (thr=0.420) ==\n",
      "Accuracy: 0.4981 | Precision: 0.2106 | Recall: 0.6939 | F1: 0.3231 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[352603 418592]\n",
      " [ 49248 111641]]\n",
      "‚Üí Umbral F1 √≥ptimo (XGB): {'thr': 0.42, 'f1': 0.32307175867647103}\n",
      "\n",
      "== Precisi√≥n ‚â≥ 0.30 (aprox) (thr=0.499) ==\n",
      "Accuracy: 0.7203 | Precision: 0.2458 | Recall: 0.3000 | F1: 0.2702 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[623093 148102]\n",
      " [112622  48267]]\n",
      "\n",
      "== Recall ‚â• 0.70 (thr=0.050) ==\n",
      "Accuracy: 0.1726 | Precision: 0.1726 | Recall: 1.0000 | F1: 0.2944 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[     0 771195]\n",
      " [     0 160889]]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# XGBoost: entrenamiento\n",
    "# =========================\n",
    "import time, numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_model, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid_model, label=y_valid)\n",
    "\n",
    "grid = [\n",
    "    dict(eta=0.05, max_depth=8,   min_child_weight=5,  subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.05, max_depth=10,  min_child_weight=10, subsample=0.8, colsample_bytree=0.8),\n",
    "    dict(eta=0.03, max_depth=10,  min_child_weight=10, subsample=0.9, colsample_bytree=0.9),\n",
    "]\n",
    "\n",
    "best = {\"auc\": -1, \"params\": None, \"booster\": None, \"nrounds\": None}\n",
    "\n",
    "for p in grid:\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": p[\"eta\"],\n",
    "        \"max_depth\": p[\"max_depth\"],\n",
    "        \"min_child_weight\": p[\"min_child_weight\"],\n",
    "        \"subsample\": p[\"subsample\"],\n",
    "        \"colsample_bytree\": p[\"colsample_bytree\"],\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"nthread\": -1,\n",
    "        \"seed\": 42,\n",
    "        # \"tree_method\": \"hist\",      # CPU r√°pido\n",
    "        # \"device\": \"cuda\"            # si tienes GPU\n",
    "    }\n",
    "    print(\"Probando XGB:\", params)\n",
    "    t0 = time.time()\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=400,\n",
    "        verbose_eval=400\n",
    "    )\n",
    "    t1 = time.time()\n",
    "    proba = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1))\n",
    "    auc  = roc_auc_score(y_valid, proba)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={booster.best_iteration} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "    if auc > best[\"auc\"]:\n",
    "        best.update({\"auc\": auc, \"params\": params, \"booster\": booster, \"nrounds\": booster.best_iteration})\n",
    "\n",
    "print(\"=== XGB MEJOR ===\")\n",
    "print(best[\"params\"], \"| AUC:\", round(best[\"auc\"], 4), \"| best_iter:\", best[\"nrounds\"])\n",
    "\n",
    "# M√©tricas en umbrales (igual que LightGBM)\n",
    "valid_proba = best[\"booster\"].predict(dvalid, iteration_range=(0, best[\"nrounds\"]+1))\n",
    "_ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_f1[\"f1\"]:\n",
    "        best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1\")\n",
    "print(\"‚Üí Umbral F1 √≥ptimo (XGB):\", best_f1)\n",
    "\n",
    "# Objetivos de negocio (opcionales)\n",
    "thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "_ = report_metrics(y_valid, valid_proba, thr_prec, \"Precisi√≥n ‚â≥ 0.30 (aprox)\")\n",
    "thr_rec = 0.05\n",
    "for thr in np.linspace(0.05, 0.4, 71):\n",
    "    if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "        thr_rec = float(thr); break\n",
    "_ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall ‚â• 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258a578",
   "metadata": {},
   "source": [
    "B) CatBoost (usa mismas features num√©ricas) **NO SE USA DIO ERROR**\n",
    "\n",
    "Podr√≠amos explotar categor√≠as nativas, pero como se hizo Target Encoding, lo m√°s estable es alimentar a CatBoost con el mismo set num√©rico que LGBM/XGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36f0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # CatBoost: entrenamiento\n",
    "# # =========================\n",
    "# import time, numpy as np\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "# from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "#     y_hat = (y_prob >= thr).astype(int)\n",
    "#     acc = accuracy_score(y_true, y_hat)\n",
    "#     pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "#     rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "#     f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "#     auc = roc_auc_score(y_true, y_prob)\n",
    "#     cm  = confusion_matrix(y_true, y_hat)\n",
    "#     print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "#     print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "#     print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "#     return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=float(thr))\n",
    "\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# scale_pos_weight = max(neg/max(pos,1), 1.0)\n",
    "\n",
    "# train_pool = Pool(X_train_model, label=y_train)\n",
    "# valid_pool = Pool(X_valid_model, label=y_valid)\n",
    "\n",
    "# grid = [\n",
    "#     dict(learning_rate=0.05, depth=8,  l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "#     dict(learning_rate=0.05, depth=10, l2_leaf_reg=5.0, bagging_temperature=0.5),\n",
    "#     dict(learning_rate=0.03, depth=10, l2_leaf_reg=8.0, bagging_temperature=1.0),\n",
    "# ]\n",
    "\n",
    "# best = {\"auc\": -1, \"model\": None, \"params\": None}\n",
    "\n",
    "# for p in grid:\n",
    "#     params = dict(\n",
    "#         loss_function=\"Logloss\",\n",
    "#         eval_metric=\"AUC\",\n",
    "#         iterations=12000,\n",
    "#         learning_rate=p[\"learning_rate\"],\n",
    "#         depth=p[\"depth\"],\n",
    "#         l2_leaf_reg=p[\"l2_leaf_reg\"],\n",
    "#         bagging_temperature=p[\"bagging_temperature\"],\n",
    "#         random_seed=42,\n",
    "#         od_type=\"Iter\",     # early stopping\n",
    "#         od_wait=400,\n",
    "#         verbose=400,\n",
    "#         class_weights=[1.0, scale_pos_weight],  # desbalance\n",
    "#         # task_type=\"GPU\", devices=\"0\",       # si tienes GPU\n",
    "#     )\n",
    "#     print(\"Probando CAT:\", params)\n",
    "#     t0 = time.time()\n",
    "#     model_cb = CatBoostClassifier(**params)\n",
    "#     model_cb.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "#     t1 = time.time()\n",
    "\n",
    "#     proba = model_cb.predict_proba(valid_pool)[:,1]\n",
    "#     auc   = roc_auc_score(y_valid, proba)\n",
    "#     print(f\"AUC valid={auc:.4f} | best_iter={model_cb.get_best_iteration()} | tiempo={(t1-t0)/60:.1f} min\\n\")\n",
    "#     if auc > best[\"auc\"]:\n",
    "#         best.update({\"auc\": auc, \"model\": model_cb, \"params\": params})\n",
    "\n",
    "# print(\"=== CAT MEJOR ===\")\n",
    "# print(best[\"params\"][\"learning_rate\"], best[\"params\"][\"depth\"], \"| AUC:\", round(best[\"auc\"],4),\n",
    "#       \"| best_iter:\", best[\"model\"].get_best_iteration())\n",
    "\n",
    "# valid_proba = best[\"model\"].predict_proba(valid_pool)[:,1]\n",
    "# _ = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# best_f1 = {\"thr\":0.5, \"f1\":-1}\n",
    "# for thr in np.linspace(0.05, 0.5, 46):\n",
    "#     f1 = f1_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0)\n",
    "#     if f1 > best_f1[\"f1\"]:\n",
    "#         best_f1 = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "# _ = report_metrics(y_valid, valid_proba, best_f1[\"thr\"], \"Mejor F1 (CatBoost)\")\n",
    "# print(\"‚Üí Umbral F1 √≥ptimo (CAT):\", best_f1)\n",
    "\n",
    "# thr_prec = np.quantile(valid_proba[y_valid==1], 0.70) if (y_valid==1).sum() else best_f1[\"thr\"]\n",
    "# _ = report_metrics(y_valid, valid_proba, thr_prec, \"Precisi√≥n ‚â≥ 0.30 (aprox)\")\n",
    "# thr_rec = 0.05\n",
    "# for thr in np.linspace(0.05, 0.4, 71):\n",
    "#     if recall_score(y_valid, (valid_proba>=thr).astype(int), zero_division=0) >= 0.70:\n",
    "#         thr_rec = float(thr); break\n",
    "# _ = report_metrics(y_valid, valid_proba, thr_rec, \"Recall ‚â• 0.70\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198edf48",
   "metadata": {},
   "source": [
    "Random Forest (sklearn) ‚Äî Entrenar, evaluar y guardar **Esta Opci√≥n Saturo la memoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f92a4fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Entrenando RF (base)‚Ä¶\n",
      "RF AUC valid=0.6026 | tiempo=40.0 min\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8141 | Precision: 0.2983 | Recall: 0.0568 | F1: 0.0954 | ROC-AUC: 0.6026\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[749712  21483]\n",
      " [151756   9133]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.220) ==\n",
      "Accuracy: 0.5059 | Precision: 0.2103 | Recall: 0.6758 | F1: 0.3207 | ROC-AUC: 0.6026\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[362788 408407]\n",
      " [ 52157 108732]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.22000000000000003, 'f1': 0.32073011734028684}\n",
      "\n",
      "Probando RF: {'n_estimators': 300, 'max_depth': 24, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "RF AUC valid=0.6072 | tiempo=23.3 min\n",
      "\n",
      "Probando RF: {'n_estimators': 500, 'max_depth': None, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 134217728 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ~~~~~~~~~^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 147, in __call__\n    return self.function(*args, **kwargs)\n           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n    tree._fit(\n    ~~~~~~~~~^\n        X,\n        ^^\n    ...<3 lines>...\n        missing_values_in_feature_mask=missing_values_in_feature_mask,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"sklearn/tree/_tree.pyx\", line 141, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn/tree/_tree.pyx\", line 256, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"sklearn/tree/_tree.pyx\", line 911, in sklearn.tree._tree.Tree._add_node\n  File \"sklearn/tree/_tree.pyx\", line 879, in sklearn.tree._tree.Tree._resize_c\n  File \"sklearn/tree/_utils.pyx\", line 29, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 134217728 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m params = base_params.copy(); params.update(cfg)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProbando RF:\u001b[39m\u001b[33m\"\u001b[39m, {k: params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmin_samples_leaf\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmax_features\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m mdl, proba, auc = \u001b[43mtrain_eval_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > best_rf[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     66\u001b[39m     best_rf = {\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m: auc, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: mdl, \u001b[33m\"\u001b[39m\u001b[33mproba\u001b[39m\u001b[33m\"\u001b[39m: proba, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: params}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_eval_rf\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     27\u001b[39m rf = RandomForestClassifier(**params)\n\u001b[32m     28\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m secs = time.time() - t0\n\u001b[32m     31\u001b[39m proba = rf.predict_proba(X_valid_model)[:, \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: could not allocate 134217728 bytes"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Random Forest: entrenamiento\n",
    "# ============================\n",
    "import time, os, json, joblib, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# --- 1) Manejo del desbalance con class_weight ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "class_w = {0: 1.0, 1: float(scale_pos_weight)}   # alternativa: \"balanced_subsample\"\n",
    "\n",
    "# --- 2) Par√°metros base (buen compromiso calidad/tiempo/memoria) ---\n",
    "base_params = dict(\n",
    "    n_estimators=400,          # subir a 600/800 si tienes m√°s tiempo\n",
    "    max_depth=None,            # None = √°rboles profundos; si RAM justa, prueba 18-24\n",
    "    min_samples_leaf=2,        # ayuda a generalizar y bajar varianza\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",       # t√≠pico en RF\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight=class_w\n",
    ")\n",
    "\n",
    "def train_eval_rf(params):\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    rf.fit(X_train_model, y_train)\n",
    "    secs = time.time() - t0\n",
    "    proba = rf.predict_proba(X_valid_model)[:, 1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(f\"RF AUC valid={auc:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return rf, proba, auc\n",
    "\n",
    "print(\"‚Üí Entrenando RF (base)‚Ä¶\")\n",
    "rf, rf_proba, rf_auc = train_eval_rf(base_params)\n",
    "\n",
    "# --- 3) Barrido peque√±o de umbral para F1 (puedes reutilizar tu l√≥gica existente) ---\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best)\n",
    "\n",
    "# (Opcional) Si tu objetivo operativo es recall alto o precisi√≥n ‚â• 0.30,\n",
    "# puedes reutilizar tus funciones de umbrales de negocio con rf_proba:\n",
    "# _ = report_metrics(y_valid, rf_proba, thr_prec_calculado, \"RF Precisi√≥n ‚â• 0.30\")\n",
    "# _ = report_metrics(y_valid, rf_proba, thr_rec_calculado, \"RF Recall ‚â• 0.70\")\n",
    "\n",
    "# --- 4) Mini tuning r√°pido (opcional, 2-3 pruebas sin reventar la RAM/tiempo) ---\n",
    "quick_grid = [\n",
    "    dict(n_estimators=300, max_depth=24, min_samples_leaf=2, max_features=\"sqrt\"),\n",
    "    dict(n_estimators=500, max_depth=None, min_samples_leaf=2, max_features=\"sqrt\"),\n",
    "    dict(n_estimators=400, max_depth=18, min_samples_leaf=3, max_features=\"sqrt\"),\n",
    "]\n",
    "best_rf = {\"auc\": rf_auc, \"model\": rf, \"proba\": rf_proba, \"params\": base_params}\n",
    "for cfg in quick_grid:\n",
    "    params = base_params.copy(); params.update(cfg)\n",
    "    print(\"\\nProbando RF:\", {k: params[k] for k in [\"n_estimators\",\"max_depth\",\"min_samples_leaf\",\"max_features\"]})\n",
    "    mdl, proba, auc = train_eval_rf(params)\n",
    "    if auc > best_rf[\"auc\"]:\n",
    "        best_rf = {\"auc\": auc, \"model\": mdl, \"proba\": proba, \"params\": params}\n",
    "\n",
    "print(\"\\n=== RF MEJOR ===\")\n",
    "print(best_rf[\"params\"], \"| AUC:\", best_rf[\"auc\"])\n",
    "\n",
    "# Recalcular umbral F1 con el mejor modelo\n",
    "best_thr = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (best_rf[\"proba\"] >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best_thr[\"f1\"]:\n",
    "        best_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_rf[\"proba\"], 0.5, \"RF(best) Base 0.5\")\n",
    "_ = report_metrics(y_valid, best_rf[\"proba\"], best_thr[\"thr\"], \"RF(best) Mejor F1\")\n",
    "print(\"‚Üí RF(best) umbral F1 √≥ptimo:\", best_thr)\n",
    "\n",
    "# ============================\n",
    "# Guardado de artefactos RF\n",
    "# ============================\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "rf_path = os.path.join(MODELS_DIR, \"rf_retrasos.pkl\")\n",
    "joblib.dump(best_rf[\"model\"], rf_path)\n",
    "print(f\"‚úÖ RF guardado en: {rf_path}\")\n",
    "\n",
    "# Guardar orden de features para inferencia (reutilizamos el que ya generaste con LGBM)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    # si no existe por alguna raz√≥n, lo creamos\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(X_train_model.columns)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Metadatos espec√≠ficos del RF\n",
    "rf_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": rf_path,\n",
    "    \"model_type\": \"random_forest\",\n",
    "    \"auc_valid\": float(best_rf[\"auc\"]),\n",
    "    \"best_thr_f1\": float(best_thr[\"thr\"]),\n",
    "    \"feature_order_path\": feat_path\n",
    "}\n",
    "rf_meta_path = os.path.join(ARTIF_DIR, \"metadata_rf.json\")\n",
    "with open(rf_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rf_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Metadatos RF guardados en: {rf_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec875f67",
   "metadata": {},
   "source": [
    "(Opcional) Probabilidades mejor calibradas\n",
    "\n",
    "Los bosques tienden a dar probabilidades poco calibradas. Si lo necesitas para umbrales finos/reportes, puedes calibrar con sigmoid sobre una muestra del validation (para no morir en memoria/tiempo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Calibraci√≥n (sigmoid) opcional\n",
    "# ============================\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Usamos el mejor RF ya entrenado (prefit) y calibramos en un subset del valid\n",
    "cal_idx = np.random.RandomState(42).choice(len(X_valid_model), size=min(200_000, len(X_valid_model)), replace=False)\n",
    "X_cal = X_valid_model.iloc[cal_idx]\n",
    "y_cal = y_valid.iloc[cal_idx]\n",
    "\n",
    "cal = CalibratedClassifierCV(best_rf[\"model\"], method=\"sigmoid\", cv=\"prefit\")\n",
    "t0 = time.time()\n",
    "cal.fit(X_cal, y_cal)\n",
    "t1 = time.time()\n",
    "print(f\"‚úÖ Calibrado en {(t1-t0):.1f}s\")\n",
    "\n",
    "cal_proba = cal.predict_proba(X_valid_model)[:,1]\n",
    "cal_auc   = roc_auc_score(y_valid, cal_proba)\n",
    "print(f\"AUC valid calibrado={cal_auc:.4f}\")\n",
    "\n",
    "# Re-barrido de umbral para F1 con probabilidades calibradas\n",
    "best_cal = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (cal_proba >= thr).astype(int), zero_division=0)\n",
    "    if f1 > best_cal[\"f1\"]:\n",
    "        best_cal = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, cal_proba, 0.5, \"RF Calibrado Base 0.5\")\n",
    "_ = report_metrics(y_valid, cal_proba, best_cal[\"thr\"], \"RF Calibrado Mejor F1\")\n",
    "print(\"‚Üí RF Calibrado umbral F1 √≥ptimo:\", best_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888c06b",
   "metadata": {},
   "source": [
    "**Sustituye Radom Forest anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b453e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Entrenando tanda 1/4: +100 √°rboles (total=100)\n",
      "‚Üí Entrenando tanda 2/4: +100 √°rboles (total=200)\n",
      "‚Üí Entrenando tanda 3/4: +100 √°rboles (total=300)\n",
      "‚Üí Entrenando tanda 4/4: +100 √°rboles (total=400)\n",
      "‚úì RF AUC valid=0.6103 | tiempo=47.7 min | √°rboles=400\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7723 | Precision: 0.2702 | Recall: 0.1878 | F1: 0.2216 | ROC-AUC: 0.6103\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[689606  81589]\n",
      " [130679  30210]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.310) ==\n",
      "Accuracy: 0.5029 | Precision: 0.2121 | Recall: 0.6926 | F1: 0.3248 | ROC-AUC: 0.6103\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[357344 413851]\n",
      " [ 49461 111428]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.31, 'f1': 0.32478343496053447}\n",
      "‚úÖ RF guardado en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\models\\rf_retrasos.pkl\n",
      "‚úÖ Metadatos RF guardados en: d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\artifacts\\metadata_rf.json\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Random Forest: entrenamiento (memoria-seguro)\n",
    "# ============================\n",
    "import os, json, time, joblib, numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# --- utilidades m√©tricas (reusa tus helpers si ya las tienes) ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def scan_best_f1(y_true, proba, lo=0.05, hi=0.5, steps=46):\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (proba >= thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# --- 0) Downcasting agresivo para ahorrar RAM ---\n",
    "def _downcast_df(df):\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        if np.issubdtype(df2[c].dtype, np.floating):\n",
    "            df2[c] = df2[c].astype(np.float32)\n",
    "        elif np.issubdtype(df2[c].dtype, np.integer):\n",
    "            # cuentas/aggregates -> int32 es suficiente\n",
    "            if str(df2[c].dtype) not in (\"int8\", \"int16\", \"int32\"):\n",
    "                df2[c] = df2[c].astype(np.int32)\n",
    "    return df2\n",
    "\n",
    "Xtr = _downcast_df(X_train_model)\n",
    "Xva = _downcast_df(X_valid_model)\n",
    "\n",
    "# --- 1) Desbalance via class_weight (m√°s estable que sample_weight a esta escala) ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "class_w = {0: 1.0, 1: float(scale_pos_weight)}\n",
    "\n",
    "# --- 2) Par√°metros memoria-seguros ---\n",
    "# Claves para bajar el pico:\n",
    "# - n_jobs=4 (menos paralelismo => menos matrices en RAM al mismo tiempo)\n",
    "# - bootstrap=True + max_samples<1.0 (cada √°rbol entrena con una fracci√≥n aleatoria)\n",
    "# - max_depth limitado y hojas m√≠nimas mayores\n",
    "base_params = dict(\n",
    "    n_estimators=0,            # usamos warm_start, empezamos en 0\n",
    "    warm_start=True,\n",
    "    max_depth=22,              # si sigue alto el pico, prueba 18‚Äì20\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=6,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    max_samples=0.55,          # 55% de las filas por √°rbol\n",
    "    n_jobs=4,                  # reduce el pico; sube a 6‚Äì8 si tienes mucha RAM\n",
    "    random_state=42,\n",
    "    class_weight=class_w\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(**base_params)\n",
    "\n",
    "# --- 3) Entrenamiento por tandas (evita picos de memoria gordos) ---\n",
    "# Puedes ajustar: 4 tandas x 100 √°rboles = 400 √°rboles finales\n",
    "batches = [100, 100, 100, 100]\n",
    "t0 = time.time()\n",
    "for i, add_trees in enumerate(batches, start=1):\n",
    "    rf.n_estimators += add_trees\n",
    "    print(f\"‚Üí Entrenando tanda {i}/{len(batches)}: +{add_trees} √°rboles (total={rf.n_estimators})\")\n",
    "    rf.fit(Xtr, y_train)  # warm_start mantiene √°rboles previos\n",
    "\n",
    "secs = time.time() - t0\n",
    "proba_va = rf.predict_proba(Xva)[:, 1]\n",
    "auc_va   = roc_auc_score(y_valid, proba_va)\n",
    "print(f\"‚úì RF AUC valid={auc_va:.4f} | tiempo={secs/60:.1f} min | √°rboles={rf.n_estimators}\")\n",
    "\n",
    "# --- 4) Umbral F1 y reportes ---\n",
    "best = scan_best_f1(y_valid, proba_va)\n",
    "_ = report_metrics(y_valid, proba_va, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, proba_va, best[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best)\n",
    "\n",
    "# (Opcional) Otros objetivos de negocio:\n",
    "# thr_prec = 0.229  # si lo calculaste para Precisi√≥n‚âà0.30\n",
    "# _ = report_metrics(y_valid, proba_va, thr_prec, \"RF Precisi√≥n ‚â≥ 0.30\")\n",
    "# thr_rec  = 0.200  # si lo calculaste para Recall‚â•0.70\n",
    "# _ = report_metrics(y_valid, proba_va, thr_rec, \"RF Recall ‚â• 0.70\")\n",
    "\n",
    "# --- 5) Guardado artefactos RF ---\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "\n",
    "rf_path = os.path.join(MODELS_DIR, \"rf_retrasos.pkl\")\n",
    "joblib.dump(rf, rf_path)\n",
    "print(f\"‚úÖ RF guardado en: {rf_path}\")\n",
    "\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(Xtr.columns)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "rf_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": rf_path,\n",
    "    \"model_type\": \"random_forest\",\n",
    "    \"auc_valid\": float(auc_va),\n",
    "    \"best_thr_f1\": float(best[\"thr\"]),\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"params\": {\n",
    "        **{k: v for k, v in base_params.items() if k != \"n_estimators\"},\n",
    "        \"n_estimators\": int(rf.n_estimators)\n",
    "    }\n",
    "}\n",
    "rf_meta_path = os.path.join(ARTIF_DIR, \"metadata_rf.json\")\n",
    "with open(rf_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rf_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Metadatos RF guardados en: {rf_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734538a",
   "metadata": {},
   "source": [
    "selector de modelos (LightGBM, XGBoost, Random Forest y ExtraTrees) que:\n",
    "\n",
    "Entrena cada modelo con una config base razonable (r√°pida y estable para 5.2M filas).\n",
    "\n",
    "Eval√∫a ROC-AUC y hace un barrido de umbral para encontrar el mejor F1.\n",
    "\n",
    "Muestra un cuadro comparativo (AUC, F1@best_thr, tiempo).\n",
    "\n",
    "Selecciona el mejor por AUC (desempate por F1) y guarda artefactos compatibles con tu estructura actual:\n",
    "\n",
    "models/<mejor_modelo>.pkl\n",
    "\n",
    "artifacts/feature_order.json (si no existe)\n",
    "\n",
    "artifacts/summary_models.json (todos los resultados)\n",
    "\n",
    "artifacts/best_model.json (metadatos del ganador)\n",
    "\n",
    "Requisitos previos en tu notebook: tener listas las matrices X_train_model, X_valid_model, y_train, y_valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Selector de modelos: LGBM, XGBoost, RandomForest, ExtraTrees\n",
    "# ============================================================\n",
    "import os, json, time, joblib, numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "def ensure_dirs():\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "    MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "    ARTIF_DIR  = os.path.join(BASE_DIR, \"artifacts\")\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(ARTIF_DIR,  exist_ok=True)\n",
    "    return BASE_DIR, MODELS_DIR, ARTIF_DIR\n",
    "\n",
    "def scan_best_f1(y_true, proba, lo=0.05, hi=0.5, steps=46):\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        y_hat = (proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_true, y_hat, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "def quick_report(y_true, proba, thr, title=\"\"):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, proba)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "@dataclass\n",
    "class ModelResult:\n",
    "    name: str\n",
    "    model: Any\n",
    "    auc: float\n",
    "    f1: float\n",
    "    best_thr: float\n",
    "    secs: float\n",
    "    extra: Optional[Dict[str, Any]] = None\n",
    "\n",
    "def pick_winner(results):\n",
    "    # ganador por AUC (desempate por F1)\n",
    "    results_sorted = sorted(results, key=lambda r: (r.auc, r.f1), reverse=True)\n",
    "    return results_sorted[0], results_sorted\n",
    "\n",
    "# ---------- Entrenadores ----------\n",
    "def train_lgbm(X_tr, y_tr, X_va, y_va):\n",
    "    import lightgbm as lgb\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=12000,\n",
    "        num_leaves=127,\n",
    "        min_child_samples=100,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=5.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw\n",
    "    )\n",
    "    print(\"\\n‚Üí Entrenando LightGBM‚Ä¶\")\n",
    "    mdl = lgb.LGBMClassifier(**params)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"LGBM Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"LGBM Mejor F1\")\n",
    "    print(f\"‚úì LGBM AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min | best_iter={getattr(mdl,'best_iteration_',None)}\")\n",
    "    return ModelResult(\"lgbm\", mdl, auc, best[\"f1\"], best[\"thr\"], secs, extra={\"best_iter\": getattr(mdl,\"best_iteration_\",None)})\n",
    "\n",
    "def train_xgb(X_tr, y_tr, X_va, y_va):\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è  XGBoost no disponible, salto:\", e)\n",
    "        return None\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": 0.05,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 10,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"scale_pos_weight\": spw,\n",
    "        \"nthread\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    print(\"\\n‚Üí Entrenando XGBoost‚Ä¶\")\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=5000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        early_stopping_rounds=400,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"XGB Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(f\"‚úì XGB AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min | best_iter={mdl.best_iteration}\")\n",
    "    return ModelResult(\"xgboost\", mdl, auc, best[\"f1\"], best[\"thr\"], secs, extra={\"best_iter\": mdl.best_iteration})\n",
    "\n",
    "def train_rf(X_tr, y_tr, X_va, y_va):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    class_w = {0: 1.0, 1: float(spw)}\n",
    "    params = dict(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=class_w\n",
    "    )\n",
    "    print(\"\\n‚Üí Entrenando RandomForest‚Ä¶\")\n",
    "    t0 = time.time()\n",
    "    mdl = RandomForestClassifier(**params).fit(X_tr, y_tr)\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"RF Mejor F1\")\n",
    "    print(f\"‚úì RF AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return ModelResult(\"random_forest\", mdl, auc, best[\"f1\"], best[\"thr\"], secs)\n",
    "\n",
    "def train_extra(X_tr, y_tr, X_va, y_va):\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    neg = int((y_tr==0).sum()); pos = int((y_tr==1).sum())\n",
    "    spw = max(neg / max(pos,1), 1.0)\n",
    "    class_w = {0: 1.0, 1: float(spw)}\n",
    "    params = dict(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight=class_w\n",
    "    )\n",
    "    print(\"\\n‚Üí Entrenando ExtraTrees‚Ä¶\")\n",
    "    t0 = time.time()\n",
    "    mdl = ExtraTreesClassifier(**params).fit(X_tr, y_tr)\n",
    "    secs = time.time()-t0\n",
    "    proba = mdl.predict_proba(X_va)[:,1]\n",
    "    auc   = roc_auc_score(y_va, proba)\n",
    "    best  = scan_best_f1(y_va, proba)\n",
    "    _ = quick_report(y_va, proba, 0.5, \"ExtraTrees Base 0.5\")\n",
    "    _ = quick_report(y_va, proba, best[\"thr\"], \"ExtraTrees Mejor F1\")\n",
    "    print(f\"‚úì ExtraTrees AUC={auc:.4f} | best_thr={best['thr']:.3f} | F1@best={best['f1']:.4f} | tiempo={secs/60:.1f} min\")\n",
    "    return ModelResult(\"extra_trees\", mdl, auc, best[\"f1\"], best[\"thr\"], secs)\n",
    "\n",
    "# ---------- Ejecutar todo ----------\n",
    "BASE_DIR, MODELS_DIR, ARTIF_DIR = ensure_dirs()\n",
    "\n",
    "results = []\n",
    "# LightGBM\n",
    "try:\n",
    "    results.append(train_lgbm(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  Error LGBM:\", e)\n",
    "\n",
    "# XGBoost (si no est√° instalado, se salta)\n",
    "xgb_res = train_xgb(X_train_model, y_train, X_valid_model, y_valid)\n",
    "if xgb_res is not None:\n",
    "    results.append(xgb_res)\n",
    "\n",
    "# RandomForest\n",
    "try:\n",
    "    results.append(train_rf(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  Error RF:\", e)\n",
    "\n",
    "# ExtraTrees\n",
    "try:\n",
    "    results.append(train_extra(X_train_model, y_train, X_valid_model, y_valid))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  Error ExtraTrees:\", e)\n",
    "\n",
    "# Filtrar None por si alguno fall√≥\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "# Selecci√≥n del ganador\n",
    "winner, ranked = pick_winner(results)\n",
    "\n",
    "# ---------- Mostrar comparativa ----------\n",
    "import pandas as pd\n",
    "df_cmp = pd.DataFrame([{\n",
    "    \"modelo\": r.name,\n",
    "    \"AUC_valid\": round(r.auc, 6),\n",
    "    \"F1@best\": round(r.f1, 6),\n",
    "    \"best_thr\": round(r.best_thr, 4),\n",
    "    \"mins\": round(r.secs/60, 2),\n",
    "    **({\"best_iter\": r.extra.get(\"best_iter\")} if r.extra else {})\n",
    "} for r in ranked])\n",
    "print(\"\\n=== COMPARATIVA MODELOS (ordenada) ===\")\n",
    "display(df_cmp.sort_values([\"AUC_valid\",\"F1@best\"], ascending=False).reset_index(drop=True))\n",
    "\n",
    "print(f\"\\nüèÜ GANADOR: {winner.name} | AUC={winner.auc:.4f} | F1@best={winner.f1:.4f} | thr={winner.best_thr:.3f}\")\n",
    "\n",
    "# ---------- Guardar mejor modelo + metadatos ----------\n",
    "# 1) feature_order (si no existe)\n",
    "feat_path = os.path.join(ARTIF_DIR, \"feature_order.json\")\n",
    "if not os.path.exists(feat_path):\n",
    "    with open(feat_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(X_train_model.columns)}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Orden de features guardado en: {feat_path}\")\n",
    "\n",
    "# 2) Modelo\n",
    "best_model_path = os.path.join(MODELS_DIR, f\"{winner.name}_best.pkl\")\n",
    "joblib.dump(winner.model, best_model_path)\n",
    "print(f\"‚úÖ Modelo ganador guardado en: {best_model_path}\")\n",
    "\n",
    "# 3) Summary de todos los modelos\n",
    "summary_path = os.path.join(ARTIF_DIR, \"summary_models.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df_cmp.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Resumen comparativo guardado en: {summary_path}\")\n",
    "\n",
    "# 4) Metadatos del ganador\n",
    "best_meta = {\n",
    "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"winner\": winner.name,\n",
    "    \"model_path\": best_model_path,\n",
    "    \"AUC_valid\": float(winner.auc),\n",
    "    \"F1_best\": float(winner.f1),\n",
    "    \"best_threshold\": float(winner.best_thr),\n",
    "    \"feature_order_path\": feat_path,\n",
    "    \"extra\": winner.extra or {}\n",
    "}\n",
    "best_meta_path = os.path.join(ARTIF_DIR, \"best_model.json\")\n",
    "with open(best_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_meta, f, ensure_ascii=False, indent=2)\n",
    "print(f\"‚úÖ Metadatos del ganador guardados en: {best_meta_path}\")\n",
    "\n",
    "# ---------- Nota de uso en inferencia ----------\n",
    "print(\"\\n‚ÑπÔ∏è Para inferencia, usa el 'feature_order.json' para alinear columnas y carga el modelo ganador:\")\n",
    "print(f\"   - Modelo: {best_model_path}\")\n",
    "print(f\"   - Metadatos: {best_meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7a33a",
   "metadata": {},
   "source": [
    "###Revisi√≥n 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af27c88",
   "metadata": {},
   "source": [
    "Respeta tus columnas existentes (no recalcula si ya vienen), hace split sin fuga, aplica Target Encoding, agregados hist√≥ricos, entrena LGBM, XGB y RF, compara, y guarda solo en models/ dos archivos .joblib:\n",
    "\n",
    "models/<mejor_modelo>_retrasos.joblib ‚Üí el modelo entrenado\n",
    "\n",
    "models/artifacts.joblib ‚Üí TODO lo adicional (TE, agregados, orden de features, umbrales, best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee87c42",
   "metadata": {},
   "source": [
    "1 ‚Äî Imports, rutas y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087a5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1) Imports, rutas y utilidades\n",
    "# ================================\n",
    "import os, time, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Rutas del proyecto ----------\n",
    "PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cambia aqu√≠ si tu CSV est√° en otra ruta:\n",
    "CSV_PATH = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "# ---------- Configuraci√≥n general ----------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Columnas esperadas del CSV (seg√∫n tu lista)\n",
    "COLUMNS_EXPECTED = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"DEPARTURE_TIME\",\"DEPARTURE_DELAY\",\n",
    "    \"SCHEDULED_TIME\",\"DISTANCE\",\"SCHEDULED_ARRIVAL\",\"ARRIVAL_TIME\",\"ARRIVAL_DELAY\",\n",
    "    \"AIRLINE_NAME\",\"ORIGEN_AEROPUERTO\",\"ORIGEN_CIUDAD\",\"ORIGEN_ESTADO\",\"ORIGEN_LAT\",\"ORIGEN_LON\",\n",
    "    \"DEST_AEROPUERTO\",\"DEST_CIUDAD\",\"DEST_ESTADO\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"MOTIVO_RETRASO\",\"CANTIDAD_CAUSAS\",\"RETRASADO_LLEGADA\",\"RETRASADO_SALIDA\",\n",
    "    \"HORA_SALIDA\",\"HORA_LLEGADA\",\"MIN_SALIDA\",\"MIN_LLEGADA\",\"MINUTO_DIA_SALIDA\",\"MINUTO_DIA_LLEGADA\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\"LLEGADA_SIN\",\"LLEGADA_COS\",\"PERIODO_SALIDA\",\"PERIODO_LLEGADA\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eba5cd",
   "metadata": {},
   "source": [
    "2 ‚Äî Carga del CSV + dtypes seguros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea396dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cargado: (5231130, 41) | en 32.4s\n",
      "‚úì Columnas clave presentes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2) Cargar flights_clean.csv y optimizar tipos num√©ricos\n",
    "#     - NO recalcula columnas si ya existen en el CSV\n",
    "# =====================================================\n",
    "def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(path_csv)\n",
    "    # Downcast num√©ricos cuando sea seguro\n",
    "    for c in df.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    print(f\"‚úì Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "    return df\n",
    "\n",
    "df = load_flights(CSV_PATH)\n",
    "\n",
    "# Validaci√≥n soft de columnas clave\n",
    "faltantes = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "                         \"MINUTO_DIA_SALIDA\",\"SALIDA_SIN\",\"SALIDA_COS\",\"RETRASADO_LLEGADA\"] if c not in df.columns]\n",
    "if faltantes:\n",
    "    print(\"‚ö†Ô∏è Faltan columnas clave (se derivar√°n si es posible):\", faltantes)\n",
    "else:\n",
    "    print(\"‚úì Columnas clave presentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d3a80",
   "metadata": {},
   "source": [
    "3 ‚Äî Ensamblado de features base (respetando las ya calculadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc6180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231130, 45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 3) Features base: RUTA, se√±ales trigonom√©tricas y\n",
    "#    distancia Haversine (si faltara DISTANCE_HAV)\n",
    "# ==================================================\n",
    "def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"RUTA\" not in out.columns:\n",
    "        out[\"RUTA\"] = (out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    return out\n",
    "\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Si ya existen, no recalcular\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        # HHMM -> minutos del d√≠a\n",
    "        hh = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23)\n",
    "        mm = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59)\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (hh * 60 + mm).astype(\"int16\")\n",
    "\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "\n",
    "    # Se√±ales mensuales (√∫tiles y muy baratas)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Distancia Haversine (si la quieres y no la tienes en tu CSV)\n",
    "def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"DISTANCIA_HAV\" in out.columns:\n",
    "        return out\n",
    "    if not {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns):\n",
    "        return out\n",
    "\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        p1 = np.radians(lat1); p2 = np.radians(lat2)\n",
    "        dlat = p2 - p1\n",
    "        dlon = np.radians(lon2) - np.radians(lon1)\n",
    "        a = np.sin(dlat/2)**2 + np.cos(p1)*np.cos(p2)*np.sin(dlon/2)**2\n",
    "        return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "    out[\"DISTANCIA_HAV\"] = haversine(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "df = add_route(df)\n",
    "df = ensure_time_features(df)\n",
    "df = ensure_distance(df)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522de6ef",
   "metadata": {},
   "source": [
    "4 ‚Äî Split sin fuga + matrices base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef22fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 45) (932084, 45) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Split temporal sin fuga (train meses 1-9, valid 10-12)\n",
    "#    y definici√≥n de matrices base para modelado\n",
    "# ============================================================\n",
    "target_col = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# M√°scaras de tiempo (2015)\n",
    "train_mask = (df[\"MONTH\"] >= 1) & (df[\"MONTH\"] <= 9)\n",
    "valid_mask = (df[\"MONTH\"] >= 10) & (df[\"MONTH\"] <= 12)\n",
    "\n",
    "v_train = df.loc[train_mask].copy()\n",
    "v_valid = df.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Shapes ->\", v_train.shape, v_valid.shape,\n",
    "      \"| rate train\", v_train[target_col].mean(),\n",
    "      \"| rate valid\", v_valid[target_col].mean())\n",
    "\n",
    "# Conjunto m√≠nimo de features base (num√©ricas + derivadas)\n",
    "BASE_FEATS = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# Columnas categ√≥ricas para Target Encoding\n",
    "TE_COLS = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "\n",
    "# Construimos X/y de partida (solo num√©ricas; TE/Agregados luego)\n",
    "X_train = v_train[BASE_FEATS].copy()\n",
    "X_valid = v_valid[BASE_FEATS].copy()\n",
    "y_train = v_train[target_col].astype(\"int8\")\n",
    "y_valid = v_valid[target_col].astype(\"int8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71f7b2",
   "metadata": {},
   "source": [
    "5 ‚Äî Target Encoding (sin fuga) y agregados hist√≥ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172cb1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 27) (932084, 27)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5) Target Encoding y Agregados (sin fuga)\n",
    "#     - TE: medias suavizadas por categor√≠a\n",
    "#     - Agregs: tasas y conteos por llaves (AIR, ORI, DES, RUTA, RUTA_HORA)\n",
    "# =====================================================\n",
    "def target_encode_fit(df: pd.DataFrame, cols, target=target_col, smooth=20):\n",
    "    \"\"\"\n",
    "    Calcula TE por columna categ√≥rica usando SOLO TRAIN.\n",
    "    Devuelve:\n",
    "      - mappings: {col: {cat: valor_te}}\n",
    "      - defaults: {col: global_mean}\n",
    "    \"\"\"\n",
    "    mappings, defaults = {}, {}\n",
    "    global_mean = df[target].mean()\n",
    "    for c in cols:\n",
    "        g = df.groupby(c)[target]\n",
    "        stats = g.mean()\n",
    "        cnts  = g.size()\n",
    "        te = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "        mappings[c] = te.to_dict()\n",
    "        defaults[c] = float(global_mean)\n",
    "    return mappings, defaults, float(global_mean)\n",
    "\n",
    "def target_encode_apply(df: pd.DataFrame, mappings, defaults):\n",
    "    out = df.copy()\n",
    "    for c, mp in mappings.items():\n",
    "        col = c + \"_TE\"\n",
    "        out[col] = out[c].astype(str).map(mp).astype(\"float32\")\n",
    "        out[col].fillna(defaults[c], inplace=True)\n",
    "    return out\n",
    "\n",
    "def build_agg(df: pd.DataFrame, keys, target=target_col, pref=\"AIR\", smooth=20):\n",
    "    \"\"\"\n",
    "    Construye agregados suavizados por clave:\n",
    "      - <pref>_rate : tasa suavizada del target\n",
    "      - <pref>_n    : conteo\n",
    "    \"\"\"\n",
    "    g = df.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "    global_mean = df[target].mean()\n",
    "    rate = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "    out = rate.rename(f\"{pref}_rate\").reset_index()\n",
    "    out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "    return out.astype({f\"{pref}_rate\":\"float32\", f\"{pref}_n\":\"float32\"})\n",
    "\n",
    "def apply_aggs(df: pd.DataFrame, agg_tables: dict):\n",
    "    out = df.copy()\n",
    "    for pref, (keys, table) in agg_tables.items():\n",
    "        out = out.merge(table, on=keys, how=\"left\")\n",
    "        out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(0).astype(\"float32\")\n",
    "        out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# --- Fit TE en TRAIN y aplicar a TRAIN/VALID ---\n",
    "mappings, defaults, global_mean = target_encode_fit(v_train, TE_COLS, target_col)\n",
    "X_train_te = target_encode_apply(v_train[TE_COLS + BASE_FEATS], mappings, defaults)\n",
    "X_valid_te = target_encode_apply(v_valid[TE_COLS + BASE_FEATS], mappings, defaults)\n",
    "\n",
    "# --- Construir agregados SOLO con TRAIN y aplicar a TRAIN/VALID ---\n",
    "aggs_specs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\")\n",
    "]\n",
    "agg_tables = {}\n",
    "for keys, pref in aggs_specs:\n",
    "    agg_tables[pref] = (keys, build_agg(v_train, keys, target_col, pref, smooth=20))\n",
    "\n",
    "X_train_full = apply_aggs(X_train_te, agg_tables)\n",
    "X_valid_full = apply_aggs(X_valid_te, agg_tables)\n",
    "\n",
    "print(\"Shapes ->\", X_train_full.shape, X_valid_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676ad4d",
   "metadata": {},
   "source": [
    "6 ‚Äî Orden de features y helpers de m√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d01af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 6) Alinear columnas y helpers de m√©tricas\n",
    "# =========================================\n",
    "FEATURE_ORDER = list(X_train_full.columns)  # orden congelado para inferencia\n",
    "\n",
    "def align_features(df: pd.DataFrame, feature_order):\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    return X[feature_order]\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb2e87",
   "metadata": {},
   "source": [
    "7 ‚Äî Entrenar LGBM, XGB y RF (par√°metros seguros RAM/tiempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe1b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices num√©ricas | X_train: (4299046, 23) | X_valid: (932084, 23)\n",
      "Dtypes ejemplo: {'MONTH': dtype('int8'), 'DAY_OF_WEEK': dtype('int8'), 'SALIDA_SIN': dtype('float32'), 'SALIDA_COS': dtype('float32'), 'MONTH_SIN': dtype('float32'), 'MONTH_COS': dtype('float32'), 'DISTANCIA_HAV': dtype('float32'), 'MINUTO_DIA_SALIDA': dtype('int32')}\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.639319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3706\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.602295\tvalid_0's binary_logloss: 0.583084\n",
      "[400]\tvalid_0's auc: 0.602473\tvalid_0's binary_logloss: 0.580826\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.602496\tvalid_0's binary_logloss: 0.460413\n",
      "‚úì LGBM entrenado en 225.7s | best_iter=1 | AUC valid=0.6025\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4275 | Precision: 0.2009 | Recall: 0.7782 | F1: 0.3194 | ROC-AUC: 0.6025\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[273276 497919]\n",
      " [ 35688 125201]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.2, 'f1': 0.3193866396941872}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6051 | best_iter=17 | tiempo=5.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6048 | best_iter=10 | tiempo=4.3 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6004 | best_iter=143 | tiempo=6.2 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7214 | Precision: 0.2462 | Recall: 0.2977 | F1: 0.2695 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[624549 146646]\n",
      " [112995  47894]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.420) ==\n",
      "Accuracy: 0.4981 | Precision: 0.2106 | Recall: 0.6939 | F1: 0.3231 | ROC-AUC: 0.6051\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[352603 418592]\n",
      " [ 49248 111641]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.42, 'f1': 0.32307175867647103}\n",
      "‚úì RF entrenado en 18.9 min | AUC valid=0.6097\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7220 | Precision: 0.2525 | Recall: 0.3114 | F1: 0.2789 | ROC-AUC: 0.6097\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[622895 148300]\n",
      " [110783  50106]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.370) ==\n",
      "Accuracy: 0.5181 | Precision: 0.2136 | Recall: 0.6681 | F1: 0.3237 | ROC-AUC: 0.6097\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[375465 395730]\n",
      " [ 53397 107492]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.37, 'f1': 0.32371696900066405}\n",
      "\n",
      "=== MEJOR MODELO: RF | AUC=0.6097 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 ‚Äî Entrenar LGBM, XGB y RF (NUM√âRICO)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers m√©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos num√©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a int32\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")  # primero float para evitar NA issues\n",
    "            # si quisieras enteros estrictos: X[c] = X[c].fillna(0).astype(\"int32\")\n",
    "\n",
    "    # convertir objetos/categor√≠as si quedara alguno (no deber√≠a si TE ya se aplic√≥)\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"‚ö†Ô∏è  Columnas no num√©ricas detectadas, se codificar√°n como category‚Üícodes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar las matrices YA TRANSFORMADAS (TE + agregados) ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid, FEATURE_ORDER\n",
    "# Asegura que FEATURE_ORDER sea el de X_train_model:\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "X_train_num  = ensure_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_num  = ensure_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "print(\"‚úì Matrices num√©ricas | X_train:\", X_train_num.shape, \"| X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", X_train_num.dtypes.head(8).to_dict())\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (clasificador)\n",
    "# =========================\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=12000,\n",
    "    num_leaves=127,\n",
    "    min_child_samples=100,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "t0 = time.time()\n",
    "lgbm.fit(\n",
    "    X_train_num, y_train,\n",
    "    eval_set=[(X_valid_num, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    ")\n",
    "lgb_secs = time.time() - t0\n",
    "lgb_best_iter = getattr(lgbm, \"best_iteration_\", None)\n",
    "lgb_proba = lgbm.predict_proba(X_valid_num)[:,1]\n",
    "lgb_auc   = roc_auc_score(y_valid, lgb_proba)\n",
    "print(f\"‚úì LGBM entrenado en {lgb_secs:.1f}s | best_iter={lgb_best_iter} | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "# mejor F1\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b132c9e",
   "metadata": {},
   "source": [
    "para nuevo entrenamiento LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c6e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sanity OK | X_train_model: (4299046, 27) | X_valid_model: (932084, 27)\n",
      "Dtypes (primeras): {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# Celda 6.9 ‚Äî Sanity\n",
    "# ================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _to_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    # bool -> int8\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "    # pandas nullable ints -> float32 (evita NA issues)\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "    # object/category -> codes\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    for c in non_numeric:\n",
    "        X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "    # float homogenizado\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    # int homogenizado\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "# 1) Comprobar que las matrices base existen\n",
    "vars_needed = [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]\n",
    "missing = [v for v in vars_needed if v not in globals()]\n",
    "if missing:\n",
    "    # Si faltan, intenta reconstruir desde X_train_full / X_valid_full (num√©ricas ya alineadas)\n",
    "    alt_needed = [\"X_train_full\",\"X_valid_full\",\"y_train\",\"y_valid\"]\n",
    "    alt_missing = [v for v in alt_needed if v not in globals()]\n",
    "    if alt_missing:\n",
    "        raise RuntimeError(\n",
    "            f\"Faltan variables previas del pipeline: {missing} \"\n",
    "            f\"y tampoco est√°n disponibles alternativas {alt_missing}. \"\n",
    "            f\"Ejecuta las celdas de TE + agregados + split antes de la Celda 7.\"\n",
    "        )\n",
    "    # Reconstruir ‚Äú_model‚Äù a partir de las num√©ricas completas\n",
    "    X_train_model = _to_numeric_frame(X_train_full)\n",
    "    X_valid_model = _to_numeric_frame(X_valid_full)\n",
    "\n",
    "# 2) Asegurar orden de columnas consistente\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "# 3) Forzar num√©rico para esta celda (igual que har√° la celda 7)\n",
    "X_train_model = _to_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_model = _to_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "# 4) Validaciones r√°pidas\n",
    "assert set(X_train_model.columns) == set(X_valid_model.columns), \"Desalineaci√≥n de columnas entre train y valid\"\n",
    "assert len(X_train_model) == len(y_train), \"X_train_model y y_train con longitudes distintas\"\n",
    "assert len(X_valid_model) == len(y_valid), \"X_valid_model y y_valid con longitudes distintas\"\n",
    "\n",
    "print(\"‚úì Sanity OK | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Dtypes (primeras):\", {c: str(X_train_model.dtypes[c]) for c in X_train_model.columns[:8]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1ca42",
   "metadata": {},
   "source": [
    "Similar a la celda anterior pero con cambios, Celda 7 ‚Äî Entrenar LGBM, XGB y RF (par√°metros seguros RAM/tiempo) lista para copiar y pegar. Asume que ya ejecutaste las celdas previas que generan X_train_full, X_valid_full, y_train, y_valid, FEATURE_ORDER, align_features(...) y (opcional) report_metrics(...). La celda incluye salvaguardas y comentarios en cada bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f125fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices num√©ricas | X_train: (4299046, 27) | X_valid: (932084, 27)\n",
      "Dtypes ejemplo: {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592065\tvalid_0's binary_logloss: 0.59038\n",
      "[400]\tvalid_0's auc: 0.591969\tvalid_0's binary_logloss: 0.597104\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.594861\tvalid_0's binary_logloss: 0.46068\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5949 | best_iter=2 | tiempo=1.9 min | params={lr:0.05, leaves:63, ff:0.8, bf:0.8}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592298\tvalid_0's binary_logloss: 0.585581\n",
      "[400]\tvalid_0's auc: 0.592428\tvalid_0's binary_logloss: 0.591143\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.59538\tvalid_0's binary_logloss: 0.460127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5954 | best_iter=2 | tiempo=2.8 min | params={lr:0.03, leaves:127, ff:0.8, bf:0.8}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 2341\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.593665\tvalid_0's binary_logloss: 0.590183\n",
      "[400]\tvalid_0's auc: 0.593732\tvalid_0's binary_logloss: 0.603358\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.595565\tvalid_0's binary_logloss: 0.46019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5956 | best_iter=2 | tiempo=3.0 min | params={lr:0.03, leaves:95, ff:0.7, bf:0.9}\n",
      "‚úì LGBM (mejor de 3) | AUC valid=0.5956\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5956\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2871 | Precision: 0.1860 | Recall: 0.9273 | F1: 0.3099 | ROC-AUC: 0.5956\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[118376 652819]\n",
      " [ 11692 149197]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19, 'f1': 0.30988934526251294}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6062 | best_iter=2 | tiempo=4.5 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6049 | best_iter=2 | tiempo=4.1 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6014 | best_iter=4 | tiempo=4.7 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7045 | Precision: 0.2430 | Recall: 0.3365 | F1: 0.2822 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[602549 168646]\n",
      " [106742  54147]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4385 | Precision: 0.2030 | Recall: 0.7698 | F1: 0.3213 | ROC-AUC: 0.6062\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284898 486297]\n",
      " [ 37031 123858]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.48, 'f1': 0.3212734941196611}\n",
      "‚úì RF entrenado en 19.6 min | AUC valid=0.6024\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7208 | Precision: 0.2515 | Recall: 0.3123 | F1: 0.2786 | ROC-AUC: 0.6024\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[621633 149562]\n",
      " [110642  50247]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.380) ==\n",
      "Accuracy: 0.5442 | Precision: 0.2164 | Recall: 0.6258 | F1: 0.3216 | ROC-AUC: 0.6024\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[406560 364635]\n",
      " [ 60211 100678]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.38, 'f1': 0.32155119274611066}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6062 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 ‚Äî Entrenar LGBM, XGB y RF (NUM√âRICO)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers m√©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime m√©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos num√©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a float32 para evitar NA issues\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # convertir objetos/categor√≠as si quedara alguno\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"‚ö†Ô∏è  Columnas no num√©ricas detectadas, se codificar√°n como category‚Üícodes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar las matrices YA TRANSFORMADAS (TE + agregados) ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid\n",
    "# Asegura que FEATURE_ORDER sea el de X_train_model:\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "X_train_num  = ensure_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_num  = ensure_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "print(\"‚úì Matrices num√©ricas | X_train:\", X_train_num.shape, \"| X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", {c: str(X_train_num.dtypes[c]) for c in list(X_train_num.columns[:8])})\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (clasificador) ‚Äî mini-tuning r√°pido (3 configs)\n",
    "# =========================\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,\n",
    "    min_child_samples=100,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    # tips de eficiencia\n",
    "    max_bin=128,\n",
    "    force_col_wise=True,  # reduce overhead con muchas columnas\n",
    ")\n",
    "\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.05, num_leaves=63,  feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "    dict(learning_rate=0.03, num_leaves=127, feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "    dict(learning_rate=0.03, num_leaves=95,  feature_fraction=0.70, bagging_fraction=0.90, bagging_freq=1),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_train_num, y_train,\n",
    "        eval_set=[(X_valid_num, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(X_valid_num)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1 (LGBM)\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4dcb8",
   "metadata": {},
   "source": [
    "versi√≥n reemplazo de la Celda 7 que aplica exactamente tus sugerencias para cerrar con LightGBM: m√°s capacidad (num_leaves ‚Üë, lr ‚Üì), max_bin=255, min_child_samples m√°s bajo y features de interacci√≥n baratas (relaci√≥n distancia/tiempo, abs(DEPARTURE_DELAY) y buckets de 30‚Äô).\n",
    "La celda es autosuficiente: si alguna columna no existe, la crea; si ya existe, la respeta. XGB y RF se mantienen para comparar, pero si quieres solo LGBM, te indico qu√© l√≠neas comentar al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015ca4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices num√©ricas | X_train: (4299046, 28)  | X_valid: (932084, 28)\n",
      "Dtypes ejemplo: {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.592299\tvalid_0's binary_logloss: 0.583984\n",
      "[400]\tvalid_0's auc: 0.592228\tvalid_0's binary_logloss: 0.595969\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.591258\tvalid_0's binary_logloss: 0.459871\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5913 | best_iter=4 | tiempo=3.2 min | params={lr:0.02, leaves:255, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.594793\tvalid_0's binary_logloss: 0.591906\n",
      "[400]\tvalid_0's auc: 0.594567\tvalid_0's binary_logloss: 0.605517\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.5917\tvalid_0's binary_logloss: 0.459739\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5917 | best_iter=4 | tiempo=4.5 min | params={lr:0.02, leaves:191, mcs:60, ff:0.8, bf:0.9}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4262\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 28\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.594958\tvalid_0's binary_logloss: 0.59879\n",
      "[400]\tvalid_0's auc: 0.595052\tvalid_0's binary_logloss: 0.606772\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's auc: 0.593671\tvalid_0's binary_logloss: 0.45971\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5937 | best_iter=3 | tiempo=3.4 min | params={lr:0.025, leaves:223, mcs:80, ff:0.8, bf:0.8}\n",
      "‚úì LGBM (mejor de 3) | AUC valid=0.5937\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5937\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5012 | Precision: 0.2075 | Recall: 0.6701 | F1: 0.3169 | ROC-AUC: 0.5937\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[359365 411830]\n",
      " [ 53073 107816]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19999999999999996, 'f1': 0.3168565907704967}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.5960 | best_iter=8 | tiempo=6.1 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.5947 | best_iter=8 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.5921 | best_iter=1 | tiempo=5.5 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7183 | Precision: 0.2435 | Recall: 0.3000 | F1: 0.2688 | ROC-AUC: 0.5960\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[621275 149920]\n",
      " [112621  48268]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.440) ==\n",
      "Accuracy: 0.4837 | Precision: 0.2063 | Recall: 0.6994 | F1: 0.3186 | ROC-AUC: 0.5960\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[338320 432875]\n",
      " [ 48367 112522]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.44, 'f1': 0.3186301300039927}\n",
      "‚úì RF entrenado en 19.4 min | AUC valid=0.6019\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7197 | Precision: 0.2505 | Recall: 0.3131 | F1: 0.2783 | ROC-AUC: 0.6019\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[620448 150747]\n",
      " [110519  50370]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.380) ==\n",
      "Accuracy: 0.5425 | Precision: 0.2160 | Recall: 0.6275 | F1: 0.3213 | ROC-AUC: 0.6019\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404669 366526]\n",
      " [ 59926 100963]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.37999999999999995, 'f1': 0.3213447956484791}\n",
      "\n",
      "=== MEJOR MODELO: RF | AUC=0.6019 ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Celda 7 ‚Äî Entrenar LGBM, XGB y RF (NUM√âRICO, con interacciones)\n",
    "# ============================================\n",
    "import time, numpy as np, json, os, joblib\n",
    "import lightgbm as lgb\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- 0) Helpers m√©tricas y casting seguro ---\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime m√©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def ensure_numeric_frame(df):\n",
    "    \"\"\"Convierte todo a tipos num√©ricos compatibles con LightGBM/XGBoost/Sklearn.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # convertir booleanos\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "\n",
    "    # convertir enteros 'nullable' a float32 para evitar NA issues\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):   # ej. 'Int64'\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # convertir objetos/categor√≠as si quedara alguno\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(\"‚ö†Ô∏è  Columnas no num√©ricas detectadas, se codificar√°n como category‚Üícodes:\", non_numeric)\n",
    "        for c in non_numeric:\n",
    "            X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "\n",
    "    return X\n",
    "\n",
    "# --- 1) Usar matrices YA transformadas (TE + agregados) y a√±adir interacciones baratas ---\n",
    "# Deben existir de celdas previas: X_train_model, X_valid_model, y_train, y_valid\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "\n",
    "def add_interactions(X: np.ndarray.__class__, base_cols: list[str]) -> tuple:\n",
    "    \"\"\"\n",
    "    A√±ade, si es posible y no existen a√∫n:\n",
    "      - DIST_T_P: DISTANCIA_HAV / (SCHEDULED_TIME + 1e-3)\n",
    "      - ABS_DEP_DELAY: abs(DEPARTURE_DELAY)  (si est√° disponible)\n",
    "      - BUCKET_30_SALIDA: floor(MINUTO_DIA_SALIDA/30)*30\n",
    "      - BUCKET_30_LLEGADA: floor(MINUTO_DIA_LLEGADA/30)*30\n",
    "    Retorna (X_modificado, nuevas_columnas_agregadas_en_orden).\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    new_cols = []\n",
    "\n",
    "    def maybe_add(col_name, series, dtype=None):\n",
    "        nonlocal X, new_cols\n",
    "        if col_name not in X.columns:\n",
    "            if dtype is not None:\n",
    "                X[col_name] = series.astype(dtype)\n",
    "            else:\n",
    "                X[col_name] = series\n",
    "            new_cols.append(col_name)\n",
    "\n",
    "    # 1) DIST_T_P = DISTANCIA_HAV / SCHEDULED_TIME\n",
    "    if \"DISTANCIA_HAV\" in X.columns and \"SCHEDULED_TIME\" in X.columns:\n",
    "        eps = 1e-3\n",
    "        maybe_add(\"DIST_T_P\", (X[\"DISTANCIA_HAV\"].astype(\"float32\") / (X[\"SCHEDULED_TIME\"].astype(\"float32\") + eps)), \"float32\")\n",
    "\n",
    "    # 2) ABS_DEP_DELAY = abs(DEPARTURE_DELAY)\n",
    "    if \"DEPARTURE_DELAY\" in X.columns:\n",
    "        maybe_add(\"ABS_DEP_DELAY\", X[\"DEPARTURE_DELAY\"].astype(\"float32\").abs(), \"float32\")\n",
    "\n",
    "    # 3) Buckets de 30' (enteros)\n",
    "    if \"MINUTO_DIA_SALIDA\" in X.columns:\n",
    "        maybe_add(\"BUCKET_30_SALIDA\", ((X[\"MINUTO_DIA_SALIDA\"].astype(\"int32\") // 30) * 30), \"int32\")\n",
    "    if \"MINUTO_DIA_LLEGADA\" in X.columns:\n",
    "        maybe_add(\"BUCKET_30_LLEGADA\", ((X[\"MINUTO_DIA_LLEGADA\"].astype(\"int32\") // 30) * 30), \"int32\")\n",
    "\n",
    "    # Casting final homog√©neo\n",
    "    X = ensure_numeric_frame(X)\n",
    "    return X, new_cols\n",
    "\n",
    "# Aplicar interacciones a train y valid (sim√©trico)\n",
    "X_train_ext, added_cols_train = add_interactions(X_train_model, FEATURE_ORDER)\n",
    "X_valid_ext, _               = add_interactions(X_valid_model, FEATURE_ORDER)\n",
    "\n",
    "# Actualizar FEATURE_ORDER preservando orden previo + nuevas columnas en cola\n",
    "FEATURE_ORDER = list(X_train_model.columns) + [c for c in added_cols_train if c not in X_train_model.columns]\n",
    "\n",
    "# Ensamblar matrices num√©ricas finales en el mismo orden de features\n",
    "X_train_num = ensure_numeric_frame(X_train_ext[FEATURE_ORDER])\n",
    "X_valid_num = ensure_numeric_frame(X_valid_ext[FEATURE_ORDER])\n",
    "\n",
    "print(\"‚úì Matrices num√©ricas | X_train:\", X_train_num.shape, \" | X_valid:\", X_valid_num.shape)\n",
    "print(\"Dtypes ejemplo:\", {c: str(X_train_num.dtypes[c]) for c in list(X_train_num.columns[:8])})\n",
    "\n",
    "# --- 2) Peso por desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM ‚Äî enfoque principal (m√°s capacidad + bins finos + min_child_samples bajo)\n",
    "# =========================\n",
    "# Sugerencias aplicadas:\n",
    "#   - num_leaves ‚Üë (hasta 255)\n",
    "#   - learning_rate ‚Üì (0.02)\n",
    "#   - max_bin = 255 (m√°s granularidad en histogramas)\n",
    "#   - min_child_samples = 60‚Äì80\n",
    "#   - force_col_wise para mejor rendimiento con muchas columnas\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,         # con early_stopping, se corta antes\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# probamos 3 configuraciones centradas en la consigna\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.02, num_leaves=255, min_child_samples=70, feature_fraction=0.85, bagging_fraction=0.85, bagging_freq=1),\n",
    "    dict(learning_rate=0.02, num_leaves=191, min_child_samples=60, feature_fraction=0.80, bagging_fraction=0.90, bagging_freq=1),\n",
    "    dict(learning_rate=0.025, num_leaves=223, min_child_samples=80, feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        X_train_num, y_train,\n",
    "        eval_set=[(X_valid_num, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(X_valid_num)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p['min_child_samples']}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral para F1 (LGBM)\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (clasificador) ‚Äî (se mantiene para comparar)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(X_train_num, label=y_train)\n",
    "    dvalid = DMatrix(X_valid_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (opcional/seguro) ‚Äî (se mantiene para comparar)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,            # limita RAM/tiempo; ajusta si puedes\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(X_train_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(X_valid_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor (para reporte)\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f891",
   "metadata": {},
   "source": [
    "### Revisi√≥n 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810dc0f8",
   "metadata": {},
   "source": [
    "1 ‚Äî Imports, rutas y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a78806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1) Imports, rutas y utilidades\n",
    "# ================================\n",
    "import os, time, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier  # (no lo usamos directo, pero mantenemos compatibilidad)\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Rutas del proyecto ----------\n",
    "PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
    "ARTIF_DIR    = PROJECT_ROOT / \"artifacts\"\n",
    "for d in (MODELS_DIR, ARTIF_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cambia aqu√≠ si tu CSV est√° en otra ruta:\n",
    "CSV_PATH = r\"D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "# ---------- Configuraci√≥n general ----------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Columnas esperadas del CSV (seg√∫n tu lista)\n",
    "COLUMNS_EXPECTED = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\"DEPARTURE_TIME\",\"DEPARTURE_DELAY\",\n",
    "    \"SCHEDULED_TIME\",\"DISTANCE\",\"SCHEDULED_ARRIVAL\",\"ARRIVAL_TIME\",\"ARRIVAL_DELAY\",\n",
    "    \"AIRLINE_NAME\",\"ORIGEN_AEROPUERTO\",\"ORIGEN_CIUDAD\",\"ORIGEN_ESTADO\",\"ORIGEN_LAT\",\"ORIGEN_LON\",\n",
    "    \"DEST_AEROPUERTO\",\"DEST_CIUDAD\",\"DEST_ESTADO\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"MOTIVO_RETRASO\",\"CANTIDAD_CAUSAS\",\"RETRASADO_LLEGADA\",\"RETRASADO_SALIDA\",\n",
    "    \"HORA_SALIDA\",\"HORA_LLEGADA\",\"MIN_SALIDA\",\"MIN_LLEGADA\",\"MINUTO_DIA_SALIDA\",\"MINUTO_DIA_LLEGADA\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\"LLEGADA_SIN\",\"LLEGADA_COS\",\"PERIODO_SALIDA\",\"PERIODO_LLEGADA\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987d351",
   "metadata": {},
   "source": [
    "2 ‚Äî Cargar CSV y optimizar tipos (no recalcula si ya existen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2311a2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cargado: (5231130, 41) | en 40.2s\n",
      "‚úì Columnas clave presentes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2) Cargar flights_clean.csv y optimizar tipos num√©ricos\n",
    "#     - NO recalcula columnas si ya existen en el CSV\n",
    "# =====================================================\n",
    "def load_flights(path_csv: str) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    df = pd.read_csv(path_csv)\n",
    "    # Downcast num√©ricos cuando sea seguro\n",
    "    for c in df.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "    print(f\"‚úì Cargado: {df.shape} | en {time.time()-t0:.1f}s\")\n",
    "    return df\n",
    "\n",
    "df = load_flights(CSV_PATH)\n",
    "\n",
    "# Validaci√≥n soft de columnas clave\n",
    "faltantes = [c for c in [\"MONTH\",\"DAY_OF_WEEK\",\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "                         \"MINUTO_DIA_SALIDA\",\"SALIDA_SIN\",\"SALIDA_COS\",\"RETRASADO_LLEGADA\"] if c not in df.columns]\n",
    "if faltantes:\n",
    "    print(\"‚ö†Ô∏è Faltan columnas clave (se derivar√°n si es posible):\", faltantes)\n",
    "else:\n",
    "    print(\"‚úì Columnas clave presentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fedcff",
   "metadata": {},
   "source": [
    "3 ‚Äî Features base (RUTA, trigonom√©tricas, Haversine) solo si faltan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3a368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231130, 45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 3) Features base: RUTA, se√±ales trigonom√©tricas y\n",
    "#    distancia Haversine (si faltara DISTANCE_HAV)\n",
    "# ==================================================\n",
    "def add_route(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"RUTA\" not in out.columns:\n",
    "        out[\"RUTA\"] = (out[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + out[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    return out\n",
    "\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Si ya existen, no recalcular\n",
    "    if \"MINUTO_DIA_SALIDA\" not in out.columns and \"SCHEDULED_DEPARTURE\" in out.columns:\n",
    "        hh = (out[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23)\n",
    "        mm = (out[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59)\n",
    "        out[\"MINUTO_DIA_SALIDA\"] = (hh * 60 + mm).astype(\"int16\")\n",
    "\n",
    "    if \"SALIDA_SIN\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_SIN\"] = np.sin(rad).astype(\"float32\")\n",
    "\n",
    "    if \"SALIDA_COS\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        rad = 2*np.pi*(out[\"MINUTO_DIA_SALIDA\"].astype(float)/(24*60))\n",
    "        out[\"SALIDA_COS\"] = np.cos(rad).astype(\"float32\")\n",
    "\n",
    "    if \"HORA_SALIDA\" not in out.columns and \"MINUTO_DIA_SALIDA\" in out.columns:\n",
    "        out[\"HORA_SALIDA\"] = (out[\"MINUTO_DIA_SALIDA\"] // 60).astype(\"int8\")\n",
    "\n",
    "    # Se√±ales mensuales (baratas)\n",
    "    if \"MONTH_SIN\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_SIN\"] = np.sin(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "    if \"MONTH_COS\" not in out.columns and \"MONTH\" in out.columns:\n",
    "        out[\"MONTH_COS\"] = np.cos(2*np.pi*(out[\"MONTH\"].astype(float)/12)).astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# Distancia Haversine (si la quieres y no la tienes en tu CSV)\n",
    "def ensure_distance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"DISTANCIA_HAV\" in out.columns:\n",
    "        return out\n",
    "    if not {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(out.columns):\n",
    "        return out\n",
    "\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        R = 6371.0\n",
    "        p1 = np.radians(lat1); p2 = np.radians(lat2)\n",
    "        dlat = p2 - p1\n",
    "        dlon = np.radians(lon2) - np.radians(lon1)\n",
    "        a = np.sin(dlat/2)**2 + np.cos(p1)*np.cos(p2)*np.sin(dlon/2)**2\n",
    "        return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "    out[\"DISTANCIA_HAV\"] = haversine(out[\"ORIGEN_LAT\"], out[\"ORIGEN_LON\"], out[\"DEST_LAT\"], out[\"DEST_LON\"]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "df = add_route(df)\n",
    "df = ensure_time_features(df)\n",
    "df = ensure_distance(df)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a19329",
   "metadata": {},
   "source": [
    "4 ‚Äî Split temporal + matrices base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e3b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> (4299046, 45) (932084, 45) | rate train 0.18733737671101913 | rate valid 0.17261212508743848\n",
      "BASE_FEATS: ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'HORA_SALIDA']\n",
      "TE_COLS:    ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Split temporal sin fuga (train 1-9, valid 10-12)\n",
    "#    y definici√≥n de matrices base para modelado\n",
    "# ============================================================\n",
    "target_col = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "# M√°scaras de tiempo (2015)\n",
    "train_mask = (df[\"MONTH\"] >= 1) & (df[\"MONTH\"] <= 9)\n",
    "valid_mask = (df[\"MONTH\"] >= 10) & (df[\"MONTH\"] <= 12)\n",
    "\n",
    "v_train = df.loc[train_mask].copy()\n",
    "v_valid = df.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Shapes ->\", v_train.shape, v_valid.shape,\n",
    "      \"| rate train\", v_train[target_col].mean(),\n",
    "      \"| rate valid\", v_valid[target_col].mean())\n",
    "\n",
    "# Conjunto m√≠nimo de features base (num√©ricas + derivadas)\n",
    "BASE_FEATS = [\n",
    "    \"MONTH\",\"DAY_OF_WEEK\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"MONTH_SIN\",\"MONTH_COS\",\n",
    "    \"DISTANCIA_HAV\" if \"DISTANCIA_HAV\" in df.columns else \"DISTANCE\",\n",
    "    \"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"\n",
    "]\n",
    "\n",
    "# Columnas categ√≥ricas para Target Encoding\n",
    "TE_COLS = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in df.columns]\n",
    "\n",
    "# Construimos X/y de partida\n",
    "X_train = v_train[BASE_FEATS].copy()\n",
    "X_valid = v_valid[BASE_FEATS].copy()\n",
    "y_train = v_train[target_col].astype(\"int8\")\n",
    "y_valid = v_valid[target_col].astype(\"int8\")\n",
    "\n",
    "print(\"BASE_FEATS:\", BASE_FEATS)\n",
    "print(\"TE_COLS:   \", TE_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a7146",
   "metadata": {},
   "source": [
    "5 ‚Äî Target Encoding + agregados (sin fuga) + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af61a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TE guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\onehot_encoder.joblib\n",
      "Shapes -> (4299046, 27) (932084, 27)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 5) Target Encoding y Agregados (sin fuga)\n",
    "#     - TE: medias suavizadas por categor√≠a\n",
    "#     - Agregs: tasas y conteos por llaves (AIR, DES, ORI, RUTA, RUTA_HORA)\n",
    "# =====================================================\n",
    "import joblib\n",
    "\n",
    "def target_encode_fit(df_train: pd.DataFrame, cols, target=target_col, smooth=200):\n",
    "    mappings, defaults = {}, {}\n",
    "    global_mean = df_train[target].mean()\n",
    "    for c in cols:\n",
    "        g = df_train.groupby(c)[target]\n",
    "        stats = g.mean()\n",
    "        cnts  = g.size()\n",
    "        te = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "        mappings[c] = te.to_dict()\n",
    "        defaults[c] = float(global_mean)\n",
    "    return mappings, defaults, float(global_mean)\n",
    "\n",
    "def target_encode_apply(df_part: pd.DataFrame, cols, mappings, defaults):\n",
    "    out = df_part.copy()\n",
    "    for c in cols:\n",
    "        col_te = f\"{c}_TE\"\n",
    "        out[col_te] = out[c].astype(str).map(mappings[c]).fillna(defaults[c]).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "def build_agg(df_train: pd.DataFrame, keys, target=target_col, pref=\"AIR\", smooth=20):\n",
    "    g = df_train.groupby(keys)[target]\n",
    "    stats = g.mean()\n",
    "    cnts  = g.size()\n",
    "    global_mean = df_train[target].mean()\n",
    "    rate = (stats*cnts + global_mean*smooth) / (cnts + smooth)\n",
    "    out = rate.rename(f\"{pref}_rate\").reset_index()\n",
    "    out = out.merge(cnts.rename(f\"{pref}_n\").reset_index(), on=keys, how=\"left\")\n",
    "    return out.astype({f\"{pref}_rate\":\"float32\", f\"{pref}_n\":\"float32\"})\n",
    "\n",
    "def apply_aggs(df_part: pd.DataFrame, agg_tables: dict):\n",
    "    out = df_part.copy()\n",
    "    for pref, (keys, table) in agg_tables.items():\n",
    "        out = out.merge(table, on=keys, how=\"left\")\n",
    "        out[f\"{pref}_rate\"] = out[f\"{pref}_rate\"].fillna(0).astype(\"float32\")\n",
    "        out[f\"{pref}_n\"]    = out[f\"{pref}_n\"].fillna(0).astype(\"float32\")\n",
    "    return out\n",
    "\n",
    "# --- FIT TE solo con TRAIN ---\n",
    "mappings, defaults, global_mean = target_encode_fit(v_train, TE_COLS, target_col, smooth=200)\n",
    "\n",
    "# --- Aplicar TE a TRAIN/VALID (agregando columnas *_TE) ---\n",
    "X_train_te_base = target_encode_apply(v_train[TE_COLS + BASE_FEATS], TE_COLS, mappings, defaults)\n",
    "X_valid_te_base = target_encode_apply(v_valid[TE_COLS + BASE_FEATS], TE_COLS, mappings, defaults)\n",
    "\n",
    "# --- Agregados SOLO con TRAIN y aplicar a ambos ---\n",
    "aggs_specs = [\n",
    "    ([\"AIRLINE\"], \"AIR\"),\n",
    "    ([\"DESTINATION_AIRPORT\"], \"DES\"),\n",
    "    ([\"ORIGIN_AIRPORT\"], \"ORI\"),\n",
    "    ([\"RUTA\"], \"RUTA\"),\n",
    "    ([\"RUTA\",\"HORA_SALIDA\"], \"RUTA_HORA\")\n",
    "]\n",
    "agg_tables = {}\n",
    "for keys, pref in aggs_specs:\n",
    "    agg_tables[pref] = (keys, build_agg(v_train, keys, target_col, pref, smooth=20))\n",
    "\n",
    "X_train_full = apply_aggs(X_train_te_base, agg_tables)\n",
    "X_valid_full = apply_aggs(X_valid_te_base, agg_tables)\n",
    "\n",
    "# Orden de features congelado\n",
    "FEATURE_ORDER = list(X_train_full.columns)\n",
    "\n",
    "# Guardar el \"encoder\" (payload TE + orden features) con el nombre que ya usabas\n",
    "ENCODER_PATH = MODELS_DIR / \"onehot_encoder.joblib\"\n",
    "joblib.dump(\n",
    "    {\"type\":\"target_encoding\",\"mappings\":mappings,\"defaults\":defaults,\n",
    "     \"global_mean\":global_mean,\"cols\":TE_COLS,\"feature_order\":FEATURE_ORDER},\n",
    "    ENCODER_PATH\n",
    ")\n",
    "print(f\"‚úì TE guardado en: {ENCODER_PATH}\")\n",
    "\n",
    "# Guardar tambi√©n feature_order.json (opcional para inspecci√≥n)\n",
    "with open(ARTIF_DIR / \"feature_order.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_order\": FEATURE_ORDER}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Shapes ->\", X_train_full.shape, X_valid_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faad45c",
   "metadata": {},
   "source": [
    "6 ‚Äî Sanity (construir X_train_model si faltara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d81610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sanity OK | X_train_model: (4299046, 27) | X_valid_model: (932084, 27)\n",
      "Dtypes (primeras): {'AIRLINE': 'int32', 'ORIGIN_AIRPORT': 'int32', 'DESTINATION_AIRPORT': 'int32', 'RUTA': 'int32', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6) Alinear columnas y helpers de m√©tricas\n",
    "# =========================================\n",
    "def align_features(df: pd.DataFrame, feature_order):\n",
    "    X = df.copy()\n",
    "    for c in feature_order:\n",
    "        if c not in X.columns:\n",
    "            X[c] = 0\n",
    "    return X[feature_order]\n",
    "\n",
    "def _to_numeric_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "    # bool -> int8\n",
    "    for c in X.select_dtypes(include=[\"bool\"]).columns:\n",
    "        X[c] = X[c].astype(\"int8\")\n",
    "    # pandas nullable ints -> float32 (evita NA issues en LightGBM)\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype).startswith(\"Int\"):\n",
    "            X[c] = X[c].astype(\"float32\")\n",
    "    # object/category -> codes (por si queda algo no num√©rico)\n",
    "    non_numeric = X.select_dtypes(exclude=[\"number\", \"bool\"]).columns.tolist()\n",
    "    for c in non_numeric:\n",
    "        X[c] = X[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
    "    # homogenizar floats\n",
    "    for c in X.select_dtypes(include=[\"float64\",\"float32\"]).columns:\n",
    "        X[c] = X[c].astype(\"float32\")\n",
    "    # homogenizar ints\n",
    "    for c in X.select_dtypes(include=[\"int64\",\"int32\"]).columns:\n",
    "        X[c] = X[c].astype(\"int32\")\n",
    "    return X\n",
    "\n",
    "# Si no existen matrices _model, se construyen desde X_train_full / X_valid_full\n",
    "vars_needed = [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]\n",
    "missing = [v for v in vars_needed if v not in globals()]\n",
    "if missing:\n",
    "    alt_needed = [\"X_train_full\",\"X_valid_full\",\"y_train\",\"y_valid\"]\n",
    "    alt_missing = [v for v in alt_needed if v not in globals()]\n",
    "    if alt_missing:\n",
    "        raise RuntimeError(\n",
    "            f\"Faltan variables: {missing} y tampoco est√°n {alt_missing}. \"\n",
    "            f\"Ejecuta las celdas 4 y 5 antes de esta.\"\n",
    "        )\n",
    "    X_train_model = _to_numeric_frame(align_features(X_train_full, FEATURE_ORDER))\n",
    "    X_valid_model = _to_numeric_frame(align_features(X_valid_full, FEATURE_ORDER))\n",
    "\n",
    "# Asegurar orden y tipos\n",
    "FEATURE_ORDER = list(X_train_model.columns)\n",
    "X_train_model = _to_numeric_frame(X_train_model[FEATURE_ORDER])\n",
    "X_valid_model = _to_numeric_frame(X_valid_model[FEATURE_ORDER])\n",
    "\n",
    "# Chequeos\n",
    "assert set(X_train_model.columns) == set(X_valid_model.columns), \"Desalineaci√≥n de columnas entre train y valid\"\n",
    "assert len(X_train_model) == len(y_train), \"X_train_model y y_train con longitudes distintas\"\n",
    "assert len(X_valid_model) == len(y_valid), \"X_valid_model y y_valid con longitudes distintas\"\n",
    "\n",
    "print(\"‚úì Sanity OK | X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Dtypes (primeras):\", {c: str(X_train_model.dtypes[c]) for c in X_train_model.columns[:8]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a01c6",
   "metadata": {},
   "source": [
    "6.1 ‚Äî (Opcional) Matrices con categ√≥ricas nativas para LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb19303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LGBM categ√≥ricas nativas listas: (4299046, 13) (932084, 13)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6.1) (Opcional) Categ√≥ricas nativas para LGBM (sin TE)\n",
    "#      ‚Äî Solo para LGBM; XGB/RF usar√°n las matrices num√©ricas TE\n",
    "# ============================================================\n",
    "# Reconstruimos la base sin TE (solo columnas originales de BASE_FEATS + TE_COLS)\n",
    "X_train_base_for_lgb = v_train[TE_COLS + BASE_FEATS].copy()\n",
    "X_valid_base_for_lgb = v_valid[TE_COLS + BASE_FEATS].copy()\n",
    "for c in TE_COLS:\n",
    "    X_train_base_for_lgb[c] = X_train_base_for_lgb[c].astype(\"category\")\n",
    "    X_valid_base_for_lgb[c] = X_valid_base_for_lgb[c].astype(\"category\")\n",
    "\n",
    "LGBM_CATEGORICAL_FEATURES = TE_COLS[:]  # se pasa esta lista a LightGBM\n",
    "print(\"‚úì LGBM categ√≥ricas nativas listas:\", X_train_base_for_lgb.shape, X_valid_base_for_lgb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693105a4",
   "metadata": {},
   "source": [
    "6.6 enriquecer matrices LGBM con agregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e90fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LGBM FULL shapes: (4299046, 27) (932084, 27)\n",
      "‚úì Ejemplo dtypes (primeras): {'AIRLINE': 'category', 'ORIGIN_AIRPORT': 'category', 'DESTINATION_AIRPORT': 'category', 'RUTA': 'category', 'MONTH': 'int8', 'DAY_OF_WEEK': 'int8', 'SALIDA_SIN': 'float32', 'SALIDA_COS': 'float32', 'MONTH_SIN': 'float32', 'MONTH_COS': 'float32'}\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 6.6 ‚Äî Enriquecer LGBM con AGGs/TE num√©ricos\n",
    "#  - Mantener categ√≥ricas nativas s√≥lo en: AIRLINE, ORIGIN_AIRPORT, DESTINATION_AIRPORT, RUTA\n",
    "#  - A√±adir columnas num√©ricas de X_train_full/X_valid_full (rates, counts, TE)\n",
    "# =============================================\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Partimos de las bases para LGBM (ya tipadas como 'category')\n",
    "X_train_lgb_full = X_train_base_for_lgb.copy()\n",
    "X_valid_lgb_full = X_valid_base_for_lgb.copy()\n",
    "\n",
    "# 2) Identificamos columnas num√©ricas interesantes en los datasets \"full\" (TE + agregados)\n",
    "#    Regla: tomamos todas las columnas de X_train_full que NO est√©n en la base categ√≥rica\n",
    "#    ni sean las 4 categ√≥ricas nativas (para no romper dtypes)\n",
    "base_cols_cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "base_cols_all  = list(X_train_base_for_lgb.columns)  # incluye tambi√©n num√©ricas base (MONTH, DAY_OF_WEEK, etc.)\n",
    "\n",
    "cand_extra = [c for c in X_train_full.columns\n",
    "              if c not in base_cols_cats and c not in base_cols_all]\n",
    "\n",
    "# 3) Filtramos solo num√©ricas reales (rates, counts, TE), evitando objetos\n",
    "num_extra = [c for c in cand_extra if str(X_train_full[c].dtype) not in (\"object\",)]\n",
    "\n",
    "# 4) (Opcional) Si quieres ser a√∫n m√°s expl√≠cito: quedarte s√≥lo con *_rate, *_n y *_TE\n",
    "# num_extra = [c for c in num_extra if c.endswith((\"_rate\",\"_n\",\"_TE\"))]\n",
    "\n",
    "# 5) Anexamos esas columnas num√©ricas a las matrices LGBM\n",
    "X_train_lgb_full = pd.concat([X_train_lgb_full.reset_index(drop=True),\n",
    "                              X_train_full[num_extra].reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_valid_lgb_full = pd.concat([X_valid_lgb_full.reset_index(drop=True),\n",
    "                              X_valid_full[num_extra].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 6) Sanidad: asegurarnos de que las 4 categ√≥ricas siguen siendo 'category'\n",
    "for c in base_cols_cats:\n",
    "    if c in X_train_lgb_full.columns:\n",
    "        X_train_lgb_full[c] = X_train_lgb_full[c].astype('category')\n",
    "    if c in X_valid_lgb_full.columns:\n",
    "        X_valid_lgb_full[c] = X_valid_lgb_full[c].astype('category')\n",
    "\n",
    "# 7) Redefinimos qu√© usar√° LGBM:\n",
    "X_train_base_for_lgb = X_train_lgb_full\n",
    "X_valid_base_for_lgb = X_valid_lgb_full\n",
    "\n",
    "# 8) Lista de categ√≥ricas nativas se mantiene igual:\n",
    "LGBM_CATEGORICAL_FEATURES = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "\n",
    "print(\"‚úì LGBM FULL shapes:\", X_train_base_for_lgb.shape, X_valid_base_for_lgb.shape)\n",
    "print(\"‚úì Ejemplo dtypes (primeras):\", {c: str(X_train_base_for_lgb.dtypes[c]) for c in list(X_train_base_for_lgb.columns[:10])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e560d",
   "metadata": {},
   "source": [
    "Entrenar LGBM, XGB y RF (Numerico/Categ√≥rico) + guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecef9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices | LGBM: (4299046, 13)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.610325\tvalid_0's binary_logloss: 0.546958\n",
      "[400]\tvalid_0's auc: 0.607027\tvalid_0's binary_logloss: 0.546569\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580286\tvalid_0's binary_logloss: 0.460432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5803 | best_iter=2 | tiempo=5.7 min | params={lr:0.02, leaves:255, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.486863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.611638\tvalid_0's binary_logloss: 0.548258\n",
      "[400]\tvalid_0's auc: 0.608812\tvalid_0's binary_logloss: 0.547344\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580037\tvalid_0's binary_logloss: 0.460504\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5800 | best_iter=2 | tiempo=4.6 min | params={lr:0.02, leaves:191, mcs:60, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\tvalid_0's auc: 0.609563\tvalid_0's binary_logloss: 0.547503\n",
      "[400]\tvalid_0's auc: 0.606852\tvalid_0's binary_logloss: 0.546267\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.576525\tvalid_0's binary_logloss: 0.460466\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5765 | best_iter=1 | tiempo=4.8 min | params={lr:0.025, leaves:223, mcs:80, ff:0.85, bf:0.85}\n",
      "‚úì LGBM (mejor de 3) | AUC valid=0.5803\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2951 | Precision: 0.1868 | Recall: 0.9200 | F1: 0.3106 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[127033 644162]\n",
      " [ 12871 148018]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19, 'f1': 0.31061339735108373}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=5.3 min\n",
      "\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6047 | best_iter=2 | tiempo=4.6 min\n",
      "\n",
      "Probando XGB: {'eta': 0.03, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "AUC valid=0.6012 | best_iter=4 | tiempo=4.8 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "‚úì RF entrenado en 21.1 min | AUC valid=0.6027\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7145 | Precision: 0.2498 | Recall: 0.3265 | F1: 0.2831 | ROC-AUC: 0.6027\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[613436 157759]\n",
      " [108353  52536]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.370) ==\n",
      "Accuracy: 0.5233 | Precision: 0.2133 | Recall: 0.6554 | F1: 0.3219 | ROC-AUC: 0.6027\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[382309 388886]\n",
      " [ 55446 105443]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.37, 'f1': 0.3218562371607618}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "‚úÖ Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "‚úÖ Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (NUM√âRICO/CAT) + guardado\n",
    "# ============================================\n",
    "import joblib\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime m√©tricas a un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === Selecci√≥n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # True: LGBM con categ√≥ricas nativas; False: LGBM con TE num√©rico\n",
    "\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"  # ya es num√©rico (TE)\n",
    "\n",
    "# XGB/RF con TE num√©ricas\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"‚úì Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM ‚Äî mini-tuning (3 configs mejoradas)\n",
    "# =========================\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=12000,\n",
    "    min_child_samples=70,      # 60‚Äì80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,               # ‚Üë resoluci√≥n histogramas\n",
    "    feature_fraction=0.85,     # NO usar colsample_bytree simult√°neo\n",
    "    bagging_fraction=0.85,\n",
    "    bagging_freq=1,\n",
    ")\n",
    "\n",
    "lgb_try = [\n",
    "    dict(learning_rate=0.02, num_leaves=255),\n",
    "    dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "    dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=400), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor de 3) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# Barrido de umbral\n",
    "best_lgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.6, 56):\n",
    "    f1 = f1_score(y_valid, (lgb_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_lgb_thr[\"f1\"]:\n",
    "        best_lgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE num√©ricas)\n",
    "# =========================\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=12000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=400\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "grid = [\n",
    "    xgb_base,\n",
    "    {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "    {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "]\n",
    "best_xgb = {\"auc\":-1}\n",
    "for p in grid:\n",
    "    print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "    mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "    print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "    if auc > best_xgb[\"auc\"]:\n",
    "        best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "best_xgb_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.8, 76):\n",
    "    f1 = f1_score(y_valid, (best_xgb[\"proba\"]>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_xgb_thr[\"f1\"]:\n",
    "        best_xgb_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE num√©ricas)\n",
    "# ==================================\n",
    "rf_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=18,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "best_rf_thr = {\"thr\":0.5,\"f1\":-1}\n",
    "for thr in np.linspace(0.05, 0.5, 46):\n",
    "    f1 = f1_score(y_valid, (rf_proba>=thr).astype(int), zero_division=0)\n",
    "    if f1 > best_rf_thr[\"f1\"]:\n",
    "        best_rf_thr = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "_ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "_ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, adem√°s guardamos con tu nombre hist√≥rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"‚úÖ Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos m√≠nimos (opcional)\n",
    "meta = {\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee49e7",
   "metadata": {},
   "source": [
    "otro 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices | LGBM: (4299046, 13)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580037\tvalid_0's binary_logloss: 0.460504\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5800 | best_iter=2 | tiempo=1.0 min | params={lr:0.02, leaves:191, mcs:70, ff:0.85, bf:0.85}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.580286\tvalid_0's binary_logloss: 0.460432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.85, subsample=1.0 will be ignored. Current value: bagging_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5803 | best_iter=2 | tiempo=1.0 min | params={lr:0.02, leaves:255, mcs:60, ff:0.85, bf:0.85}\n",
      "‚úì LGBM (mejor) | AUC valid=0.5803\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.190) ==\n",
      "Accuracy: 0.2951 | Precision: 0.1868 | Recall: 0.9200 | F1: 0.3106 | ROC-AUC: 0.5803\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[127033 644162]\n",
      " [ 12871 148018]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19, 'f1': 0.31061339735108373}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=1.4 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "‚úì RF entrenado en 8.2 min | AUC valid=0.6052\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.6686 | Precision: 0.2393 | Recall: 0.4224 | F1: 0.3055 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[555221 215974]\n",
      " [ 92933  67956]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.410) ==\n",
      "Accuracy: 0.5272 | Precision: 0.2139 | Recall: 0.6503 | F1: 0.3220 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[386720 384475]\n",
      " [ 56255 104634]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.41, 'f1': 0.32195175985156876}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "‚úÖ Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "‚úÖ Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (modos FAST/BALANCED/FULL)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---------- Configura el modo ----------\n",
    "# \"FAST\":     solo LGBM nativo (r√°pido)\n",
    "# \"BALANCED\": LGBM + XGB (grid peque√±o) + RF ligero   ‚Üê recomendado\n",
    "# \"FULL\":     LGBM + XGB (grid 3) + RF grande (lento)\n",
    "MODE = \"BALANCED\"\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === Selecci√≥n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # LGBM con categ√≥ricas nativas\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    # fallback a TE num√©rico si prefieres\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"\n",
    "\n",
    "# XGB/RF usan TE num√©rico\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"‚úì Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ======== util: barrido de umbral F1 ========\n",
    "import numpy as np\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (categ√≥ricas nativas)\n",
    "# =========================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Ajustes que ayudan con categ√≥ricas de alta cardinalidad\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=3000,            # ‚Üì porque early_stopping corta antes\n",
    "    min_child_samples=70,         # 60‚Äì80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    feature_fraction=0.85,\n",
    "    bagging_fraction=0.85,\n",
    "    bagging_freq=1,\n",
    "    # regularizadores de categ√≥ricas:\n",
    "    min_data_per_group=100,       # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=191)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=191),\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "        dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=120), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# M√©tricas LGBM\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE num√©ricas)\n",
    "# =========================\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=3000,        # ‚Üì por early stopping\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=120\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    xgb_grid = []  # no corre XGB\n",
    "elif MODE == \"BALANCED\":\n",
    "    xgb_grid = [\n",
    "        xgb_base,  # una sola config razonable\n",
    "    ]\n",
    "else:  # FULL\n",
    "    xgb_grid = [\n",
    "        xgb_base,\n",
    "        {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "        {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "    ]\n",
    "\n",
    "best_xgb = {\"auc\":-1}\n",
    "if xgb_grid:\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "else:\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE num√©ricas)\n",
    "# ==================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf_cfg = None  # no corre RF\n",
    "elif MODE == \"BALANCED\":\n",
    "    rf_cfg = dict(n_estimators=120, max_depth=14)   # mucho m√°s r√°pido, similar AUC al grande\n",
    "else:  # FULL\n",
    "    rf_cfg = dict(n_estimators=300, max_depth=18)   # tu configuraci√≥n pesada\n",
    "\n",
    "if rf_cfg:\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs = time.time()-t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "else:\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, adem√°s guardamos con tu nombre hist√≥rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"‚úÖ Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos m√≠nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1b57a",
   "metadata": {},
   "source": [
    "verificaciones r√°pidas (sin re-entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebcf25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM usado con categ√≥ricas nativas: True\n",
      "Categ√≥ricas registradas: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA']\n",
      "\n",
      "Dtypes categ√≥ricas en Xtr_lgb:\n",
      "{'AIRLINE': 'category', 'ORIGIN_AIRPORT': 'category', 'DESTINATION_AIRPORT': 'category', 'RUTA': 'category'}\n",
      "\n",
      "LGBM best_iteration_: 2\n",
      "\n",
      "Top-15 features por gain (LGBM):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MINUTO_DIA_SALIDA</td>\n",
       "      <td>574523.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORIGIN_AIRPORT</td>\n",
       "      <td>269991.314453</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESTINATION_AIRPORT</td>\n",
       "      <td>200796.485474</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUTA</td>\n",
       "      <td>179082.137115</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MONTH</td>\n",
       "      <td>157465.047974</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SALIDA_SIN</td>\n",
       "      <td>111745.900391</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SALIDA_COS</td>\n",
       "      <td>55363.924652</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>43713.608673</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MONTH_COS</td>\n",
       "      <td>12062.956177</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DISTANCIA_HAV</td>\n",
       "      <td>4638.095947</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRLINE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MONTH_SIN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HORA_SALIDA</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature           gain  split\n",
       "11    MINUTO_DIA_SALIDA  574523.000000      2\n",
       "1        ORIGIN_AIRPORT  269991.314453     96\n",
       "2   DESTINATION_AIRPORT  200796.485474     81\n",
       "3                  RUTA  179082.137115    157\n",
       "4                 MONTH  157465.047974     63\n",
       "6            SALIDA_SIN  111745.900391     20\n",
       "7            SALIDA_COS   55363.924652     25\n",
       "5           DAY_OF_WEEK   43713.608673     46\n",
       "9             MONTH_COS   12062.956177     10\n",
       "10        DISTANCIA_HAV    4638.095947      8\n",
       "0               AIRLINE       0.000000      0\n",
       "8             MONTH_SIN       0.000000      0\n",
       "12          HORA_SALIDA       0.000000      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen modelos:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1*</th>\n",
       "      <th>thr*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.310613</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.606032</td>\n",
       "      <td>0.320829</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.605190</td>\n",
       "      <td>0.321952</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  modelo       AUC       F1*  thr*\n",
       "0   LGBM  0.580286  0.310613  0.19\n",
       "1    XGB  0.606032  0.320829  0.48\n",
       "2     RF  0.605190  0.321952  0.41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buckets de prob (LGBM): [low, high, n, tasa_real]\n",
      "(np.float64(0.0), np.float64(0.1), 0, None)\n",
      "(np.float64(0.1), np.float64(0.2), 789696, 0.16292092146851447)\n",
      "(np.float64(0.2), np.float64(0.30000000000000004), 142388, 0.22636036744669494)\n",
      "(np.float64(0.30000000000000004), np.float64(0.4), 0, None)\n",
      "(np.float64(0.4), np.float64(0.5), 0, None)\n",
      "(np.float64(0.5), np.float64(0.6000000000000001), 0, None)\n",
      "(np.float64(0.6000000000000001), np.float64(0.7000000000000001), 0, None)\n",
      "(np.float64(0.7000000000000001), np.float64(0.8), 0, None)\n",
      "(np.float64(0.8), np.float64(0.9), 0, None)\n",
      "(np.float64(0.9), np.float64(1.0), 0, None)\n"
     ]
    }
   ],
   "source": [
    "# ===== Auditar la corrida actual (sin reentrenar) =====\n",
    "print(\"LGBM usado con categ√≥ricas nativas:\", True)\n",
    "print(\"Categ√≥ricas registradas:\", LGBM_CATEGORICAL_FEATURES)\n",
    "\n",
    "# 1) Verifica dtypes de las categ√≥ricas (deben ser 'category' u 'object'):\n",
    "print(\"\\nDtypes categ√≥ricas en Xtr_lgb:\")\n",
    "print({c: str(Xtr_lgb[c].dtype) for c in LGBM_CATEGORICAL_FEATURES})\n",
    "\n",
    "# 2) ¬øCu√°ntas iteraciones aprendi√≥ LGBM?\n",
    "print(\"\\nLGBM best_iteration_:\", getattr(lgbm, \"best_iteration_\", None))\n",
    "\n",
    "# 3) Importancias de LGBM (top 15):\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\": lgbm.feature_name_,\n",
    "        \"gain\": lgbm.booster_.feature_importance(importance_type=\"gain\"),\n",
    "        \"split\": lgbm.booster_.feature_importance(importance_type=\"split\"),\n",
    "    }).sort_values(\"gain\", ascending=False)\n",
    "    print(\"\\nTop-15 features por gain (LGBM):\")\n",
    "    display(fi.head(15))\n",
    "except Exception as e:\n",
    "    print(\"No fue posible extraer importancias:\", e)\n",
    "\n",
    "# 4) Comparaci√≥n resumida de AUC y mejor F1/umbral\n",
    "def _summary_row(name, proba, best):\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "    if proba is None:\n",
    "        return {\"modelo\": name, \"AUC\": None, \"F1*\": None, \"thr*\": None}\n",
    "    return {\n",
    "        \"modelo\": name,\n",
    "        \"AUC\": float(roc_auc_score(y_valid, proba)),\n",
    "        \"F1*\": float(best.get(\"f1\", None)),\n",
    "        \"thr*\": float(best.get(\"thr\", None)),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "rows.append(_summary_row(\"LGBM\", lgb_proba, best_lgb_thr))\n",
    "rows.append(_summary_row(\"XGB\",  best_xgb.get(\"proba\"), best_xgb_thr))\n",
    "rows.append(_summary_row(\"RF\",   (rf_proba if 'rf_proba' in globals() else None), best_rf_thr))\n",
    "import pandas as pd\n",
    "print(\"\\nResumen modelos:\")\n",
    "display(pd.DataFrame(rows))\n",
    "\n",
    "# 5) Sanidad de distribuci√≥n de probabilidades (calibraci√≥n gruesa)\n",
    "import numpy as np\n",
    "def _prob_buckets(y, p, k=10):\n",
    "    bins = np.linspace(0,1,k+1)\n",
    "    idx = np.digitize(p, bins)-1\n",
    "    out = []\n",
    "    for b in range(k):\n",
    "        mask = (idx==b)\n",
    "        if mask.sum()==0:\n",
    "            out.append((bins[b], bins[b+1], 0, None))\n",
    "        else:\n",
    "            out.append((bins[b], bins[b+1], int(mask.sum()), float(y[mask].mean())))\n",
    "    return out\n",
    "\n",
    "print(\"\\nBuckets de prob (LGBM): [low, high, n, tasa_real]\")\n",
    "for row in _prob_buckets(y_valid.values, lgb_proba, k=10):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c5623",
   "metadata": {},
   "source": [
    "otro 7 v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d6ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices | LGBM: (4299046, 27)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 120 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588349\tvalid_0's binary_logloss: 0.459685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5883 | best_iter=4 | tiempo=1.5 min | params={lr:0.02, leaves:255, mcs:70, ff:0.9, bf:0.9}\n",
      "‚úì LGBM (mejor) | AUC valid=0.5883\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5395 | Precision: 0.2115 | Recall: 0.6114 | F1: 0.3143 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404475 366720]\n",
      " [ 62517  98372]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19999999999999996, 'f1': 0.31429707930432393}\n",
      "\n",
      "=== MEJOR MODELO: LGBM | AUC=0.5883 ===\n",
      "‚úÖ Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "‚úÖ Copia LGBM (compatibilidad): D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_regressor_default.joblib\n",
      "‚úÖ Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (modos FAST/BALANCED/FULL)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---------- Configura el modo ----------\n",
    "# \"FAST\":     solo LGBM nativo (r√°pido)\n",
    "# \"BALANCED\": LGBM + XGB (grid peque√±o) + RF ligero   ‚Üê recomendado\n",
    "# \"FULL\":     LGBM + XGB (grid 3) + RF grande (lento)\n",
    "MODE = \"FAST\"\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# === Selecci√≥n de matrices ===\n",
    "USE_LGBM_NATIVE_CATS = True  # LGBM con categ√≥ricas nativas\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    Xtr_lgb, Xva_lgb = X_train_base_for_lgb, X_valid_base_for_lgb\n",
    "    lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "else:\n",
    "    # fallback a TE num√©rico si prefieres\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"\n",
    "\n",
    "# XGB/RF usan TE num√©rico\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"‚úì Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# --- desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ======== util: barrido de umbral F1 ========\n",
    "import numpy as np\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# =========================\n",
    "# A) LightGBM (categ√≥ricas nativas)\n",
    "# =========================\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Ajustes que ayudan con categ√≥ricas de alta cardinalidad\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=3000,            # ‚Üì porque early_stopping corta antes\n",
    "    min_child_samples=70,         # 60‚Äì80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,\n",
    "    feature_fraction=0.90,\n",
    "    bagging_fraction=0.90,\n",
    "    bagging_freq=1,\n",
    "    # regularizadores de categ√≥ricas:\n",
    "    min_data_per_group=100,       # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=255)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=191),\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=60),\n",
    "        dict(learning_rate=0.025, num_leaves=223, min_child_samples=80),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=120), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# M√©tricas LGBM\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# =========================\n",
    "# B) XGBoost (usa TE num√©ricas)\n",
    "# =========================\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=3000,        # ‚Üì por early stopping\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=120\n",
    "    )\n",
    "    secs = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    xgb_grid = []  # no corre XGB\n",
    "elif MODE == \"BALANCED\":\n",
    "    xgb_grid = [\n",
    "        xgb_base,  # una sola config razonable\n",
    "    ]\n",
    "else:  # FULL\n",
    "    xgb_grid = [\n",
    "        xgb_base,\n",
    "        {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "        {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9}\n",
    "    ]\n",
    "\n",
    "best_xgb = {\"auc\":-1}\n",
    "if xgb_grid:\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "else:\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# ==================================\n",
    "# C) Random Forest (usa TE num√©ricas)\n",
    "# ==================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf_cfg = None  # no corre RF\n",
    "elif MODE == \"BALANCED\":\n",
    "    rf_cfg = dict(n_estimators=120, max_depth=14)   # mucho m√°s r√°pido, similar AUC al grande\n",
    "else:  # FULL\n",
    "    rf_cfg = dict(n_estimators=300, max_depth=18)   # tu configuraci√≥n pesada\n",
    "\n",
    "if rf_cfg:\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs = time.time()-t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "else:\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "\n",
    "# =========================\n",
    "# D) Selecci√≥n del mejor + guardado\n",
    "# =========================\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb[\"auc\"], best_xgb[\"model\"], best_xgb[\"proba\"], best_xgb_thr),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar el mejor modelo en .joblib (compatible con tu estructura)\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Si el mejor fue LGBM, adem√°s guardamos con tu nombre hist√≥rico\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"‚úÖ Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos m√≠nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255a9a2",
   "metadata": {},
   "source": [
    "0tro 7 v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49442fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices | LGBM: (4299046, 27)  | XGB/RF: (4299046, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.589119\tvalid_0's binary_logloss: 0.563532\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588349\tvalid_0's binary_logloss: 0.459685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5883 | best_iter=4 | tiempo=2.3 min | params={lr:0.02, leaves:255, mcs:60, ff:0.9, bf:0.9}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.506626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.589839\tvalid_0's binary_logloss: 0.56256\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.588188\tvalid_0's binary_logloss: 0.459791\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "AUC valid=0.5882 | best_iter=4 | tiempo=2.9 min | params={lr:0.02, leaves:191, mcs:70, ff:0.9, bf:0.9}\n",
      "‚úì LGBM (mejor) | AUC valid=0.5883\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.5395 | Precision: 0.2115 | Recall: 0.6114 | F1: 0.3143 | ROC-AUC: 0.5883\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[404475 366720]\n",
      " [ 62517  98372]]\n",
      "‚Üí LGBM umbral F1 √≥ptimo: {'thr': 0.19999999999999996, 'f1': 0.31429707930432393}\n",
      "Probando XGB: {'eta': 0.05, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
      "AUC valid=0.6060 | best_iter=2 | tiempo=2.5 min\n",
      "\n",
      "\n",
      "== XGB Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.7056 | Precision: 0.2432 | Recall: 0.3342 | F1: 0.2815 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[603897 167298]\n",
      " [107126  53763]]\n",
      "\n",
      "== XGB Mejor F1 (thr=0.480) ==\n",
      "Accuracy: 0.4384 | Precision: 0.2027 | Recall: 0.7685 | F1: 0.3208 | ROC-AUC: 0.6060\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[284965 486230]\n",
      " [ 37248 123641]]\n",
      "‚Üí XGB umbral F1 √≥ptimo: {'thr': 0.48, 'f1': 0.32082879236078676}\n",
      "‚úì RF entrenado en 7.5 min | AUC valid=0.6052\n",
      "\n",
      "== RF Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.6686 | Precision: 0.2393 | Recall: 0.4224 | F1: 0.3055 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[555221 215974]\n",
      " [ 92933  67956]]\n",
      "\n",
      "== RF Mejor F1 (thr=0.410) ==\n",
      "Accuracy: 0.5272 | Precision: 0.2139 | Recall: 0.6503 | F1: 0.3220 | ROC-AUC: 0.6052\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[386720 384475]\n",
      " [ 56255 104634]]\n",
      "‚Üí RF umbral F1 √≥ptimo: {'thr': 0.41, 'f1': 0.32195175985156876}\n",
      "\n",
      "=== MEJOR MODELO: XGB | AUC=0.6060 ===\n",
      "‚úÖ Modelo guardado: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\xgb_retrasos.joblib\n",
      "‚úÖ Metadatos: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) Entrenar LGBM, XGB y RF (LGBM con CAT nativas + AGGs)\n",
    "#     Modos: FAST / BALANCED / FULL\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuraci√≥n del experimento\n",
    "# -----------------------------\n",
    "# \"FAST\":     Solo LGBM (r√°pido) para iterar.\n",
    "# \"BALANCED\": LGBM + XGB (1 cfg) + RF ligero  ‚Üê recomendado\n",
    "# \"FULL\":     LGBM + XGB (3 cfg) + RF grande  (m√°s lento)\n",
    "MODE = \"BALANCED\"\n",
    "\n",
    "# LGBM usar√° categ√≥ricas nativas + columnas num√©ricas de agregados/TE\n",
    "USE_LGBM_NATIVE_CATS = True\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Selecci√≥n y sanidad de matrices\n",
    "# -----------------------------\n",
    "# Requisitos de celdas previas:\n",
    "#  - X_train_base_for_lgb, X_valid_base_for_lgb (con ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA'] como 'category'\n",
    "#    y adem√°s anexadas las columnas num√©ricas de agregados/TE)  ‚Üê generado en la celda 6.6\n",
    "#  - LGBM_CATEGORICAL_FEATURES = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "#  - X_train_model, X_valid_model, FEATURE_ORDER (matrices num√©ricas TE/AGG para XGB/RF)\n",
    "#  - y_train, y_valid\n",
    "#  - MODELS_DIR, ARTIF_DIR (rutas de guardado)\n",
    "\n",
    "if USE_LGBM_NATIVE_CATS:\n",
    "    # LGBM ver√° categ√≥ricas nativas + num√©ricas adicionales\n",
    "    Xtr_lgb = X_train_base_for_lgb.copy()\n",
    "    Xva_lgb = X_valid_base_for_lgb.copy()\n",
    "    try:\n",
    "        lgb_cats = LGBM_CATEGORICAL_FEATURES\n",
    "    except NameError:\n",
    "        lgb_cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "        # Forzar dtype 'category' por si acaso\n",
    "        for c in lgb_cats:\n",
    "            if c in Xtr_lgb.columns: Xtr_lgb[c] = Xtr_lgb[c].astype('category')\n",
    "            if c in Xva_lgb.columns: Xva_lgb[c] = Xva_lgb[c].astype('category')\n",
    "else:\n",
    "    # Fallback: todo num√©rico (TE) para LGBM\n",
    "    Xtr_lgb = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "    Xva_lgb = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "    lgb_cats = \"auto\"  # ya no hay categ√≥ricas nativas\n",
    "\n",
    "# XGB/RF usan las matrices num√©ricas (TE/AGG) alineadas\n",
    "Xtr_num = pd.DataFrame(X_train_model, columns=FEATURE_ORDER)\n",
    "Xva_num = pd.DataFrame(X_valid_model, columns=FEATURE_ORDER)\n",
    "\n",
    "print(f\"‚úì Matrices | LGBM: {Xtr_lgb.shape}  | XGB/RF: {Xtr_num.shape}\")\n",
    "\n",
    "# Peso por desbalance (clase 1 retraso ~18-19%)\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos, 1), 1.0)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utilidades de m√©tricas\n",
    "# -----------------------------\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    \"\"\"Imprime m√©tricas para un umbral dado y retorna un dict con los valores.\"\"\"\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.8, steps=76):\n",
    "    \"\"\"Barrido simple de umbral para maximizar F1 (√∫til en clases desbalanceadas).\"\"\"\n",
    "    best = {\"thr\":0.5, \"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LightGBM (categ√≥ricas nativas + AGGs)\n",
    "# -----------------------------\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hiperpar√°metros base (dise√±ados para entrenar \"de verdad\" con early stopping)\n",
    "lgb_common = dict(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=4000,           # suficiente; early_stopping corta antes si no mejora\n",
    "    min_child_samples=60,        # 60‚Äì80 sugerido\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=5.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_bin=255,                 # histograma m√°s fino\n",
    "    feature_fraction=0.90,       # ‚Üë variaci√≥n por columna\n",
    "    bagging_fraction=0.90,       # ‚Üë variaci√≥n por fila\n",
    "    bagging_freq=1,\n",
    "    # Regularizadores para categ√≥ricas de alta cardinalidad:\n",
    "    min_data_per_group=100,      # (a.k.a. min_data_per_categorical_bin)\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    ")\n",
    "\n",
    "# Peque√±o espacio de b√∫squeda seg√∫n el modo\n",
    "if MODE == \"FAST\":\n",
    "    lgb_try = [dict(learning_rate=0.02, num_leaves=191, min_child_samples=60)]\n",
    "elif MODE == \"BALANCED\":\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02, num_leaves=255, min_child_samples=60),\n",
    "        dict(learning_rate=0.02, num_leaves=191, min_child_samples=70),\n",
    "    ]\n",
    "else:  # FULL\n",
    "    lgb_try = [\n",
    "        dict(learning_rate=0.02,  num_leaves=255, min_child_samples=60),\n",
    "        dict(learning_rate=0.02,  num_leaves=191, min_child_samples=70),\n",
    "        dict(learning_rate=0.015, num_leaves=255, min_child_samples=60),\n",
    "    ]\n",
    "\n",
    "def train_eval_lgb(params):\n",
    "    \"\"\"Entrena LGBM con eval AUC y early stopping; retorna modelo, proba, auc y tiempo.\"\"\"\n",
    "    p = {**lgb_common, **params}\n",
    "    mdl = lgb.LGBMClassifier(**p)\n",
    "    t0 = time.time()\n",
    "    mdl.fit(\n",
    "        Xtr_lgb, y_train,\n",
    "        eval_set=[(Xva_lgb, y_valid)],\n",
    "        eval_metric=\"auc\",\n",
    "        categorical_feature=lgb_cats,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    secs  = time.time() - t0\n",
    "    proba = mdl.predict_proba(Xva_lgb)[:, 1]\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    print(\n",
    "        f\"AUC valid={auc:.4f} | best_iter={getattr(mdl,'best_iteration_',None)} | \"\n",
    "        f\"tiempo={secs/60:.1f} min | params={{lr:{p['learning_rate']}, leaves:{p['num_leaves']}, \"\n",
    "        f\"mcs:{p.get('min_child_samples')}, ff:{p['feature_fraction']}, bf:{p['bagging_fraction']}}}\"\n",
    "    )\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "best_lgb = {\"auc\": -1}\n",
    "for cfg in lgb_try:\n",
    "    mdl, proba, auc, secs = train_eval_lgb(cfg)\n",
    "    if auc > best_lgb[\"auc\"]:\n",
    "        best_lgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": cfg}\n",
    "\n",
    "lgbm      = best_lgb[\"model\"]\n",
    "lgb_proba = best_lgb[\"proba\"]\n",
    "lgb_auc   = best_lgb[\"auc\"]\n",
    "print(f\"‚úì LGBM (mejor) | AUC valid={lgb_auc:.4f}\")\n",
    "\n",
    "# M√©tricas LGBM base y con umbral √≥ptimo F1\n",
    "_ = report_metrics(y_valid, lgb_proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_lgb_thr = best_f1_threshold(y_valid, lgb_proba, lo=0.05, hi=0.6, steps=56)\n",
    "_ = report_metrics(y_valid, lgb_proba, best_lgb_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí LGBM umbral F1 √≥ptimo:\", best_lgb_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) XGBoost (TE num√©ricas)\n",
    "# -----------------------------\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "\n",
    "xgb_base = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "def train_eval_xgb(params):\n",
    "    \"\"\"Entrena XGB con eval AUC y early stopping; retorna modelo, proba, auc y tiempo.\"\"\"\n",
    "    dtrain = DMatrix(Xtr_num, label=y_train)\n",
    "    dvalid = DMatrix(Xva_num, label=y_valid)\n",
    "    t0 = time.time()\n",
    "    mdl = xgb_train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=4000 if MODE==\"FULL\" else 3000,\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    secs  = time.time() - t0\n",
    "    proba = mdl.predict(dvalid, iteration_range=(0, mdl.best_iteration+1))\n",
    "    auc   = roc_auc_score(y_valid, proba)\n",
    "    return mdl, proba, auc, secs\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    best_xgb = {\"auc\": -1, \"model\": None, \"proba\": None}\n",
    "    best_xgb_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "else:\n",
    "    if MODE == \"BALANCED\":\n",
    "        xgb_grid = [xgb_base]  # 1 configuraci√≥n razonable\n",
    "    else:  # FULL\n",
    "        xgb_grid = [\n",
    "            xgb_base,\n",
    "            {**xgb_base, \"max_depth\":8, \"min_child_weight\":5},\n",
    "            {**xgb_base, \"eta\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9},\n",
    "        ]\n",
    "    best_xgb = {\"auc\": -1}\n",
    "    for p in xgb_grid:\n",
    "        print(\"Probando XGB:\", {k:p[k] for k in [\"eta\",\"max_depth\",\"min_child_weight\",\"subsample\",\"colsample_bytree\"]})\n",
    "        mdl, proba, auc, secs = train_eval_xgb(p)\n",
    "        print(f\"AUC valid={auc:.4f} | best_iter={mdl.best_iteration} | tiempo={secs/60:.1f} min\\n\")\n",
    "        if auc > best_xgb[\"auc\"]:\n",
    "            best_xgb = {\"model\": mdl, \"proba\": proba, \"auc\": auc, \"params\": p}\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], 0.5, \"XGB Base 0.5\")\n",
    "    best_xgb_thr = best_f1_threshold(y_valid, best_xgb[\"proba\"], lo=0.05, hi=0.8, steps=76)\n",
    "    _ = report_metrics(y_valid, best_xgb[\"proba\"], best_xgb_thr[\"thr\"], \"XGB Mejor F1\")\n",
    "    print(\"‚Üí XGB umbral F1 √≥ptimo:\", best_xgb_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Random Forest (TE num√©ricas)\n",
    "# -----------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    rf, rf_proba, rf_auc = None, None, -1\n",
    "    best_rf_thr = {\"thr\": 0.5, \"f1\": -1}\n",
    "else:\n",
    "    rf_cfg = (dict(n_estimators=120, max_depth=14) if MODE==\"BALANCED\"\n",
    "              else dict(n_estimators=300, max_depth=18))\n",
    "    rf_params = dict(\n",
    "        **rf_cfg,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=4,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rf = RandomForestClassifier(**rf_params).fit(Xtr_num, y_train)\n",
    "    rf_secs  = time.time() - t0\n",
    "    rf_proba = rf.predict_proba(Xva_num)[:, 1]\n",
    "    rf_auc   = roc_auc_score(y_valid, rf_proba)\n",
    "    print(f\"‚úì RF entrenado en {rf_secs/60:.1f} min | AUC valid={rf_auc:.4f}\")\n",
    "\n",
    "    best_rf_thr = best_f1_threshold(y_valid, rf_proba, lo=0.05, hi=0.5, steps=46)\n",
    "    _ = report_metrics(y_valid, rf_proba, 0.5, \"RF Base 0.5\")\n",
    "    _ = report_metrics(y_valid, rf_proba, best_rf_thr[\"thr\"], \"RF Mejor F1\")\n",
    "    print(\"‚Üí RF umbral F1 √≥ptimo:\", best_rf_thr)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Selecci√≥n del mejor y guardado\n",
    "# -----------------------------\n",
    "candidatos = [\n",
    "    (\"lgbm\", lgb_auc, lgbm, lgb_proba, best_lgb_thr),\n",
    "    (\"xgb\",  best_xgb.get(\"auc\", -1), best_xgb.get(\"model\", None), best_xgb.get(\"proba\", None), locals().get(\"best_xgb_thr\", {\"thr\":0.5,\"f1\":-1})),\n",
    "    (\"rf\",   rf_auc, rf, rf_proba, best_rf_thr),\n",
    "]\n",
    "candidatos.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_auc, best_model, best_proba, best_thr = candidatos[0]\n",
    "print(f\"\\n=== MEJOR MODELO: {best_name.upper()} | AUC={best_auc:.4f} ===\")\n",
    "\n",
    "# Guardar best model en tu estructura\n",
    "best_model_path = MODELS_DIR / f\"{best_name}_retrasos.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Compatibilidad hist√≥rica si el mejor fue LGBM\n",
    "if best_name == \"lgbm\":\n",
    "    lgbm_default_path = MODELS_DIR / \"lgbm_regressor_default.joblib\"\n",
    "    joblib.dump(best_model, lgbm_default_path)\n",
    "    print(f\"‚úÖ Copia LGBM (compatibilidad): {lgbm_default_path}\")\n",
    "\n",
    "# Guardar metadatos m√≠nimos\n",
    "meta = {\n",
    "    \"mode\": MODE,\n",
    "    \"best_model\": best_name,\n",
    "    \"auc_valid\": float(best_auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_order\": FEATURE_ORDER,                      # orden de columnas num√©ricas (XGB/RF)\n",
    "    \"use_lgbm_native_cats\": bool(USE_LGBM_NATIVE_CATS), # bandera informativa\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Metadatos:\", ARTIF_DIR / \"metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951189e9",
   "metadata": {},
   "source": [
    "otro 7 v4 con parametros anteriores que dan uac > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e684659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrices num√©ricas | X_train: (4299046, 27)  | X_valid: (932084, 27)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 4196\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 27\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.592492\tvalid_0's binary_logloss: 0.596723\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.594905\tvalid_0's binary_logloss: 0.46069\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "‚úÖ LGBM listo | AUC valid=0.5949 | best_iter=2 | tiempo=1.5 min\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5949\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4012 | Precision: 0.1978 | Recall: 0.8079 | F1: 0.3178 | ROC-AUC: 0.5949\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[243967 527228]\n",
      " [ 30912 129977]]\n",
      "‚Üí Umbral F1 √≥ptimo: {'thr': 0.19999999999999996, 'f1': 0.31775566132987165}\n",
      "üíæ Modelo guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "üìù Metadatos guardados: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata_lgbm.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) LGBM \"WINNER\" ‚Äî lr=0.05, leaves=63, mcs=100, ff=0.8, bf=0.8\n",
    "#     (replica la corrida que te dio AUC ~0.6025)\n",
    "# ============================================\n",
    "import time, json, joblib\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ---- Utilidades de reporte ----\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.60, steps=56):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# ---- Asegurar rutas de artefactos ----\n",
    "try:\n",
    "    ARTIF_DIR\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "    ARTIF_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Selecci√≥n de matrices (prioriza num√©ricas con AGGs/TE) ----\n",
    "use_numeric = 'X_train_model' in globals() and 'X_valid_model' in globals()\n",
    "if use_numeric:\n",
    "    Xtr, Xva = X_train_model, X_valid_model\n",
    "    cats = None  # ya es todo num√©rico\n",
    "    print(f\"‚úì Matrices num√©ricas | X_train: {Xtr.shape}  | X_valid: {Xva.shape}\")\n",
    "else:\n",
    "    # fallback a las categ√≥ricas nativas\n",
    "    Xtr, Xva = Xtr_lgb.copy(), Xva_lgb.copy()\n",
    "    cats = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','RUTA']\n",
    "    for c in cats:\n",
    "        if c in Xtr.columns: Xtr[c] = Xtr[c].astype('category')\n",
    "        if c in Xva.columns: Xva[c] = Xva[c].astype('category')\n",
    "    print(f\"‚úì Matrices LGBM (cats nativas) | X_train: {Xtr.shape}  | X_valid: {Xva.shape}\")\n",
    "\n",
    "# ---- Pos/Neg para desbalance ----\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ---- Par√°metros ganadores (los que te dieron ~0.6025) ----\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=100,\n",
    "    n_estimators=3000,           # early_stopping cortar√° mucho antes\n",
    "    feature_fraction=0.80,\n",
    "    bagging_fraction=0.80,\n",
    "    bagging_freq=1,\n",
    "    max_bin=255,\n",
    "    # regularizadores √∫tiles con alta cardinalidad (no estorban en num√©rico)\n",
    "    min_data_per_group=100,\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    "    # desbalance\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    # estabilidad/performance\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    # en datasets grandes suele ir mejor col-wise\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# ---- Entrenamiento ----\n",
    "t0 = time.time()\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "fit_kwargs = dict(\n",
    "    X=Xtr, y=y_train,\n",
    "    eval_set=[(Xva, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "if cats is not None:\n",
    "    fit_kwargs[\"categorical_feature\"] = cats\n",
    "\n",
    "lgbm.fit(**fit_kwargs)\n",
    "secs = time.time() - t0\n",
    "\n",
    "proba = lgbm.predict_proba(Xva)[:,1]\n",
    "auc   = roc_auc_score(y_valid, proba)\n",
    "print(f\"‚úÖ LGBM listo | AUC valid={auc:.4f} | best_iter={getattr(lgbm,'best_iteration_',None)} | tiempo={secs/60:.1f} min\")\n",
    "\n",
    "# ---- Reportes: base 0.5 y mejor F1 ----\n",
    "_ = report_metrics(y_valid, proba, 0.5, \"LGBM Base 0.5\")\n",
    "best_thr = best_f1_threshold(y_valid, proba, lo=0.05, hi=0.60, steps=56)\n",
    "_ = report_metrics(y_valid, proba, best_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí Umbral F1 √≥ptimo:\", best_thr)\n",
    "\n",
    "# ---- Guardado de modelo + metadatos ----\n",
    "best_model_path = MODELS_DIR / \"lgbm_retrasos.joblib\"\n",
    "joblib.dump(lgbm, best_model_path)\n",
    "print(f\"üíæ Modelo guardado en: {best_model_path}\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"lgbm\",\n",
    "    \"auc_valid\": float(auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_count\": int(Xtr.shape[1]),\n",
    "    \"used_numeric_matrix\": bool(use_numeric),\n",
    "    \"categorical_features\": (cats or []),\n",
    "    \"params\": {\n",
    "        **{k: (float(v) if isinstance(v, (np.floating,)) else v) for k, v in lgb_params.items()},\n",
    "        \"scale_pos_weight\": float(scale_pos_weight)\n",
    "    },\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata_lgbm.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"üìù Metadatos guardados:\", ARTIF_DIR / \"metadata_lgbm.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def371e",
   "metadata": {},
   "source": [
    "Celda 7 (patch LGBM con 23 features y sin IDs crudos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc080da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LGBM num√©rico (filtrado) | X_train: (4299046, 23) | X_valid: (932084, 23)\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Total Bins 3531\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 23\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's auc: 0.592105\tvalid_0's binary_logloss: 0.596356\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.592992\tvalid_0's binary_logloss: 0.460485\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "‚úÖ LGBM listo | AUC valid=0.5930 | best_iter=1 | tiempo=1.3 min\n",
      "\n",
      "== LGBM Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000 | ROC-AUC: 0.5930\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== LGBM Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4339 | Precision: 0.1987 | Recall: 0.7515 | F1: 0.3143 | ROC-AUC: 0.5930\n",
      "CM [TN, FP; FN, TP]:\n",
      " [[283500 487695]\n",
      " [ 39975 120914]]\n",
      "‚Üí Umbral F1 √≥ptimo: {'thr': 0.19999999999999996, 'f1': 0.31426722356653297}\n",
      "üíæ Modelo guardado en: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\models\\lgbm_retrasos.joblib\n",
      "üìù Metadatos guardados: D:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\script_prueba\\artifacts\\metadata_lgbm.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7) LGBM WINNER (num√©rico TE+aggs) - 23 features (sin IDs crudos)\n",
    "# ============================================\n",
    "import time, json, joblib, numpy as np, lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | ROC-AUC: {auc:.4f}\")\n",
    "    print(\"CM [TN, FP; FN, TP]:\\n\", confusion_matrix(y_true, y_hat))\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.60, steps=56):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "# --- Asegurar rutas ---\n",
    "try:\n",
    "    ARTIF_DIR\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "    ARTIF_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Whitelist de 23 features (SIN ids crudos) ---\n",
    "FEATS_BASE = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\n",
    "              \"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\",\"HORA_SALIDA\"]\n",
    "FEATS_TE   = [\"AIRLINE_TE\",\"ORIGIN_AIRPORT_TE\",\"DESTINATION_AIRPORT_TE\",\"RUTA_TE\"]\n",
    "FEATS_AGG  = [\"AIR_rate\",\"AIR_n\",\"DES_rate\",\"DES_n\",\"ORI_rate\",\"ORI_n\",\n",
    "              \"RUTA_rate\",\"RUTA_n\",\"RUTA_HORA_rate\",\"RUTA_HORA_n\"]\n",
    "FEATS_23   = FEATS_BASE + FEATS_TE + FEATS_AGG\n",
    "\n",
    "# --- Selecci√≥n segura de matrices num√©ricas y filtrado de columnas ---\n",
    "assert 'X_train_model' in globals() and 'X_valid_model' in globals(), \\\n",
    "    \"No encuentro X_train_model / X_valid_model. Asegura ejecutar TE+aggs antes.\"\n",
    "missing_cols = [c for c in FEATS_23 if c not in X_train_model.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"Faltan columnas esperadas en X_train_model: {missing_cols}\")\n",
    "\n",
    "Xtr = X_train_model[FEATS_23].copy()\n",
    "Xva = X_valid_model[FEATS_23].copy()\n",
    "print(f\"‚úì LGBM num√©rico (filtrado) | X_train: {Xtr.shape} | X_valid: {Xva.shape}\")\n",
    "\n",
    "# --- Desbalance ---\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# --- Hiperpar√°metros ganadores ---\n",
    "lgb_params = dict(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=100,\n",
    "    n_estimators=3000,\n",
    "    feature_fraction=0.80,\n",
    "    bagging_fraction=0.80,\n",
    "    bagging_freq=1,\n",
    "    max_bin=255,\n",
    "    min_data_per_group=100,\n",
    "    cat_l2=10.0,\n",
    "    cat_smooth=10.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    force_col_wise=True,\n",
    ")\n",
    "\n",
    "# --- Entrenar ---\n",
    "t0 = time.time()\n",
    "lgbm = lgb.LGBMClassifier(**lgb_params)\n",
    "lgbm.fit(\n",
    "    Xtr, y_train,\n",
    "    eval_set=[(Xva, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(200)]\n",
    ")\n",
    "secs  = time.time() - t0\n",
    "proba = lgbm.predict_proba(Xva)[:,1]\n",
    "auc   = roc_auc_score(y_valid, proba)\n",
    "\n",
    "print(f\"‚úÖ LGBM listo | AUC valid={auc:.4f} | best_iter={getattr(lgbm,'best_iteration_',None)} | tiempo={secs/60:.1f} min\")\n",
    "_ = report_metrics(y_valid, proba, 0.5,               \"LGBM Base 0.5\")\n",
    "best_thr = best_f1_threshold(y_valid, proba, 0.05, 0.60, 56)\n",
    "_ = report_metrics(y_valid, proba, best_thr[\"thr\"], \"LGBM Mejor F1\")\n",
    "print(\"‚Üí Umbral F1 √≥ptimo:\", best_thr)\n",
    "\n",
    "# --- Guardar modelo + metadatos ---\n",
    "best_model_path = MODELS_DIR / \"lgbm_retrasos.joblib\"\n",
    "joblib.dump(lgbm, best_model_path)\n",
    "print(f\"üíæ Modelo guardado en: {best_model_path}\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"lgbm\",\n",
    "    \"auc_valid\": float(auc),\n",
    "    \"best_threshold_f1\": best_thr,\n",
    "    \"feature_list\": FEATS_23,\n",
    "    \"feature_count\": int(Xtr.shape[1]),\n",
    "    \"used_numeric_matrix\": True,\n",
    "    \"categorical_features\": [],\n",
    "    \"params\": {**lgb_params, \"scale_pos_weight\": float(scale_pos_weight)},\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "with open(ARTIF_DIR / \"metadata_lgbm.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"üìù Metadatos guardados:\", ARTIF_DIR / \"metadata_lgbm.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c974b3",
   "metadata": {},
   "source": [
    "Celda 7B (comparativa r√°pida XGB/RF + tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d918848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì XGB | AUC=0.5972 | best_iter=9 | t=2.1m | F1*=0.319 @thr=0.440\n",
      "‚úì RF  | AUC=0.5978 | t=21.4m | F1*=0.319 @thr=0.300\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "\n",
      "=== Resumen modelos (AUC, F1*, thr*) ===\n",
      " LGBM  AUC=0.5930  F1*=0.314  thr*=0.200  (acc=0.434, pre=0.199, rec=0.752)\n",
      "  XGB  AUC=0.5972  F1*=0.319  thr*=0.440  (acc=0.498, pre=0.208, rec=0.682)\n",
      "   RF  AUC=0.5978  F1*=0.319  thr*=0.300  (acc=0.525, pre=0.212, rec=0.644)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7B) Comparativa r√°pida: XGB y RF sobre las MISMAS 23 features\n",
    "# ============================================\n",
    "import time, numpy as np, json, joblib\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob, lo=0.05, hi=0.80, steps=76):\n",
    "    best = {\"thr\":0.5,\"f1\":-1}\n",
    "    for thr in np.linspace(lo, hi, steps):\n",
    "        f1 = f1_score(y_true, (y_prob>=thr).astype(int), zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"thr\": float(thr), \"f1\": float(f1)}\n",
    "    return best\n",
    "\n",
    "def quick_report(name, y_true, proba):\n",
    "    thr = best_f1_threshold(y_true, proba)\n",
    "    yhat = (proba >= thr[\"thr\"]).astype(int)\n",
    "    return dict(\n",
    "        modelo=name,\n",
    "        auc=float(roc_auc_score(y_true, proba)),\n",
    "        f1=float(thr[\"f1\"]),\n",
    "        thr=float(thr[\"thr\"]),\n",
    "        acc=float(accuracy_score(y_true, yhat)),\n",
    "        pre=float(precision_score(y_true, yhat, zero_division=0)),\n",
    "        rec=float(recall_score(y_true, yhat, zero_division=0)),\n",
    "    )\n",
    "\n",
    "# Matrices de 23 features (mismas que us√≥ LGBM arriba)\n",
    "Xtr_23 = X_train_model[FEATS_23]\n",
    "Xva_23 = X_valid_model[FEATS_23]\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = max(neg / max(pos,1), 1.0)\n",
    "\n",
    "# ---- XGBoost (setup probado) ----\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    eta=0.05,\n",
    "    max_depth=10,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=0.0,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    nthread=-1,\n",
    "    seed=42\n",
    ")\n",
    "dtr = DMatrix(Xtr_23, label=y_train)\n",
    "dva = DMatrix(Xva_23, label=y_valid)\n",
    "t0 = time.time()\n",
    "xgbm = xgb_train(\n",
    "    xgb_params, dtr,\n",
    "    num_boost_round=3000,\n",
    "    evals=[(dva, \"valid\")],\n",
    "    verbose_eval=False,\n",
    "    early_stopping_rounds=120\n",
    ")\n",
    "xgb_secs = time.time()-t0\n",
    "xgb_proba = xgbm.predict(dva, iteration_range=(0, xgbm.best_iteration+1))\n",
    "xgb_res = quick_report(\"XGB\", y_valid, xgb_proba)\n",
    "print(f\"‚úì XGB | AUC={xgb_res['auc']:.4f} | best_iter={xgbm.best_iteration} | t={xgb_secs/60:.1f}m | F1*={xgb_res['f1']:.3f} @thr={xgb_res['thr']:.3f}\")\n",
    "\n",
    "# ---- Random Forest (ligero probado) ----\n",
    "rf_params = dict(\n",
    "    n_estimators=300, max_depth=24,\n",
    "    min_samples_leaf=2, min_samples_split=4,\n",
    "    max_features=\"sqrt\", bootstrap=True,\n",
    "    n_jobs=-1, random_state=42,\n",
    "    class_weight={0:1.0, 1:float(scale_pos_weight)}\n",
    ")\n",
    "t0 = time.time()\n",
    "rf = RandomForestClassifier(**rf_params).fit(Xtr_23, y_train)\n",
    "rf_secs = time.time()-t0\n",
    "rf_proba = rf.predict_proba(Xva_23)[:,1]\n",
    "rf_res = quick_report(\"RF\", y_valid, rf_proba)\n",
    "print(f\"‚úì RF  | AUC={rf_res['auc']:.4f} | t={rf_secs/60:.1f}m | F1*={rf_res['f1']:.3f} @thr={rf_res['thr']:.3f}\")\n",
    "\n",
    "# ---- Resumen incluyendo LGBM entrenado arriba ----\n",
    "try:\n",
    "    lgb_proba_ = lgbm.predict_proba(Xva_23)[:,1]  # reutiliza objeto de Celda 7\n",
    "    lgb_res = quick_report(\"LGBM\", y_valid, lgb_proba_)\n",
    "except Exception:\n",
    "    lgb_res = {\"modelo\":\"LGBM\",\"auc\":np.nan,\"f1\":np.nan,\"thr\":np.nan,\"acc\":np.nan,\"pre\":np.nan,\"rec\":np.nan}\n",
    "\n",
    "resumen = [lgb_res, xgb_res, rf_res]\n",
    "print(\"\\n=== Resumen modelos (AUC, F1*, thr*) ===\")\n",
    "for r in resumen:\n",
    "    print(f\"{r['modelo']:>5}  AUC={r['auc']:.4f}  F1*={r['f1']:.3f}  thr*={r['thr']:.3f}  (acc={r['acc']:.3f}, pre={r['pre']:.3f}, rec={r['rec']:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
