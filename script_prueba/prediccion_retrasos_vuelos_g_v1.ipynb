{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Machine Learning - Proyecto: Predicci칩n de Retrasos de Vuelos** 九걾잺"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Librer칤as"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Librer칤as Generales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "672d8f58",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Scripts\\python.exe\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "print(sys.executable)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "99eebab7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, time, json, math\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from joblib import dump, load\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    roc_auc_score, f1_score, precision_recall_curve, auc\n",
                ")\n",
                "import warnings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "a8b5ad5f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cargando datos desde d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv...\n",
                        "Datos preparados. Shape: (5231130, 19)\n",
                        "Realizando split temporal (Train 1-9, Valid 10-12)...\n",
                        "X_train: (4299046, 18), X_valid: (932084, 18)\n",
                        "Aplicando LabelEncoder...\n",
                        "\n",
                        "--- Entrenando Experimento: LabelEncoder ---\n",
                        "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
                        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100499 seconds.\n",
                        "You can set `force_row_wise=true` to remove the overhead.\n",
                        "And if memory is not enough, you can set `force_col_wise=true`.\n",
                        "[LightGBM] [Info] Total Bins 2802\n",
                        "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 18\n",
                        "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
                        "[LightGBM] [Info] Start training from score -0.000000\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[200]\tvalid_0's auc: 0.612077\n",
                        "[400]\tvalid_0's auc: 0.613413\n",
                        "[600]\tvalid_0's auc: 0.613906\n",
                        "[800]\tvalid_0's auc: 0.614524\n",
                        "[1000]\tvalid_0's auc: 0.614725\n",
                        "Did not meet early stopping. Best iteration is:\n",
                        "[937]\tvalid_0's auc: 0.61476\n",
                        "Entrenamiento completado en 307.7s\n",
                        "Aplicando Target Encoding K-Fold...\n",
                        "\n",
                        "--- Entrenando Experimento: TargetEncoding (TE) ---\n",
                        "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
                        "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.454331 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 2973\n",
                        "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 18\n",
                        "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
                        "[LightGBM] [Info] Start training from score -0.000000\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[200]\tvalid_0's auc: 0.612114\n",
                        "[400]\tvalid_0's auc: 0.612965\n",
                        "Early stopping, best iteration is:\n",
                        "[468]\tvalid_0's auc: 0.613116\n",
                        "Entrenamiento completado en 191.0s\n",
                        "Aplicando Agregados Hist칩ricos...\n",
                        "Aplicando Target Encoding K-Fold...\n",
                        "\n",
                        "--- Entrenando Experimento: TE + Agregados Hist칩ricos ---\n",
                        "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
                        "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.608504 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 3890\n",
                        "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 24\n",
                        "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
                        "[LightGBM] [Info] Start training from score -0.000000\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "Early stopping, best iteration is:\n",
                        "[2]\tvalid_0's auc: 0.607966\n",
                        "Entrenamiento completado en 76.4s\n",
                        "\n",
                        "\n",
                        "--- Comparaci칩n Final de Alternativas (Validadas en Meses 10-12) ---\n"
                    ]
                },
                {
                    "ename": "ImportError",
                    "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tabulate'",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 323\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Comparaci칩n Final de Alternativas (Validadas en Meses 10-12) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m df_results = pd.DataFrame(RESULTS).set_index(\u001b[33m\"\u001b[39m\u001b[33mExperimento\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfloatfmt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.4f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Determinar el ganador\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_results.empty:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:2994\u001b[39m, in \u001b[36mDataFrame.to_markdown\u001b[39m\u001b[34m(self, buf, mode, index, storage_options, **kwargs)\u001b[39m\n\u001b[32m   2992\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mtablefmt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2993\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mshowindex\u001b[39m\u001b[33m\"\u001b[39m, index)\n\u001b[32m-> \u001b[39m\u001b[32m2994\u001b[39m tabulate = \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabulate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m result = tabulate.tabulate(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\venv\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
                        "\u001b[31mImportError\u001b[39m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
                    ]
                }
            ],
            "source": [
                "import os, time, json, math\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from joblib import dump, load\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    roc_auc_score, f1_score, precision_recall_curve, auc\n",
                ")\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.options.mode.chained_assignment = None\n",
                "\n",
                "# --- Variables Globales ---\n",
                "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
                "TARGET_COL = \"RETRASADO_LLEGADA\"\n",
                "RESULTS = []\n",
                "\n",
                "# ==============================================================================\n",
                "# PASO 1: FUNCIONES DE PREPARACI칍N DE DATOS (Helpers)\n",
                "# ==============================================================================\n",
                "\n",
                "def load_and_prep_data(data_path):\n",
                "    \"\"\"Carga y deriva todas las features necesarias del CSV.\"\"\"\n",
                "    print(f\"Cargando datos desde {data_path}...\")\n",
                "    \n",
                "    # Columnas m칤nimas necesarias del CSV original\n",
                "    need_cols = [\n",
                "        \"MONTH\", \"DAY_OF_WEEK\", \"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\",\n",
                "        \"SCHEDULED_DEPARTURE\", \"ORIGEN_LAT\", \"ORIGEN_LON\", \"DEST_LAT\", \"DEST_LON\",\n",
                "        \"SALIDA_SIN\", \"SALIDA_COS\", \"RETRASADO_LLEGADA\"\n",
                "    ]\n",
                "    \n",
                "    header = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
                "    present = [c for c in need_cols if c in header]\n",
                "    \n",
                "    dtype_map = {\n",
                "        \"MONTH\":\"int8\", \"DAY_OF_WEEK\":\"int8\", \"AIRLINE\":\"category\", \n",
                "        \"ORIGIN_AIRPORT\":\"category\", \"DESTINATION_AIRPORT\":\"category\",\n",
                "        \"SCHEDULED_DEPARTURE\":\"int32\", \"ORIGEN_LAT\":\"float32\", \"ORIGEN_LON\":\"float32\",\n",
                "        \"DEST_LAT\":\"float32\", \"DEST_LON\":\"float32\", \"SALIDA_SIN\":\"float32\", \n",
                "        \"SALIDA_COS\":\"float32\", \"RETRASADO_LLEGADA\":\"int8\"\n",
                "    }\n",
                "    dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
                "\n",
                "    v = pd.read_csv(data_path, usecols=present, dtype=dtype_eff, low_memory=False)\n",
                "\n",
                "    # --- Derivar features ---\n",
                "    def haversine_km(lat1, lon1, lat2, lon2):\n",
                "        R = 6371.0\n",
                "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
                "        dlat = lat2 - lat1; dlon = lon2 - lon1\n",
                "        a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
                "        return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
                "\n",
                "    if \"DISTANCIA_HAV\" not in v.columns:\n",
                "        v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
                "    \n",
                "    if \"MONTH_SIN\" not in v.columns:\n",
                "        v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
                "        v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(\"float32\")\n",
                "\n",
                "    if \"MINUTO_DIA_SALIDA\" not in v.columns:\n",
                "        hs = (v[\"SCHEDULED_DEPARTURE\"] // 100).clip(0, 23).astype(\"int16\")\n",
                "        ms = (v[\"SCHEDULED_DEPARTURE\"] % 100).clip(0, 59).astype(\"int16\")\n",
                "        v[\"MINUTO_DIA_SALIDA\"] = (hs * 60 + ms).astype(\"int16\")\n",
                "        v[\"HORA_SALIDA\"] = hs\n",
                "\n",
                "    if \"RUTA\" not in v.columns:\n",
                "        v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
                "    \n",
                "    print(f\"Datos preparados. Shape: {v.shape}\")\n",
                "    return v\n",
                "\n",
                "def split_temporal(df, target_col):\n",
                "    \"\"\"Split temporal: Train 1-9, Valid 10-12\"\"\"\n",
                "    print(\"Realizando split temporal (Train 1-9, Valid 10-12)...\")\n",
                "    train_mask = df[\"MONTH\"].between(1, 9)\n",
                "    valid_mask = df[\"MONTH\"].between(10, 12)\n",
                "    \n",
                "    y = df[target_col].astype(\"int8\")\n",
                "    X = df.drop(columns=[target_col])\n",
                "    \n",
                "    X_train, y_train = X.loc[train_mask].copy(), y.loc[train_mask].copy()\n",
                "    X_valid, y_valid = X.loc[valid_mask].copy(), y.loc[valid_mask].copy()\n",
                "    \n",
                "    print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}\")\n",
                "    return X_train, y_train, X_valid, y_valid\n",
                "\n",
                "# ==============================================================================\n",
                "# PASO 2: FUNCIONES DE FEATURE ENGINEERING (Codificadores)\n",
                "# ==============================================================================\n",
                "\n",
                "# --- VERSI칍N NUEVA (CORREGIDA v2) ---\n",
                "def apply_label_encoder(X_train_subset, X_valid_subset):\n",
                "    \"\"\"Aplica LabelEncoder a las columnas categ칩ricas.\"\"\"\n",
                "    print(\"Aplicando LabelEncoder...\")\n",
                "    X_train_le = X_train_subset.copy()\n",
                "    X_valid_le = X_valid_subset.copy()\n",
                "\n",
                "    # Itera sobre las columnas del DataFrame que ya le pasamos\n",
                "    cat_cols_in_subset = X_train_subset.columns \n",
                "    \n",
                "    for col in cat_cols_in_subset: \n",
                "        le = LabelEncoder()\n",
                "        X_train_le[col] = le.fit_transform(X_train_le[col].astype(str))\n",
                "        \n",
                "        # Manejar categor칤as no vistas en validaci칩n\n",
                "        le_classes = le.classes_\n",
                "        X_valid_le[col] = X_valid_le[col].astype(str).apply(lambda x: x if x in le_classes else '<unknown>')\n",
                "        if '<unknown>' not in le_classes:\n",
                "            le.classes_ = np.append(le.classes_, '<unknown>')\n",
                "        X_valid_le[col] = le.transform(X_valid_le[col])\n",
                "            \n",
                "    return X_train_le, X_valid_le\n",
                "\n",
                "def kfold_target_encode(s_train, y_train, s_valid, smoothing=50):\n",
                "    \"\"\"Aplica Target Encoding K-Fold (sin fuga) en train y lo mapea a valid.\"\"\"\n",
                "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    gmean = float(y_train.mean())\n",
                "    enc_train = pd.Series(index=s_train.index, dtype=\"float32\")\n",
                "\n",
                "    for tr_idx, val_idx in skf.split(s_train, y_train):\n",
                "        s_tr, y_tr = s_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
                "        s_val = s_train.iloc[val_idx]\n",
                "\n",
                "        stats = y_tr.groupby(s_tr.astype(str)).mean()\n",
                "        cnts = y_tr.groupby(s_tr.astype(str)).size()\n",
                "        smoothed = ((stats * cnts + gmean * smoothing) / (cnts + smoothing)).to_dict()\n",
                "        enc_train.iloc[val_idx] = s_val.astype(str).map(smoothed).fillna(gmean)\n",
                "\n",
                "    # Mapping final para validaci칩n\n",
                "    full_stats = y_train.groupby(s_train.astype(str)).mean()\n",
                "    full_cnts = y_train.groupby(s_train.astype(str)).size()\n",
                "    mapping = ((full_stats * full_cnts + gmean * smoothing) / (full_cnts + smoothing)).to_dict()\n",
                "    enc_valid = s_valid.astype(str).map(mapping).fillna(gmean).astype(\"float32\")\n",
                "    \n",
                "    return enc_train.astype(\"float32\"), enc_valid\n",
                "\n",
                "def apply_target_encoding(X_train, y_train, X_valid, cat_cols):\n",
                "    \"\"\"Aplica TE K-Fold y DEVUELVE SOLO LAS NUEVAS COLUMNAS.\"\"\"\n",
                "    print(\"Aplicando Target Encoding K-Fold...\")\n",
                "    X_train_te = X_train.copy()\n",
                "    X_valid_te = X_valid.copy()\n",
                "    new_te_cols = []\n",
                "    \n",
                "    for col in cat_cols:\n",
                "        new_col_name = f\"{col}_TE\"\n",
                "        enc_tr, enc_val = kfold_target_encode(X_train[col], y_train, X_valid[col])\n",
                "        X_train_te[new_col_name] = enc_tr\n",
                "        X_valid_te[new_col_name] = enc_val\n",
                "        new_te_cols.append(new_col_name)\n",
                "        \n",
                "    # Devolver SOLO las nuevas columnas TE, con el 칤ndice original\n",
                "    return X_train_te[new_te_cols], X_valid_te[new_te_cols]\n",
                "\n",
                "def apply_historical_aggs(X_train, y_train, X_valid, agg_specs):\n",
                "    \"\"\"Calcula agregados hist칩ricos y DEVUELVE SOLO LAS NUEVAS COLUMNAS.\"\"\"\n",
                "    print(\"Aplicando Agregados Hist칩ricos...\")\n",
                "    X_train_agg = X_train.copy()\n",
                "    X_valid_agg = X_valid.copy()\n",
                "    gmean = float(y_train.mean())\n",
                "    new_agg_cols = []\n",
                "    \n",
                "    # DataFrame temporal de entrenamiento para calcular agregados\n",
                "    df_train = X_train.copy()\n",
                "    df_train[TARGET_COL] = y_train\n",
                "    \n",
                "    for keys, pref in agg_specs:\n",
                "        rate_col, n_col = f\"{pref}_rate\", f\"{pref}_n\"\n",
                "        new_agg_cols.extend([rate_col, n_col])\n",
                "        \n",
                "        agg = df_train.groupby(keys, observed=True)[TARGET_COL].agg([\"mean\", \"size\"]).reset_index()\n",
                "        agg.columns = keys + [rate_col, n_col]\n",
                "        \n",
                "        # Merge sin fuga\n",
                "        X_train_agg = X_train_agg.merge(agg, on=keys, how=\"left\")\n",
                "        X_valid_agg = X_valid_agg.merge(agg, on=keys, how=\"left\")\n",
                "\n",
                "        # Llenar NaNs (categor칤as no vistas en train) con la media global\n",
                "        X_train_agg[rate_col] = X_train_agg[rate_col].fillna(gmean).astype(\"float32\")\n",
                "        X_valid_agg[rate_col] = X_valid_agg[rate_col].fillna(gmean).astype(\"float32\")\n",
                "        X_train_agg[n_col] = X_train_agg[n_col].fillna(0).astype(\"float32\")\n",
                "        X_valid_agg[n_col] = X_valid_agg[n_col].fillna(0).astype(\"float32\")\n",
                "\n",
                "    # Devolver SOLO las nuevas columnas de agregados, con el 칤ndice original\n",
                "    return X_train_agg[new_agg_cols], X_valid_agg[new_agg_cols]\n",
                "\n",
                "\n",
                "# ==============================================================================\n",
                "# PASO 3: FUNCIONES DE ENTRENAMIENTO Y EVALUACI칍N\n",
                "# ==============================================================================\n",
                "\n",
                "def train_lgbm(X_train, y_train, X_valid, y_valid, exp_name):\n",
                "    \"\"\"Entrena un modelo LGBM y devuelve el modelo y las m칠tricas.\"\"\"\n",
                "    print(f\"\\n--- Entrenando Experimento: {exp_name} ---\")\n",
                "    \n",
                "    # Usamos class_weight='balanced' porque funcion칩 en tu Rev 5\n",
                "    # y scale_pos_weight pareci칩 fallar en las Rev 4, 6, 7.\n",
                "    params = {\n",
                "        'objective': 'binary',\n",
                "        'metric': 'auc',\n",
                "        'n_estimators': 1000,  # Reducido para una comparaci칩n r치pida\n",
                "        'learning_rate': 0.05,\n",
                "        'num_leaves': 127,\n",
                "        'class_weight': 'balanced',\n",
                "        'n_jobs': -1,\n",
                "        'random_state': 42,\n",
                "        'colsample_bytree': 0.8,\n",
                "        'subsample': 0.8,\n",
                "        'min_child_samples': 200\n",
                "    }\n",
                "    \n",
                "    model = lgb.LGBMClassifier(**params)\n",
                "    \n",
                "    t0 = time.time()\n",
                "    model.fit(\n",
                "        X_train, y_train,\n",
                "        eval_set=[(X_valid, y_valid)],\n",
                "        eval_metric=\"auc\",\n",
                "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n",
                "    )\n",
                "    t1 = time.time()\n",
                "    \n",
                "    print(f\"Entrenamiento completado en {t1-t0:.1f}s\")\n",
                "    \n",
                "    # Calcular m칠tricas\n",
                "    y_proba = model.predict_proba(X_valid)[:, 1]\n",
                "    auc_roc = roc_auc_score(y_valid, y_proba)\n",
                "    \n",
                "    # Encontrar mejor F1\n",
                "    prec, rec, thr = precision_recall_curve(y_valid, y_proba)\n",
                "    f1s = (2 * prec * rec) / (prec + rec)\n",
                "    best_f1_idx = np.nanargmax(f1s)\n",
                "    best_f1 = f1s[best_f1_idx]\n",
                "    best_thr = thr[best_f1_idx]\n",
                "    \n",
                "    # AUC-PR\n",
                "    auc_pr = auc(rec, prec)\n",
                "    \n",
                "    metrics = {\n",
                "        \"Experimento\": exp_name,\n",
                "        \"ROC-AUC\": round(auc_roc, 4),\n",
                "        \"PR-AUC\": round(auc_pr, 4),\n",
                "        \"Best_F1\": round(best_f1, 4),\n",
                "        \"Best_F1_Threshold\": round(best_thr, 3),\n",
                "        \"Tiempo_Entrenamiento (s)\": round(t1 - t0, 1)\n",
                "    }\n",
                "    \n",
                "    RESULTS.append(metrics)\n",
                "    return model, metrics\n",
                "# ==============================================================================\n",
                "# PASO 4: EJECUCI칍N DE LOS EXPERIMENTOS (CORREGIDO v4)\n",
                "# ==============================================================================\n",
                "\n",
                "# Cargar y preparar datos (UNA SOLA VEZ)\n",
                "v_full = load_and_prep_data(DATA_PATH)\n",
                "X_train_base, y_train, X_valid_base, y_valid = split_temporal(v_full, TARGET_COL)\n",
                "\n",
                "# Columnas para ingenier칤a de features\n",
                "cat_cols = [\"AIRLINE\", \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\", \"RUTA\"]\n",
                "num_cols = [c for c in X_train_base.columns if c not in cat_cols]\n",
                "agg_specs = [\n",
                "    ([\"RUTA\", \"HORA_SALIDA\"], \"RUTA_HORA\"),\n",
                "    ([\"AIRLINE\"], \"AIR\"),\n",
                "    ([\"ORIGIN_AIRPORT\"], \"ORI\")\n",
                "]\n",
                "\n",
                "# --- Targets reseteados (se usan en todos los experimentos) ---\n",
                "y_train_reset = y_train.reset_index(drop=True)\n",
                "y_valid_reset = y_valid.reset_index(drop=True)\n",
                "\n",
                "\n",
                "# --- Experimento 1: LabelEncoder (Revisi칩n 4 corregida) ---\n",
                "X_train_le, X_valid_le = apply_label_encoder(X_train_base[cat_cols], X_valid_base[cat_cols])\n",
                "# Unir num칠ricas y resetear 칤ndice\n",
                "X_train_1 = pd.concat([X_train_base[num_cols].reset_index(drop=True), X_train_le.reset_index(drop=True)], axis=1)\n",
                "X_valid_1 = pd.concat([X_valid_base[num_cols].reset_index(drop=True), X_valid_le.reset_index(drop=True)], axis=1)\n",
                "train_lgbm(X_train_1, y_train_reset, X_valid_1, y_valid_reset, \"LabelEncoder\")\n",
                "\n",
                "\n",
                "# --- Experimento 2: Target Encoding K-Fold (Revisi칩n 5 corregida) ---\n",
                "X_train_te_cols, X_valid_te_cols = apply_target_encoding(X_train_base[cat_cols], y_train, X_valid_base[cat_cols], cat_cols)\n",
                "# Unir num칠ricas y resetear 칤ndice\n",
                "X_train_2 = pd.concat([X_train_base[num_cols].reset_index(drop=True), X_train_te_cols.reset_index(drop=True)], axis=1)\n",
                "X_valid_2 = pd.concat([X_valid_base[num_cols].reset_index(drop=True), X_valid_te_cols.reset_index(drop=True)], axis=1)\n",
                "train_lgbm(X_train_2, y_train_reset, X_valid_2, y_valid_reset, \"TargetEncoding (TE)\")\n",
                "\n",
                "\n",
                "# --- Experimento 3: TE + Agregados (Revisi칩n 6/7 corregida) ---\n",
                "\n",
                "# *** FIX: Resetear los inputs ANTES de pasarlos a las funciones de FE ***\n",
                "X_train_base_r = X_train_base.reset_index(drop=True)\n",
                "y_train_r = y_train.reset_index(drop=True)\n",
                "X_valid_base_r = X_valid_base.reset_index(drop=True)\n",
                "y_valid_r = y_valid.reset_index(drop=True)\n",
                "\n",
                "# 1. Agregados Hist칩ricos (ahora usa inputs reseteados)\n",
                "# X_train_agg_cols tendr치 칤ndice 0..M\n",
                "X_train_agg_cols, X_valid_agg_cols = apply_historical_aggs(X_train_base_r, y_train_r, X_valid_base_r, agg_specs)\n",
                "\n",
                "# 2. Target Encoding (ahora usa inputs reseteados)\n",
                "# X_train_te_cols tendr치 칤ndice 0..M\n",
                "X_train_te_cols, X_valid_te_cols = apply_target_encoding(X_train_base_r[cat_cols], y_train_r, X_valid_base_r[cat_cols], cat_cols)\n",
                "\n",
                "# 3. Concatenar (TODAS las piezas tienen 칤ndice 0..M y se alinean)\n",
                "X_train_3 = pd.concat([X_train_base_r[num_cols], X_train_te_cols, X_train_agg_cols], axis=1)\n",
                "X_valid_3 = pd.concat([X_valid_base_r[num_cols], X_valid_te_cols, X_valid_agg_cols], axis=1)\n",
                "\n",
                "# 4. Entrenar (X_train_3 e y_train_r est치n ambos reseteados 0..M)\n",
                "train_lgbm(X_train_3, y_train_r, X_valid_3, y_valid_r, \"TE + Agregados Hist칩ricos\")\n",
                "\n",
                "\n",
                "# ==============================================================================\n",
                "# PASO 5: REPORTE FINAL\n",
                "# (Esta celda no necesita cambios)\n",
                "# ==============================================================================\n",
                "\n",
                "print(\"\\n\\n--- Comparaci칩n Final de Alternativas (Validadas en Meses 10-12) ---\")\n",
                "df_results = pd.DataFrame(RESULTS).set_index(\"Experimento\")\n",
                "print(df_results.to_markdown(floatfmt=\".4f\"))\n",
                "\n",
                "# Determinar el ganador\n",
                "if not df_results.empty:\n",
                "    winner = df_results['ROC-AUC'].idxmax()\n",
                "    print(f\"\\n游끥 Ganador (por ROC-AUC): {winner}\")\n",
                "else:\n",
                "    print(\"\\nNo se completaron experimentos para determinar un ganador.\")\n",
                "print(\"---\")\n",
                "print(\"Nota: Un ROC-AUC m치s alto indica un mejor modelo para distinguir entre clases.\")\n",
                "print(\"Un PR-AUC m치s alto es mejor para problemas desbalanceados (ignora 'Aciertos a Tiempo').\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
