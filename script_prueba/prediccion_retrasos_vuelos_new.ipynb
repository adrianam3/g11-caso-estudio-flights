{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1574c18c",
   "metadata": {},
   "source": [
    "0. Encabezado y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62ddf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 0 · Imports, ruta y helpers\n",
    "import os, time, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f4e52",
   "metadata": {},
   "source": [
    "1. Carga del CSV (eficiente: usecols + dtypes compactos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9add5f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas cargadas: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'ORIGEN_LAT', 'ORIGEN_LON', 'DEST_LAT', 'DEST_LON', 'SALIDA_SIN', 'SALIDA_COS', 'RETRASADO_LLEGADA'] \n",
      "Faltantes (se derivan si aplica): []\n",
      "✅ Cargado (5231130, 14) en 25.7s | Rate retraso=0.1847\n"
     ]
    }
   ],
   "source": [
    "# Paso 1 · Carga del CSV (solo columnas útiles + dtypes compactos)\n",
    "# DATA_PATH = os.path.join(\"data\", \"processed\", \"flights_clean.csv\")  # ajusta si lo tienes en otra ruta\n",
    "DATA_PATH = r\"d:\\OneDrive\\DOCUMENTOS\\Personales\\2024\\uniandes\\8 S\\seminario\\g11-caso-estudio-flights\\data\\processed\\flights_clean.csv\"\n",
    "\n",
    "need_cols = [\n",
    "    \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\n",
    "    \"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\n",
    "    \"SCHEDULED_DEPARTURE\",\n",
    "    \"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\",\n",
    "    \"SALIDA_SIN\",\"SALIDA_COS\",\n",
    "    \"RETRASADO_LLEGADA\"\n",
    "]\n",
    "\n",
    "# leer solo el header para ver qué hay\n",
    "header = pd.read_csv(DATA_PATH, nrows=0).columns.tolist()\n",
    "present = [c for c in need_cols if c in header]\n",
    "missing = [c for c in need_cols if c not in header]\n",
    "print(\"Columnas cargadas:\", present, \"\\nFaltantes (se derivan si aplica):\", missing)\n",
    "\n",
    "dtype_map = {\n",
    "    \"MONTH\":\"int8\",\"DAY\":\"int8\",\"DAY_OF_WEEK\":\"int8\",\n",
    "    \"AIRLINE\":\"category\",\"ORIGIN_AIRPORT\":\"category\",\"DESTINATION_AIRPORT\":\"category\",\n",
    "    \"SCHEDULED_DEPARTURE\":\"int32\",\n",
    "    \"ORIGEN_LAT\":\"float32\",\"ORIGEN_LON\":\"float32\",\"DEST_LAT\":\"float32\",\"DEST_LON\":\"float32\",\n",
    "    \"SALIDA_SIN\":\"float32\",\"SALIDA_COS\":\"float32\",\n",
    "    \"RETRASADO_LLEGADA\":\"int8\"\n",
    "}\n",
    "dtype_eff = {k:v for k,v in dtype_map.items() if k in present}\n",
    "\n",
    "t0 = time.time()\n",
    "v = pd.read_csv(DATA_PATH, usecols=present, dtype=dtype_eff, low_memory=False)\n",
    "t1 = time.time()\n",
    "print(f\"✅ Cargado {v.shape} en {t1-t0:.1f}s | Rate retraso={float(v['RETRASADO_LLEGADA'].mean()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce900b0",
   "metadata": {},
   "source": [
    "2. Derivar columnas que falten (DISTANCIA_HAV, MONTH_SIN/COS, MINUTO_DIA_SALIDA, RUTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a9aee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Derivadas OK | columnas: 19\n"
     ]
    }
   ],
   "source": [
    "# Paso 2 · Derivación de features faltantes\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return (2*R*np.arcsin(np.sqrt(a))).astype(np.float32)\n",
    "\n",
    "# Distancia\n",
    "if {\"ORIGEN_LAT\",\"ORIGEN_LON\",\"DEST_LAT\",\"DEST_LON\"}.issubset(v.columns) and \"DISTANCIA_HAV\" not in v.columns:\n",
    "    v[\"DISTANCIA_HAV\"] = haversine_km(v[\"ORIGEN_LAT\"], v[\"ORIGEN_LON\"], v[\"DEST_LAT\"], v[\"DEST_LON\"])\n",
    "\n",
    "# Estacionalidad mes\n",
    "if \"MONTH\" in v.columns and \"MONTH_SIN\" not in v.columns:\n",
    "    v[\"MONTH_SIN\"] = np.sin(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "    v[\"MONTH_COS\"] = np.cos(2*np.pi * v[\"MONTH\"]/12).astype(np.float32)\n",
    "\n",
    "# Minuto del día (si no vino ya)\n",
    "if \"MINUTO_DIA_SALIDA\" not in v.columns and \"SCHEDULED_DEPARTURE\" in v.columns:\n",
    "    hs = (v[\"SCHEDULED_DEPARTURE\"]//100).clip(0,23)\n",
    "    ms = (v[\"SCHEDULED_DEPARTURE\"]%100).clip(0,59)\n",
    "    v[\"MINUTO_DIA_SALIDA\"] = (hs*60 + ms).astype(np.int16)\n",
    "\n",
    "# Ruta (texto)\n",
    "if {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(v.columns) and \"RUTA\" not in v.columns:\n",
    "    v[\"RUTA\"] = v[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + v[\"DESTINATION_AIRPORT\"].astype(str)\n",
    "\n",
    "print(\"✅ Derivadas OK | columnas:\", len(v.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0a694",
   "metadata": {},
   "source": [
    "3. Definir features y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35627e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5231130, 12) | y rate: 0.18471362783949166\n"
     ]
    }
   ],
   "source": [
    "# Paso 3 · Selección de variables\n",
    "target = \"RETRASADO_LLEGADA\"\n",
    "\n",
    "cat_cols = [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"]\n",
    "num_cols = [\"MONTH\",\"DAY_OF_WEEK\",\"SALIDA_SIN\",\"SALIDA_COS\",\"MONTH_SIN\",\"MONTH_COS\",\"DISTANCIA_HAV\",\"MINUTO_DIA_SALIDA\"]\n",
    "\n",
    "features = [c for c in cat_cols + num_cols if c in v.columns]\n",
    "X = v[features].copy()\n",
    "y = v[target].astype(\"int8\").copy()\n",
    "\n",
    "print(\"X:\", X.shape, \"| y rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5add5",
   "metadata": {},
   "source": [
    "4. Split temporal (train: meses 1–9, valid: 10–12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25a998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4299046, 12) Valid: (932084, 12) | rate train: 0.18733737671101913 | rate valid: 0.17261212508743848\n"
     ]
    }
   ],
   "source": [
    "# Paso 4 · Split temporal (evita fuga)\n",
    "train_mask = v[\"MONTH\"].between(1,9)\n",
    "valid_mask = v[\"MONTH\"].between(10,12)\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "X_valid = X.loc[valid_mask].copy()\n",
    "y_valid = y.loc[valid_mask].copy()\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Valid:\", X_valid.shape,\n",
    "      \"| rate train:\", float(y_train.mean()), \"| rate valid:\", float(y_valid.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f932d",
   "metadata": {},
   "source": [
    "5. Target Encoding KFold (sin fuga) para categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb92876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE sobre: ['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'RUTA']\n",
      "✅ TE aplicado sin fuga.\n",
      "Ejemplo TE: {'AIRLINE': ['AA', 'AS']}\n",
      "Listo para entrenar:\n",
      "X_train_model: (4299046, 12) | X_valid_model: (932084, 12)\n",
      "Columnas (primeras 12): ['MONTH', 'DAY_OF_WEEK', 'SALIDA_SIN', 'SALIDA_COS', 'MONTH_SIN', 'MONTH_COS', 'DISTANCIA_HAV', 'MINUTO_DIA_SALIDA', 'AIRLINE_TE', 'ORIGIN_AIRPORT_TE', 'DESTINATION_AIRPORT_TE', 'RUTA_TE']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Target Encoding KFold ROBUSTO (sin fuga) + armado de matrices\n",
    "# ============================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) PRERREQUISITOS / ALINEACIONES ---\n",
    "# Deben existir de pasos previos:\n",
    "#   v, X, y, X_train, X_valid, y_train, y_valid\n",
    "for obj_name in [\"X_train\", \"X_valid\", \"y_train\", \"y_valid\"]:\n",
    "    assert obj_name in globals(), f\"Falta {obj_name}. Ejecuta los pasos previos.\"\n",
    "\n",
    "# Asegurar que y_train/y_valid están alineados con X_train/X_valid\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_valid = y_valid.loc[X_valid.index]\n",
    "\n",
    "# Crear RUTA si no existe\n",
    "if \"RUTA\" not in X_train.columns and {\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\"}.issubset(X_train.columns):\n",
    "    X_train = X_train.copy()\n",
    "    X_valid = X_valid.copy()\n",
    "    X_train[\"RUTA\"] = (X_train[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_train[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "    X_valid[\"RUTA\"] = (X_valid[\"ORIGIN_AIRPORT\"].astype(str) + \"_\" + X_valid[\"DESTINATION_AIRPORT\"].astype(str))\n",
    "\n",
    "# Columnas a codificar (solo si existen)\n",
    "cols_te = [c for c in [\"AIRLINE\",\"ORIGIN_AIRPORT\",\"DESTINATION_AIRPORT\",\"RUTA\"] if c in X_train.columns]\n",
    "print(\"TE sobre:\", cols_te)\n",
    "\n",
    "# --- 1) Funciones TE robustas (operan con Series) ---\n",
    "def kfold_target_encode_series(s: pd.Series,\n",
    "                               y: pd.Series,\n",
    "                               n_splits=5,\n",
    "                               smoothing=50,\n",
    "                               seed=42) -> tuple[pd.Series, dict, float]:\n",
    "    \"\"\"\n",
    "    s: Serie categórica (mismo índice que y)\n",
    "    y: Serie binaria 0/1 (mismo índice que s)\n",
    "    Devuelve:\n",
    "      enc     -> Serie con el encoding KFold para s (alineada a s.index)\n",
    "      mapping -> dict valor_categoria -> encoding_final (con TODO el train)\n",
    "      gmean   -> media global (fallback)\n",
    "    \"\"\"\n",
    "    # Alineación defensiva por índice\n",
    "    idx = s.index.intersection(y.index)\n",
    "    s = s.loc[idx]\n",
    "    y = y.loc[idx].astype(float)\n",
    "\n",
    "    # Normalización de tipos\n",
    "    s = s.astype(\"string\")  # evita NaNs tipo objeto raros\n",
    "    gmean = float(y.mean())\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    enc = pd.Series(index=s.index, dtype=np.float32)\n",
    "\n",
    "    for tr_idx, val_idx in skf.split(np.zeros(len(s)), y):\n",
    "        s_tr, y_tr = s.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        s_val      = s.iloc[val_idx]\n",
    "\n",
    "        stats = y_tr.groupby(s_tr).mean()\n",
    "        cnts  = y_tr.groupby(s_tr).size()\n",
    "\n",
    "        smoothed = (stats*cnts + gmean*smoothing) / (cnts + smoothing)\n",
    "        enc.iloc[val_idx] = s_val.map(smoothed).fillna(gmean).astype(np.float32)\n",
    "\n",
    "    # Mapping final con TODO el train (para producción/valid)\n",
    "    full_stats = y.groupby(s).mean()\n",
    "    full_cnts  = y.groupby(s).size()\n",
    "    mapping = ((full_stats*full_cnts + gmean*smoothing) / (full_cnts + smoothing)).to_dict()\n",
    "\n",
    "    return enc, mapping, gmean\n",
    "\n",
    "def apply_te(series: pd.Series, mapping: dict, default: float) -> pd.Series:\n",
    "    return series.astype(\"string\").map(mapping).fillna(default).astype(np.float32)\n",
    "\n",
    "# --- 2) Ejecutar TE ---\n",
    "mappings, defaults = {}, {}\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy()\n",
    "\n",
    "for c in cols_te:\n",
    "    enc_tr, mapping, default = kfold_target_encode_series(X_train[c], y_train, n_splits=5, smoothing=50, seed=42)\n",
    "    X_train[f\"{c}_TE\"] = enc_tr\n",
    "    X_valid[f\"{c}_TE\"] = apply_te(X_valid[c], mapping, default)\n",
    "    mappings[c] = mapping\n",
    "    defaults[c] = default\n",
    "\n",
    "print(\"✅ TE aplicado sin fuga.\")\n",
    "print(\"Ejemplo TE:\", {k: list(v)[:2] if hasattr(v, \"__iter__\") else v for k,v in list(mappings.items())[:1]})\n",
    "\n",
    "# --- 3) Construir matrices finales: quitamos las categorías crudas ---\n",
    "X_train_model = X_train.drop(columns=[c for c in cols_te if c in X_train.columns]).copy()\n",
    "X_valid_model = X_valid.drop(columns=[c for c in cols_te if c in X_valid.columns]).copy()\n",
    "\n",
    "print(\"Listo para entrenar:\")\n",
    "print(\"X_train_model:\", X_train_model.shape, \"| X_valid_model:\", X_valid_model.shape)\n",
    "print(\"Columnas (primeras 12):\", list(X_train_model.columns)[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebf0b3",
   "metadata": {},
   "source": [
    "6. Entrenamiento LightGBM (early stopping, balanceo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0678c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (4299046, 12) (932084, 12)\n",
      "scale_pos_weight ~ 4.34 (neg=3493674, pos=805372)\n",
      "[LightGBM] [Info] Number of positive: 805372, number of negative: 3493674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1883\n",
      "[LightGBM] [Info] Number of data points in the train set: 4299046, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.187337 -> initscore=-1.467405\n",
      "[LightGBM] [Info] Start training from score -1.467405\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\tvalid_0's auc: 0.612396\tvalid_0's binary_logloss: 0.574345\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.610956\tvalid_0's binary_logloss: 0.460388\n",
      "✅ Entrenado en 117.2s | best_iter=2 | ROC-AUC valid=0.6110\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Chequeos rápidos\n",
    "for n in [\"X_train_model\",\"X_valid_model\",\"y_train\",\"y_valid\"]:\n",
    "    assert n in globals(), f\"Falta {n}\"\n",
    "print(\"Shapes:\", X_train_model.shape, X_valid_model.shape)\n",
    "\n",
    "# Balanceo por proporción de clases (pos/neg)\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(f\"scale_pos_weight ~ {scale_pos_weight:.2f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=127,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.85,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.5,\n",
    "    # usa uno u otro balanceo (recomiendo este):\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train_model, y_train,\n",
    "    eval_set=[(X_valid_model, y_valid)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=300), lgb.log_evaluation(300)]\n",
    ")\n",
    "t1 = time.time()\n",
    "\n",
    "valid_proba = model.predict_proba(X_valid_model)[:, 1]\n",
    "auc_val = roc_auc_score(y_valid, valid_proba)\n",
    "print(f\"✅ Entrenado en {(t1-t0):.1f}s | best_iter={model.best_iteration_} | ROC-AUC valid={auc_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265cabc",
   "metadata": {},
   "source": [
    "7) Métricas base (0.5) + búsqueda de mejor umbral (por F1) y matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b31b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Base 0.5 (thr=0.500) ==\n",
      "Accuracy: 0.8274 | Precision: 0.0000 | Recall: 0.0000 | F1=0.0000 | ROC-AUC=0.6110\n",
      "CM [[TN, FP],[FN, TP]]=\n",
      " [[771195      0]\n",
      " [160889      0]]\n",
      "\n",
      "== Mejor F1 (thr=0.200) ==\n",
      "Accuracy: 0.4794 | Precision: 0.2098 | Recall: 0.7285 | F1=0.3258 | ROC-AUC=0.6110\n",
      "CM [[TN, FP],[FN, TP]]=\n",
      " [[329672 441523]\n",
      " [ 43679 117210]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.47944391278039317,\n",
       " 'pre': 0.20977819459383998,\n",
       " 'rec': 0.7285146902522858,\n",
       " 'f1': 0.3257543543693773,\n",
       " 'auc': 0.6109562804872528,\n",
       " 'thr': 0.2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def report_metrics(y_true, y_prob, thr=0.5, title=\"\"):\n",
    "    y_hat = (y_prob >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_hat)\n",
    "    pre = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_hat, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm  = confusion_matrix(y_true, y_hat)\n",
    "    print(f\"\\n== {title} (thr={thr:.3f}) ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {pre:.4f} | Recall: {rec:.4f} | F1={f1:.4f} | ROC-AUC={auc:.4f}\")\n",
    "    print(\"CM [[TN, FP],[FN, TP]]=\\n\", cm)\n",
    "    return dict(acc=acc, pre=pre, rec=rec, f1=f1, auc=auc, thr=thr)\n",
    "\n",
    "# Base 0.5\n",
    "base = report_metrics(y_valid, valid_proba, 0.5, \"Base 0.5\")\n",
    "\n",
    "# Mejor F1 (búsqueda simple)\n",
    "best = {\"thr\":0.5, \"f1\":-1}\n",
    "for thr in np.linspace(0.1, 0.9, 33):\n",
    "    y_hat = (valid_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_valid, y_hat, zero_division=0)\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = {\"thr\":float(thr), \"f1\":float(f1)}\n",
    "best_f1_res = report_metrics(y_valid, valid_proba, best[\"thr\"], \"Mejor F1\")\n",
    "best_f1_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ccc93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
